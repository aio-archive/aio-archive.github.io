[
  {
    "objectID": "index.html",
    "href": "index.html",
    "title": "AIO2023 Course",
    "section": "",
    "text": "“All models are wrong, but some are useful” George Box\n\n\n\n\n Back to top",
    "crumbs": [
      "Resources",
      "Introduction"
    ]
  },
  {
    "objectID": "about.html",
    "href": "about.html",
    "title": "About",
    "section": "",
    "text": "About this site\n\n\n\n Back to top"
  },
  {
    "objectID": "module1.html",
    "href": "module1.html",
    "title": "Module 1 - Introduction to Python Programming",
    "section": "",
    "text": "This website is still under construction. Some contents and functions might temporarily not be available or not working properly.",
    "crumbs": [
      "Resources",
      "Course content",
      "Module 1"
    ]
  },
  {
    "objectID": "module1.html#main-lessons",
    "href": "module1.html#main-lessons",
    "title": "Module 1 - Introduction to Python Programming",
    "section": "Main Lessons",
    "text": "Main Lessons\n\n\n\n\n\n\n\n\n\n\n\nDate\nType\nContent\nInstructor\nHandout\nFurther Reading\n\n\n\n\n08/05/23\n\nOnline Office Hour\nDr. Vinh\n\n\n\n\n28/04/23\nMain Lesson\nSinh hoạt lớp\nDr. Phúc\nslide\n\n\n\n01/05/23\nAssignment\nBasic Python Exercise\nTA Khoa\n[solution]\n\n\n\n03/05/23\nMain Lesson\nBasic Python 1\nDr. Vinh\n230503 - M01ML01\nNote\n\n\n05/05/23\nMain Lesson\nBasic Python 2\nDr. Vinh\n230505 - M01ML02\nNote\n\n\n07/05/23\nExercise Session\nTA-Exercise\nTA Khoa\n230507 - M01ES01\n\n\n\n08/05/23\nAssignment\nData Structure Exercise\nDr. Vinh\n[solution]\n\n\n\n10/05/23\nMain Lesson\nData Structure\nDr. Đình Vinh\n230510 - M01ML03\n\n\n\n12/05/23\nMain Lesson\nData Structure\nDr. Đình Vinh\n230512 - M01ML04\n\n\n\n14/05/23\nExercise Session\nTA-Exercise\nDr. Vinh\n230514 - M01ES02\n\n\n\n14/05/23\nAssignment\nPython OOP Exercise\nDr. Đình Vinh\n\n\n\n\n17/05/23\nMain Lesson\nOOP with Python\nDr. Đình Vinh\n230517 - M01ML05\nPrevious offering by Dr. Vinh\n\n\n19/05/23\nMain Lesson\nOOP with Python\nDr. Đình Vinh\n230519 - M01ML06\n\n\n\n21/05/23\nExercise Session\nTA-Exercise\nDr. Vinh\n230521 - M01ES03",
    "crumbs": [
      "Resources",
      "Course content",
      "Module 1"
    ]
  },
  {
    "objectID": "module1.html#projects",
    "href": "module1.html#projects",
    "title": "Module 1 - Introduction to Python Programming",
    "section": "Projects",
    "text": "Projects\nRegarding the projects, there are three projects where you will respectively learn how to use YOLOv8, an object detection model as well as how to use Python to manipulate and crawl data from a website. Finally, the last project is about developing simple applications using ChatGPT. In particular, the three projects are:\n\nObject Detection with YOLOv8\nData Manipulation and Crawling\nChatGPT Applications",
    "crumbs": [
      "Resources",
      "Course content",
      "Module 1"
    ]
  },
  {
    "objectID": "module1.html#competition-training",
    "href": "module1.html#competition-training",
    "title": "Module 1 - Introduction to Python Programming",
    "section": "Competition Training",
    "text": "Competition Training\nAs for competition training, this module contains three lectures with the goal of teaching you the basic skills and knowledge you need before joining an AI competition, including visualizing data, knowledge about competition tasks and metrics, and design validation.",
    "crumbs": [
      "Resources",
      "Course content",
      "Module 1"
    ]
  },
  {
    "objectID": "module1.html#extra-class",
    "href": "module1.html#extra-class",
    "title": "Module 1 - Introduction to Python Programming",
    "section": "Extra class",
    "text": "Extra class\nThe central theme of the extra class for this module is about Algorithms and Complexity. In the age of AI, still, the knowledge about algorithms and their complexity including Big-O, Brute-force exhaustive, recursion, two pointer, and dynamic programming still plays an immensely important role.",
    "crumbs": [
      "Resources",
      "Course content",
      "Module 1"
    ]
  },
  {
    "objectID": "resources.html",
    "href": "resources.html",
    "title": "Resources",
    "section": "",
    "text": "Warning\n\n\nThis website is still under construction. Some contents and functions might temporarily not be available or not working properly.\n\n\n\nBooks\nBlogs\nCourses\nFundamental Papers\n\n\n\n\n Back to top",
    "crumbs": [
      "Home",
      "Others",
      "Resources"
    ]
  },
  {
    "objectID": "misc.html",
    "href": "misc.html",
    "title": "Misc",
    "section": "",
    "text": "Simple Tokenizer\n\n\n\n\n Back to top",
    "crumbs": [
      "Home",
      "Others",
      "Misc"
    ]
  },
  {
    "objectID": "resources.html#math",
    "href": "resources.html#math",
    "title": "Resources",
    "section": "Math",
    "text": "Math\n\nProbability and Statistics\n\nIntroduction to Probability for Data Science by Stanley H. Chan\nWasserman, L. (2013). All of statistics: a concise course in statistical inference. Springer Science & Business Media.\n\n\n\nLinear Algebra\n\n\nOptimization\n\n\nOthers\n\nMathematics for Machine Learning by Garret Thomas",
    "crumbs": [
      "Home",
      "Others",
      "Resources"
    ]
  },
  {
    "objectID": "resources.html#machine-learning",
    "href": "resources.html#machine-learning",
    "title": "Resources",
    "section": "Machine Learning",
    "text": "Machine Learning\n\nProbabilistic Machine Learning: An Introduction by Kevin P. Murphy\nProbabilistic Machine Learning: Advanced Topics by Kevin P. Murphy",
    "crumbs": [
      "Home",
      "Others",
      "Resources"
    ]
  },
  {
    "objectID": "resources.html#deep-learning",
    "href": "resources.html#deep-learning",
    "title": "Resources",
    "section": "Deep Learning",
    "text": "Deep Learning\n\nNLP",
    "crumbs": [
      "Home",
      "Others",
      "Resources"
    ]
  },
  {
    "objectID": "module2.html",
    "href": "module2.html",
    "title": "Module 2 - Calculus and Linear Algebra for AI",
    "section": "",
    "text": "This website is still under construction. Some contents and functions might temporarily not be available or not working properly.",
    "crumbs": [
      "Resources",
      "Course content",
      "Module 2"
    ]
  },
  {
    "objectID": "blogs.html",
    "href": "blogs.html",
    "title": "Blogs",
    "section": "",
    "text": "https://leimao.github.io/blog/\nhttps://francisbach.com/\nhttps://davidstutz.de/category/blog/\nhttps://ai.stanford.edu/~gwthomas/notes/index.html\n\n\n\n\n Back to top"
  },
  {
    "objectID": "books.html",
    "href": "books.html",
    "title": "Books",
    "section": "",
    "text": "Introduction to Probability for Data Science by Stanley H. Chan\nWasserman, L. (2013). All of statistics: a concise course in statistical inference. Springer Science & Business Media.\n\n\n\n\n\n\n\n\n\n\n\nMathematics for Machine Learning by Garret Thomas"
  },
  {
    "objectID": "courses.html",
    "href": "courses.html",
    "title": "Machine Learning",
    "section": "",
    "text": "Machine Learning\n\nCS189/289A Introduction to Machine Learning, UC Berkeley\n\n\n\n\n\n Back to top"
  },
  {
    "objectID": "books.html#probability-and-statistics",
    "href": "books.html#probability-and-statistics",
    "title": "Books",
    "section": "",
    "text": "Introduction to Probability for Data Science by Stanley H. Chan\nWasserman, L. (2013). All of statistics: a concise course in statistical inference. Springer Science & Business Media."
  },
  {
    "objectID": "books.html#others",
    "href": "books.html#others",
    "title": "Books",
    "section": "",
    "text": "Mathematics for Machine Learning by Garret Thomas"
  },
  {
    "objectID": "books.html#nlp",
    "href": "books.html#nlp",
    "title": "Books",
    "section": "NLP",
    "text": "NLP"
  },
  {
    "objectID": "courses.html#nlp",
    "href": "courses.html#nlp",
    "title": "Courses",
    "section": "NLP",
    "text": "NLP"
  },
  {
    "objectID": "courses.html#computer-vision",
    "href": "courses.html#computer-vision",
    "title": "Courses",
    "section": "Computer Vision",
    "text": "Computer Vision"
  },
  {
    "objectID": "courses.html#multimodal",
    "href": "courses.html#multimodal",
    "title": "Courses",
    "section": "Multimodal",
    "text": "Multimodal"
  },
  {
    "objectID": "module7.html",
    "href": "module7.html",
    "title": "Module 7 - Advanced Model Architectures for Computer Vision",
    "section": "",
    "text": "Type\nContent\nInstructor\nHandout\nFurther Reading\n\n\n\n\nAssignment\nDomain Conversion Exercise\nTA Khoa\n\n\n\n\nMain Lesson\nDomain Conversion - Denoising and Segmentation\nDr. Vinh\n231220-23M07MC\n\n\n\nMain Lesson\nDomain Conversion - Colorization and Super Resolution\nDr. Vinh\n231222-23M07MC\n\n\n\nExercise Session\nTA Exercise\nTA Khoa\n231224-23M07MC\n\n\n\nAssignment\nObject Detection Project\nTA Hùng\n\n\n\n\nMain Lesson\nObject Detection (1)\nDr. Đình Vinh\n231227-23M07MC\n\n\n\nMain Lesson\nObject Detection (2)\nDr. Đình Vinh\n231229-23M07MC\n\n\n\nExercise Session\nTA Exercise\nTA Hùng\n231231-23M07MC\n\n\n\nMain Lesson\nObject Detection\nDr. Đình Vinh\n240103-23M07MC\n\n\n\nMain Lesson\nObject Detection\nDr. Đình Vinh\n240105-23M07MC\n\n\n\nExercise Session\nTA-Exercise\nTA Hùng\n240107-23M07MC\n\n\n\nAssignment\nImbalanced Data Exercise\n\n\n\n\n\nMain Lesson\nAdvanced Topic: Imbalanced Data\nDr. Vinh\n240110-23M07MC\n\n\n\nMain Lesson\nAdvanced Topic: Self/semi-supervised Learning\nDr. Hưng\n240112-23M07MC\n\n\n\nMain Lesson\nAdvanced Topic: Knowledge Distillation\nDr. Hưng\n240114-23M07MC\n\n\n\nProject tutorial\nImage Project: Tracking by Detection\nTA Thắng\n240117-23M07MC\n\n\n\nProject tutorial\nImage Project: Medical Image Analysis (1)\nTA Huy\n240119-23M07MC\n\n\n\nProject tutorial\nImage Project: Medical Image Analysis (2)\nTA Huy\n240121-23M07MC\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nType\nContent\nInstructor\nHandout\nFurther Reading\n\n\n\n\nPreview\nUNet\nTA Thái\n231219-23M07-EC2\n\n\n\nPreview\nObject Detection using Pretrained Models\nTA Thái\n231226-23M07-EC2\n\n\n\nPreview\nYolov1\nTA Thái\n240102-23M07-EC2\n\n\n\nPreview\nIntroduction to Imbalanced Data\nTA Thái\n240109-23M07-EC2\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nType\nContent\nInstructor\nHandout\nFurther Reading\n\n\n\n\nResearch & Paper\nGroup Report (1)\nDr. Đình Vinh\nN/A\n\n\n\nResearch & Paper\nGroup Report (2)\nDr. Đình Vinh\nN/A\n\n\n\nResearch & Paper\nHow to Write a Paper (1)\nDr. Đình Vinh\n240106-23M07-EC1\n\n\n\nResearch & Paper\nHow to Write a Paper (2)\nDr. Đình Vinh\n240113-23M07-EC1\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nType\nContent\nInstructor\nHandout\nFurther Reading\n\n\n\n\nSeminar\nAdvanced UNet and LLMs Introduction\nDr. Vinh\n231224-23M07-EC3\n\n\n\nSeminar\nSeminar: Multimodal Language Models\n\n231231-23M07-EC3\n\n\n\nSeminar\nSeminar: Visual Instruction Tuning (LlaVa) and QLoRA\n\n240107-23M07-EC3\n\n\n\nSeminar\nData Augmentation and Imbalanced Data\n\n240114-23M07-EC3\n\n\n\nSeminar\nRNN-based Forecasting and Toolformer\n\n240121-23M07-EC3",
    "crumbs": [
      "Resources",
      "Course content",
      "Module 7"
    ]
  },
  {
    "objectID": "papers.html",
    "href": "papers.html",
    "title": "AIO Course",
    "section": "",
    "text": "Back to top"
  },
  {
    "objectID": "contents/module3.html",
    "href": "contents/module3.html",
    "title": "AIO Course",
    "section": "",
    "text": "Back to top"
  },
  {
    "objectID": "contents/module1.html",
    "href": "contents/module1.html",
    "title": "Module 1 - Introduction to Python Programming",
    "section": "",
    "text": "This website is still under construction. Some contents and functions might temporarily not be available.."
  },
  {
    "objectID": "contents/module1.html#projects",
    "href": "contents/module1.html#projects",
    "title": "Module 1 - Introduction to Python Programming",
    "section": "Projects",
    "text": "Projects\nRegarding the projects, there are three projects where you will respectively learn how to use YOLOv8, an object detection model as well as how to use Python to manipulate and crawl data from a website. Finally, the last project is about developing simple applications using ChatGPT. In particular, the three projects are:\n\nObject Detection with YOLOv8\nData Manipulation and Crawling\nChatGPT Applications"
  },
  {
    "objectID": "contents/module1.html#competition-training",
    "href": "contents/module1.html#competition-training",
    "title": "Module 1 - Introduction to Python Programming",
    "section": "Competition Training",
    "text": "Competition Training\nAs for competition training, this module contains three lectures with the goal of teaching you the basic skills and knowledge you need before joining an AI competition, including visualizing data, knowledge about competition tasks and metrics, and design validation."
  },
  {
    "objectID": "contents/module1.html#extra-class",
    "href": "contents/module1.html#extra-class",
    "title": "Module 1 - Introduction to Python Programming",
    "section": "Extra class",
    "text": "Extra class\nThe central theme of the extra class for this module is about Algorithms and Complexity. In the age of AI, still, the knowledge about algorithms and their complexity including Big-O, Brute-force exhaustive, recursion, two pointer, and dynamic programming still plays an immensely important role."
  },
  {
    "objectID": "contents/module4.html",
    "href": "contents/module4.html",
    "title": "AIO Course",
    "section": "",
    "text": "Back to top"
  },
  {
    "objectID": "contents/module7.html",
    "href": "contents/module7.html",
    "title": "AIO Course",
    "section": "",
    "text": "Back to top"
  },
  {
    "objectID": "contents/module9.html",
    "href": "contents/module9.html",
    "title": "AIO Course",
    "section": "",
    "text": "Back to top"
  },
  {
    "objectID": "contents/module8.html",
    "href": "contents/module8.html",
    "title": "AIO Course",
    "section": "",
    "text": "Back to top"
  },
  {
    "objectID": "contents/module10.html",
    "href": "contents/module10.html",
    "title": "AIO Course",
    "section": "",
    "text": "Back to top"
  },
  {
    "objectID": "contents/module6.html",
    "href": "contents/module6.html",
    "title": "AIO Course",
    "section": "",
    "text": "Back to top"
  },
  {
    "objectID": "contents/module5.html",
    "href": "contents/module5.html",
    "title": "AIO Course",
    "section": "",
    "text": "Back to top"
  },
  {
    "objectID": "contents/module2.html",
    "href": "contents/module2.html",
    "title": "Module 2 - Calculus and Linear Algebra for AI",
    "section": "",
    "text": "Warning\n\n\nThis website is still under construction. Some contents and functions might temporarily not be available..\n\n\n\n\n\n Back to top"
  },
  {
    "objectID": "module3.html",
    "href": "module3.html",
    "title": "Module 3 - Probability and Statistics for AI",
    "section": "",
    "text": "Date\nType\nContent\nInstructor\nHandout\nFurther Reading\n\n\n\n\n18/07/23\nAssignment\nProbability Exercise\nDr. Đình Vinh\n\n\n\n\n19/07/23\nMain Lesson\nBasic Probability, Histogram and Image Enhancement\nDr. Vinh\n230719\n\n\n\n21/07/23\nMain Lesson\nNaive Bayes Classifier\nDr. Vinh\n230721\n\n\n\n23/07/23\nExercise Session\nTA-Exercise\nDr. Đình Vinh\n230723\n\n\n\n31/07/23\nAssignment\nStatistic Exercise\nDr. Đình Vinh\nSolution\n\n\n\n26/07/23\nMain Lesson\nBasic Statistics and Correlation Coefficient (Basic Tracking)\nDr. Vinh\n230726\n\n\n\n28/07/23\nMain Lesson\nTemplate Matching (Cosine Similarity vs. Correlation Coefficient)\nDr. Đình Vinh\n230728\n\n\n\n30/07/23\nExercise Session\nTA-Exercise\nDr. Đình Vinh\n230730\n\n\n\n31/07/23\nAssignment\nGenetic Algorithms and its Applications Exercise\nDr. Đình Vinh\n\n\n\n\n02/08/23\nMain Lesson\nRandomness and Genetic Algorithms\nDr. Vinh\n230802\n\n\n\n04/08/23\nMain Lesson\nGenetic Algorithms (Optimization and Linear Regression)\nDr. Vinh\n230804\n\n\n\n06/08/23\nExercise Session\nTA-Exercise\nDr. Đình Vinh\n230806\n\n\n\n07/08/23\nAssignment\nGenetic Algorithms and its Applications Exercise\nDr. Đình Vinh\n\n\n\n\n07/08/23\nExercise Session\nData Analysis Exercise\nTA Thắng\n[solution]\n\n\n\n09/08/23\nMain Lesson\nData visualization and Data Analysis (1)\nDr. Vinh\n230809\n[note]\n\n\n11/08/23\nMain Lesson\nData visualization and Data Analysis (2)\nDr. Vinh\n230811\n\n\n\n13/08/23\nExercise Session\nTA-Exercise (Polar)\nTA Thắng\n230813\n[Polar doc]\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nDate\nType\nContent\nInstructor\nHandout\nFurther Reading\n\n\n\n\n17/08/23\nAssignment\nBig Data Frameworks (1)\nDr. Đình Vinh\n\n\n\n\n17/08/23\nAssignment\nBig Data Frameworks (1)\nDr. Đình Vinh\n\n\n\n\n17/08/23\nProject tutorial - PT\nBig Data Frameworks (1)\nDr. Đình Vinh\n230818\n[LSFS and MapReduce]\n\n\n20/08/23\n\nBig Data Frameworks (2)\nDr. Đình Vinh\n230820\n\n\n\n22/08/23\n\nImage Data Project: Image Retrieval\nTA Thắng\n\n\n\n\n23/08/23\n\nBig Data Frameworks (3)\nDr. Đình Vinh\n230823\n[C9, Feng 2021]\n\n\n25/08/23\n\nImage Data Project: Image Retrieval\nTA Thắng\n230825\n[VIT paper (Dosovitskiy et al., 2021)]\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nDate\nType\nContent\nInstructor\nHandout\nFurther Reading\n\n\n\n\n29/07/23\nNLP Introduction\nIntroduction, Preprocessing\nTA Thái\n230729\n\n\n\n05/08/23\nNLP Introduction\nPreprocessing, Tokenization\nTA Thái\n230805\n[note] [C2, Jurafsky et al., 2023]\n\n\n12/08/23\nNLP Introduction\nStatistical Language Model\nTA Thái\n230812\n[C3, Jurafsky et al., 2023] [Andrej Karpathy’s Lecture]\n\n\n18/08/23\nNLP Introduction\nPart-of-Speech Tagging\nTA Thái\n230819\n[CA Jurafsky et al., 2023]\n\n\n26/08/23\nNLP Introduction\nConstituency Parsing\nTA Thái\n230826\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nDate\nType\nContent\nInstructor\nHandout\nFurther Reading\n\n\n\n\n18/07/23\nPython Support\nProbability\nTA Tiềm\n230718\n\n\n\n25/07/23\nPython Support\nStatistics\nTA Bảo\n230725\n\n\n\n01/08/23\nPython Support\nGenetic Algorithms\nTA Tiềm\n230801\n\n\n\n08/08/23\nPython Support\nPandas (1)\nTA Bảo\n230808\n\n\n\n14/08/23\nPython Support\nPandas (2)\nTA Bảo\n230815\n[C7, McKinney 2023]\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nDate\nType\nContent\nInstructor\nHandout\nFurther Reading\n\n\n\n\n20/07/23\nCompetition\nTricks to improve performance\nTA Hùng\n230720\n\n\n\n27/07/23\nCompetition\nHCM AI Challenge\nTA Hùng\n230727\n\n\n\n03/08/23\nCompetition\nWeb API + Docker\nTA Hùng\n230803\n\n\n\n17/08/23\nCompetition\nOnnx\nTA Hùng\n230817\n\n\n\n24/08/23\nCompetition\nSemi-supervised Learning\nTA Hùng\n230824\n[Lil’Log] [GitHub repo] [Laine et al., 2016] [Tarvainen et al., 2017]",
    "crumbs": [
      "Resources",
      "Course content",
      "Module 3"
    ]
  },
  {
    "objectID": "module4.html",
    "href": "module4.html",
    "title": "Module 4 - Machine Learning and Data Science",
    "section": "",
    "text": "Hi, welcome to Module 4: Machine Learning and Data Science of the AIO2023 course. The main lessons of this module involves around traditional machine learning algorithms, ranging from simple to complex one such as kNN, Decision tree, Random Forest, XGBoost, and Support Vector Machines (SVMs). There are two projects, including one where we’d work with tabular type of data for the problem of heart disease protection. The second project is on Object detection.\nAs of the extra class for this module, it aims is to provide you with the knowledge involving around Large Language Models (LLMs), from the most fundamental knowledge to more recent techniques such as Prompting and Fine-tuning.",
    "crumbs": [
      "Resources",
      "Course content",
      "Module 4"
    ]
  },
  {
    "objectID": "module9.html",
    "href": "module9.html",
    "title": "Module 9 - Generative Models",
    "section": "",
    "text": "Type\nContent\nInstructor\nHandout\nFurther Reading\n\n\n\n\nMain Lesson \n\n\n\n\n\n\nAssignment\nStyle Transfer Exercise\nTA Khoa\n\n\n\n\nMain Lesson\nBasic Style Transfer\nDr. Vinh\n240306-23M09-MC\n\n\n\nMain Lesson\nMultimodal Style Transfer\nDr. Vinh\n240308-23M09-MC\n\n\n\nExercise Session\nTA Exercise\nTA Khoa\n240310-23M09-MC\n\n\n\nExercise Session\nTA Exercise\nTA Thái\n240317-23M09-MC\n\n\n\nMain Lesson\nGAN and DCGAN\nDr. Vinh\n240320-23M09-MC\n\n\n\nMain Lesson\nPix2Pix and CycleGAN\nDr. Vinh\n240322-23M09-MC\n\n\n\nExercise Session\nText to Image Synthesis with DCGAN\nTA Thái\n240324-23M09-MC\n\n\n\nAssignment\nImage Inpainting with DDPMs\nTA Thái\n[solution]\n\n\n\nMain Lesson\nDiffusion Models (1)\nDr. Đình Vinh\n240327-23M09-MC\nTutorial on Diffusion Models for Imaging and Vision\n\n\nMain Lesson\nDiffusion Models (2)\nDr. Đình Vinh\n240329-23M09-MC\n\n\n\nExercise Session\nDiffusion-based Image Inpainting\nTA Thái\n240331-23M09-MC\n\n\n\nMain Lesson\nProject: VAE-based Image Colorization\nDr. Tài\n240403-23M09-MC\n\n\n\nProject tutorial - PT\nProject: Diffusion-based Image Colorization\nDr. Tài\n240407-23M09-MC\n\n\n\nAssignment\nText to Image Synthesis with Stable Diffusion (and CLIP)\nTA Thái\n\n\n\n\nMain Lesson\nStable Diffusion\nDr. Đình Vinh\n240410-23M09-MC\n\n\n\nMain Lesson\nIntroduction to OpenAI’s Sora\nDr. Đình Vinh\n240412-23M09-MC\n\n\n\nProject tutorial - PT\nText to Image Synthesis with Stable Diffusion (and CLIP)\nTA Thái\n240414-23M09-MC\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nType\nContent\nInstructor\nHandout\nFurther Reading\n\n\n\n\nLLMs\nTraining a minChatGPT\nTA Thái\n240406-23M10-EC1\n\n\n\nLLMs\nLLM Finetuning for Math Solver\nTA Thắng\n240413-23M10-EC1\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nType\nContent\nInstructor\nHandout\nFurther Reading\n\n\n\n\nMLOps\nModel Pruning\nTA Bách\n240309-23M09-EC1\n\n\n\nMLOps\nMobile Deployment\nDr. Đình Vinh\n240316-23M09-EC1\n\n\n\nMLOps\nWeb Deployment\nDr. Đình Vinh\n240323-23M09-EC1\n\n\n\nMLOps\nDeployment as a Service (API)\nTA Thắng\n240330-23M09-EC1\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nType\nContent\nInstructor\nHandout\nFurther Reading\n\n\n\n\nSeminar\nSeminar: XAI\nDr. Anh Nguyen\n240405-23M09-MC",
    "crumbs": [
      "Resources",
      "Course content",
      "Module 9"
    ]
  },
  {
    "objectID": "module8.html",
    "href": "module8.html",
    "title": "Module 8 - Applications of Deep Learning for NLP",
    "section": "",
    "text": "Type\nContent\nInstructor\nHandout\nFurther Reading\n\n\n\n\nMain lesson\n\n\n\n\n\n\nAssignment\nPOS and Medical NER Exercise\nTA Thái\n\n\n\n\nPreview\nIntroduction to POS Tagging\nTA Khoa\n240130-23M08-EC2\n\n\n\nMain Lesson\nFrom Text Classification to POS Tagging\nDr. Vinh\n240131-23M08MC\n\n\n\nMain Lesson\nNER for Medical Data\nDr. Vinh\n240202-23M08MC\n\n\n\nExercise Session - ES\nTA-Exercise\nTA Thái\n240204-23M08MC\n\n\n\nProject handout\nAspect-based Sentiment Analysis Project\nTA Thái\n240206-23M08MC\n\n\n\nProject tutorial\nText Project: Aspect Extraction and Content Classification (Text Classification + NER)\nTA Thái\n240206-23M08MC\n\n\n\nProject tutorial\nText Project: QA for Content Inquiry (Text Classification + NER)\nTA Thắng\n240216-23M08MC\n\n\n\nProject tutorial\nText Project: End-to-end Question Answering (Building a Searching System)\nTA Thắng\n240218-23M08MC\n\n\n\nMain Lesson\nText Generation\nDr. Vinh\n240221-23M08MC\n\n\n\nMain Lesson\nMachine Translation\nDr. Vinh\n240223-23M08MC\n\n\n\nExercise Session - ES\nTA Exercise (Neural Machine Translation)\nTA Thái\n240225-23M08MC\n\n\n\nProject handout\nNeural Machine Translation\nTA Thái\n[handout]\n\n\n\nProject tutorial\nPoem Generation Project\nTA Thắng\n240228-23M08MC\n\n\n\nProject tutorial\nLow-resource Machine Translation Project\nTA Thái\n240301-23M08MC\n\n\n\nProject tutorial\nText classification with Mamba Project\nTA Đức\n\n\n\n\nProject handout\nText Project: Poem Generation\nTA Thái\n240228-23M08MC\n\n\n\nProject handout\nText Project: Low-resource Machine Translation\nTA Thắng\n240301-23M08MC\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nType\nContent\nInstructor\nHandout\nFurther Reading\n\n\n\n\nResearch and Paper\nHow to write a paper (3)\nDr. Đình Vinh\n240127-23M08-EC1\n\n\n\nResearch and Paper\nHow to write a paper (4)\nDr. Đình Vinh\n240203-23M08-EC1\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nType\nContent\nInstructor\nHandout\nFurther Reading\n\n\n\n\nMLOps\nModel Quantization\nTA Bách\n240302-23M09-EC1\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nType\nContent\nInstructor\nHandout\nFurther Reading\n\n\n\n\nSeminar\nCLIP\nTA Đức\n240128-23M08-EC3\n\n\n\nSeminar\nDetecting violation of helmet rule for motorcyclists (CVPRW2024)\nDr. Vinh\n240204-23M08-EC3\n\n\n\nSeminar\nDirect Preference Optimization\nDr. Vinh\n240225-23M08-EC3\n\n\n\nSeminar\nStudy and Job in the USA\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nType\nContent\nInstructor\nHandout\nFurther Reading\n\n\n\n\nLLMs\nIntroduction to Text Generation\nDr. Vinh\n240220-23M08-EC2",
    "crumbs": [
      "Resources",
      "Course content",
      "Module 8"
    ]
  },
  {
    "objectID": "module10.html",
    "href": "module10.html",
    "title": "Module 10 - Reinforcement Learning, GNN, and LLMs",
    "section": "",
    "text": "Type\nContent\nInstructor\nHandout\nFurther Reading\n\n\n\n\nMain Lesson\nAssignment: Reinforcement Learning Exercise\nTA Thắng\n\n\n\n\nPreview\nIntroduction to Reinforcement Learning\nTA Thuận\n240416-23M10-EC2\n\n\n\nMain Lesson\nReinforcement Learning (CartPole)\nDr. Hoàng\n240417-23M10-MC\n\n\n\nMain Lesson\nReinforcement Learning (Deep Deterministic Policy Gradient)\nDr. Hoàng\n240419-23M10-MC\n\n\n\nExercise\nTA Exercise\nTA Thắng\n240421-23M10-MC\n\n\n\nAssignment\nPoint Cloud Techniques and Applications Project\nDr. Tuấn\n\n\n\n\nAssignment\nMultimodal Large Language Models Exercise\nTA Thái\n\n\n\n\nMain Lesson\nClassification for 3D Point Cloud Data\nDr. Tuấn\n240424-23M10-MC\n\n\n\nMain Lesson\nAdvances in 3D Point Cloud Data\nDr. Tuấn\n240426-23M10-MC\n\n\n\nMain Lesson\nGNN Node Classification\nDr. Đình Vinh\n240428-23M10-MC\n\n\n\nMain Lesson\nGNN (Molecular Property Prediction)\nDr. Đình Vinh\n240501-23M10-MC\n\n\n\nExercise\nTA Exercise\nTA Đức\n240503-23M10-MC\n\n\n\nProject PT\nMultitasking networks for Vision\nTA Thái\n240505-23M10-MC\n[Exercise]\n\n\nProject PT\nVideoCLIP for Video Classification\nTA Đức\n240508-23M10-MC\n\n\n\nProject PT\nMulti-agent LLM\nTA Thắng\n240510-23M10-MC\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nType\nContent\nInstructor\nHandout\nFurther Reading\n\n\n\n\nExtra class\nLLM RAG for Applications\nTA Bách\n240421-23M10-MC\n\n\n\nExtra class\nLLMs for Multimodal Data\nTA Thái\n240427-23M10-EC1\n[Modaverse] [BLIP-2] [NExT-GPT]\n\n\nExtra class\nLLM Deployment with LangChain\nTA Thắng\n240504-23M10-EC1",
    "crumbs": [
      "Resources",
      "Course content",
      "Module 10"
    ]
  },
  {
    "objectID": "module6.html",
    "href": "module6.html",
    "title": "Module 6 - Introduction to Deep Learning",
    "section": "",
    "text": "Date\nType\nContent\nInstructor\nHandout\nFurther Reading\n\n\n\n\n06/11/23\nAssignment\nConvolutional Neural Network Exercise\nTA Thái\n[solution]\n\n\n\n08/11/23\nMain Lesson\nBasic CNN (1)\nDr. Vinh\n231108\n[CS231N Note] [CNN Explainer]\n\n\n10/11/23\nMain Lesson\nBasic CNN (2)\nDr. Vinh\n231110\n\n\n\n12/11/23\nExercise Session\nTA-Exercise\nTA Thắng\n231112\n\n\n\n13/11/23\nAssignment\nConvolutional Neural Network Exercise\nTA Thắng\n[solution]\n\n\n\n15/11/23\nMain Lesson\nCNN Training\nDr. Vinh\n231115\n[VGG Paper]\n\n\n17/11/23\nMain Lesson\nCNN Generalization\nDr. Vinh\n231117\n\n\n\n19/11/23\nExercise Session\nTA-Exercise\nTA Thắng\n231119\n\n\n\n20/11/23\nAssignment\nPretrained Models for Image Exercise\nTA Thắng\n[solution]\n\n\n\n22/11/23\nMain Lesson\nAdvanced CNN Architecture\nDr. Vinh\n231122\n\n\n\n24/11/23\nMain Lesson\nTransfer Learning for CNN\nDr. Vinh\n231124\n\n\n\n26/11/23\nExercise Session\nTA-Exercise\nTA Thái\n231126\n\n\n\n27/11/23\nAssignment\nRecurrent Neural Network Exercise\nTA Thắng\n\n\n\n\n29/11/23\nMain Lesson\nBasic RNN/LSTM (Time series and Text)\nDr. Vinh\n231129\n\n\n\n01/12/23\nMain Lesson\nBasic RNN/LSTM (Time series and Text)\nDr. Vinh\n231201\n\n\n\n03/12/23\nExercise Session\nTA-Exercise\nTA Thắng\n231203\n\n\n\n04/12/23\nAssignment\nTransformer Application Exercise\n\n\n\n\n\n06/12/23\nMain Lesson\nTransformer (Encoder - Text Classification)\nDr. Vinh\n231206\nBERT Readings\n\n\n08/12/23\nMain Lesson\nTransformer for Image and Time series Data\nDr. Vinh\n231208\n\n\n\n13/12/23\nProject Tutorial\nImage Project: OCR with YOLOv8 and CNN (Scene Text Recognition)\nTA Thắng\n231213\n\n\n\n15/12/23\nProject Tutorial\nImage-text Project: VQA\nTA Thắng\n231215\n\n\n\n17/12/23\nProject Tutorial\nTime-series forecasting project\nDr. Đình Vinh\n231217\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nDate\nType\nContent\nInstructor\nHandout\nFurther Reading\n\n\n\n\n09/11/23\nCompetition\nZalo AI\nTA Hùng\n231109\n\n\n\n16/11/23\nCompetition\nZalo AI\n\n231116\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nDate\nType\nContent\nInstructor\nHandout\nFurther Reading\n\n\n\n\n07/11/23\nPreview\nIntroduction to CNN\nTA Thái\n231107\n\n\n\n14/11/23\nPreview\nAdvanced CNN\n\n231114\n\n\n\n21/11/23\nPreview\nCNN and its variants\n\n231121\n\n\n\n28/11/23\nPreview\nIntroduction to Transfer Learning\n\n231128\n\n\n\n05/12/23\nPreview\nIntroduction to Transformer\n\n231205\nGenerative AI exists because of the transformer\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nDate\nType\nContent\nInstructor\nHandout\nFurther Reading\n\n\n\n\n11/11/23\nResearch & Paper\nResearch Idea - Brainstorming (1)\nDr. Đình Vinh\n231111\n\n\n\n18/11/23\nResearch & Paper\nResearch Idea - Brainstorming (2)\n\n231118\n\n\n\n25/11/23\nResearch & Paper\nHow to do Research (1)\n\n231125\n\n\n\n02/12/23\nResearch & Paper\nHow to do Research (2)\n\n231202\n\n\n\n09/12/23\nResearch & Paper\nHow to do Research (3)\n\n231209\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nDate\nType\nContent\nInstructor\nHandout\nFurther Reading\n\n\n\n\n10/12/23\nSeminar\nTransformers for Time series data\nDr. Vinh\n231210-1\n\n\n\n17/12/23\nSeminar\nScholarship and Feature Extraction in Time Series Data",
    "crumbs": [
      "Resources",
      "Course content",
      "Module 6"
    ]
  },
  {
    "objectID": "module5.html",
    "href": "module5.html",
    "title": "Module 5 - Deep Learning Prerequisites",
    "section": "",
    "text": "Date\nType\nContent\nInstructor\nHandout\nFurther Reading\n\n\n\n\n02/10/23\nAssignment\nLogistic Regression Exercise\nTA Thắng\n[solution]\n\n\n\n04/10/23\nMain Lesson\nFrom Linear Regression to Logistic Regression\nDr. Vinh\n231004\n\n\n\n06/10/23\nMain Lesson\nLogistic Regression - Vectorization and Applications\nDr. Vinh\n231006\n[Jurafsky et al., C5, 2023]\n\n\n08/10/23\nExercise Session\nTA Exercise\nTA Thắng\n231008\n\n\n\n09/10/23\nAssignment\nSoftmax Regression Exercise\nTA Thắng\n[solution]\n\n\n\n11/10/23\nMain Lesson\nSoftmax Regression (Multiclass Classification)\nDr. Vinh\n231011\n\n\n\n13/10/23\nMain Lesson\nPytorch Framework (Implementation for regression)\nDr. Vinh\n231013\n\n\n\n15/10/23\nExercise Session\nTA Exercise\nTA Khoa\n231015\n\n\n\n16/10/23\nAssignment\nMLP Exercise\nTA Thắng\n[Exercise]\n\n\n\n18/10/23\nMain Lesson\nMultilayer Perceptron\nDr. Vinh\n231018\n\n\n\n20/10/23\nMain Lesson\nActivations and Initializers\nDr. Vinh\n231020\n\n\n\n22/10/23\nExercise Session\nTA Exercise\nTA Khoa\n231022\n\n\n\n25/10/23\nMain Lesson\nOptimizers for Neural Network (1)\nDr. Vinh\n231025\n\n\n\n27/10/23\nMain Lesson\nOptimizers for Neural Networks (2)\nDr. Vinh\n231027\n\n\n\n29/10/23\nExercise Session\nTA-Exercise\nTA Khoa\n231029\n\n\n\n01/11/23\nProject Tutorial\nText data: Sentiment Analysis\nTA Thái\n231101\n\n\n\n03/11/23\nProject Tutorial\nTime-series Data Project: Music Genre Classification\nTA Bảo\n231103\n\n\n\n05/11/23\nProject Tutorial\nImage Data Project: Gradient Vanishing in MLP\nTA Khoa\n231105\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nDate\nType\nContent\nInstructor\nHandout\nFurther Reading\n\n\n\n\n03/10/23\nPreview\nLogistic Regression\nTA Thái\n231003\n\n\n\n10/10/23\nPreview\nSoftmax Regression\n\n231010\n\n\n\n17/10/23\nPreview\nMultilayer Perceptron\n\n231017\n\n\n\n24/10/23\nPreview\nSGD+Momentum\n\n231024\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nDate\nType\nContent\nInstructor\nHandout\nFurther Reading\n\n\n\n\n07/10/23\nComputer Vision\nIntroduction to CV and Background subtraction\nDr. Đình Vinh\n231007\n\n\n\n14/10/23\nComputer Vision\nLane Detection\n\n231014\n\n\n\n21/10/23\nComputer Vision\nImage Stitching (panorama)\n\n231021\n\n\n\n28/10/23\nComputer Vision\nFace Detection\n\n231028\n\n\n\n04/11/23\nComputer Vision\nObject Tracking using Mean Shift/Cam Shift\n\n231104\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nDate\nType\nContent\nInstructor\nHandout\nFurther Reading\n\n\n\n\n05/10/23\nCompetition\nIntroduction to Imbalance Data\nTA Hùng\n231005\n\n\n\n12/10/23\nCompetition\nModel Evaluation\n\n231012\n\n\n\n19/10/23\nCompetition\nKalapa Challenge\n\n231019\n\n\n\n02/11/23\nCompetition\nData Sampling\n\n231102",
    "crumbs": [
      "Resources",
      "Course content",
      "Module 5"
    ]
  },
  {
    "objectID": "module2.html#main-lessons",
    "href": "module2.html#main-lessons",
    "title": "Module 2 - Calculus and Linear Algebra for AI",
    "section": "Main Lessons",
    "text": "Main Lessons\n\n\n\n\n\n\n\n\n\n\n\nDate\nType\nContent\nInstructor\nHandout\nFurther Reading\n\n\n\n\n30/05/23\nAssignment\nInterpolation and Application Exercises\nDr. Đình Vinh\n[solution]\n\n\n\n31/05/23\nMain Lesson\nConstruct loss functions with different targets\nDr. Vinh\n230531 - M02ML01\n\n\n\n02/06/23\nMain Lesson\nInterpolation and Image upsampling\n\n230602 - M02ML02\n\n\n\n04/06/23\nExercise Session\nTA Exercise\nDr. Đình Vinh\n230604 - M02ES01\n\n\n\n05/06/23\nAssignment\nCalculus Exercise\n\n[solution]\n\n\n\n07/06/23\nMain Lesson\nUnderstanding Derivative and Gradient\nDr. Vinh\n230607 - M02ML03\n\n\n\n09/06/23\nMain Lesson\nEdge Detection and Gradient-based Optimization\n\n230609 - M02ML04\n\n\n\n11/06/23\nExercise Session\nTA Exercise\nDr. Đình Vinh\n230611 - M02ES02\n\n\n\n14/06/23\nMain Lesson\nLinear Regression (1)\nDr. Vinh\n230614 - M02ML05\n\n\n\n16/06/23\nMain Lesson\nLinear Regression (2)\n\n230616 - M02ML06\n\n\n\n18/06/23\nExercise Session\nTA Exercise\nDr. Đình Vinh\n230618 - M02ES03\n\n\n\n21/06/23\nMain Lesson\nVectorization for Linear Regression (1)\nDr. Vinh\n230621 - M02ML07\n\n\n\n23/06/23\nMain Lesson\nVectorization for Linear Regression (2)\n\n230623 - M02ML08\n\n\n\n25/06/23\nExercise Session\nTA Exercise\nDr. Đình Vinh\n230625 - M02ES04\n\n\n\n26/06/23\nAssignment\nCosine Background subtraction Exercise\n\n[solution]\n\n\n\n28/06/23\nMain Lesson\nBasic Linear Algebra and Its Applications\nDr. Vinh\n230628 - M02ML09\n\n\n\n30/06/23\nMain Lesson\nCosine Similarity and Decomposition\n\n230630 - M02ML10\n\n\n\n02/07/23\nExercise Session\nTA Exercise\nDr. Đình Vinh\n230702 - M02ES05",
    "crumbs": [
      "Resources",
      "Course content",
      "Module 2"
    ]
  },
  {
    "objectID": "module2.html#project-tutorials",
    "href": "module2.html#project-tutorials",
    "title": "Module 2 - Calculus and Linear Algebra for AI",
    "section": "Project Tutorials",
    "text": "Project Tutorials\n\n\n\n\n\n\n\n\n\n\n\nDate\nType\nContent\nInstructor\nHandout\nFurther Reading\n\n\n\n\n09/07/23\nProject Tutorial - PT\nSVD and its Applications\nDr. Đình Vinh\n230709 - M02ET01\n\n\n\n10/07/23\nProject handout\nImage Project: Depth Information Reconstruction\nTA Thắng\n[proposal]\n\n\n\n10/07/23\nProject handout\nText Project: Text Retrieval (using Pretrained Embedding)\nTA Thắng\n[solution]\n\n\n\n10/07/23\nProject handout\nTabular Data Project: Sales Prediction (Linear and non-linear regression)\nTA Thái\n[proposal]\n\n\n\n12/07/23\nProject Tutorial - PT\nTabular Data Project: Sales Prediction (Linear and non-linear regression)\nTA Thái\n230716 - M02PT03\n\n\n\n14/07/23\nProject Tutorial - PT\nText Project: Text Retrieval (using Pretrained Embedding)\nTA Thắng\n230714 - M02PT02\n\n\n\n16/07/23\nProject Tutorial - PT\nImage Project: Depth Information Reconstruction\nTA Thắng\n230712 - M02PT01",
    "crumbs": [
      "Resources",
      "Course content",
      "Module 2"
    ]
  },
  {
    "objectID": "module2.html#python-tutorial-sessions",
    "href": "module2.html#python-tutorial-sessions",
    "title": "Module 2 - Calculus and Linear Algebra for AI",
    "section": "Python Tutorial Sessions",
    "text": "Python Tutorial Sessions\n\n\n\n\n\n\n\n\n\n\n\nDate\nType\nContent\nInstructor\nHandout\nFurther Reading\n\n\n\n\n30/05/23\nPython Support\nList for 2D Data\nTA Tiềm\n230530 - M02CR02\n\n\n\n06/06/23\nPython Support\nList for 3D data\n\n230606 - M02CR03\n\n\n\n13/06/23\nPython Support\nArray 1D using NumPy\n\n230613 - M02CR04\n\n\n\n20/06/23\nPython Support\nArray 2D and 3D using NumPy\n\n230620 - M02CR05\n\n\n\n27/06/23\nPython Support\nBasic Numpy\n\n230627 - M02CR06",
    "crumbs": [
      "Resources",
      "Course content",
      "Module 2"
    ]
  },
  {
    "objectID": "module2.html#extra-classes-database-and-sql",
    "href": "module2.html#extra-classes-database-and-sql",
    "title": "Module 2 - Calculus and Linear Algebra for AI",
    "section": "Extra Classes: Database and SQL",
    "text": "Extra Classes: Database and SQL\n\n\n\n\n\n\n\n\n\n\n\nDate\nType\nContent\nInstructor\nHandout\nFurther Reading\n\n\n\n\n03/06/23\nDatabase and SQL\nDatabase-SQL (1)\nTA Bảo\n230603 - M02EC01\n\n\n\n10/06/23\nDatabase and SQL\nDatabase-SQL (2)\nTA Bảo\n230611 - M02ES02\n\n\n\n17/06/23\nDatabase and SQL\nDatabase-SQL (3)\nTA Bảo\n230617 - M02EC03\n\n\n\n24/06/23\nNoSQL\nDatabase-NoSQL\nTA Thái\n230624 - M02EC04\n\n\n\n01/07/23\nNoSQL\nDatabase-NoSQL (2)\nTA Thái\n230701 - M02EC05\n\n\n\n08/07/23\nBig Data\nSQL for Big Data\nTA Thắng\n230708 - M02EC06",
    "crumbs": [
      "Resources",
      "Course content",
      "Module 2"
    ]
  },
  {
    "objectID": "module2.html#competition-training",
    "href": "module2.html#competition-training",
    "title": "Module 2 - Calculus and Linear Algebra for AI",
    "section": "Competition Training",
    "text": "Competition Training\n\n\n\n\n\n\n\n\n\n\n\nDate\nType\nContent\nInstructor\nHandout\nFurther Reading\n\n\n\n\n01/06/23\nCompetition\nHyper-parameter optimization\nTA Hùng\n230601 - M02AC01\n\n\n\n08/06/23\nCompetition\nEnsembling (Voting, Averaging, and Stacked Generalization)\nTA Hùng\n230608 - M02AC02\n\n\n\n15/06/23\nCompetition\nAugmentation Strategies and Albumentation\nTA Hùn\n230615 - M02AC03\n\n\n\n22/06/23\nCompetition\nNestquant\nTA Hùng\n230622 - M02AC04\n\n\n\n29/06/23\nCompetition\nOOD-CV\nTA Hùng\n230629 - M02AC05\n\n\n\n06/07/23\nCompetition\nVision Transformer\nTA Hùng\n230706 - M02AC06",
    "crumbs": [
      "Resources",
      "Course content",
      "Module 2"
    ]
  },
  {
    "objectID": "module2.html#summer-school-on-deep-learning",
    "href": "module2.html#summer-school-on-deep-learning",
    "title": "Module 2 - Calculus and Linear Algebra for AI",
    "section": "Summer School on Deep Learning",
    "text": "Summer School on Deep Learning\n\n\n\n\n\n\n\n\n\n\n\nDate\nType\nContent\nInstructor\nHandout\nFurther Reading\n\n\n\n\n05/07/23\nSummer school on Deep Learning\nVIASM Summer school on Recent Advances in Deep Learning\n\nSildes",
    "crumbs": [
      "Resources",
      "Course content",
      "Module 2"
    ]
  },
  {
    "objectID": "misc/working-with-text.html",
    "href": "misc/working-with-text.html",
    "title": "Working with Text Data",
    "section": "",
    "text": "Back to top"
  },
  {
    "objectID": "misc/working-with-text/working-with-text.html",
    "href": "misc/working-with-text/working-with-text.html",
    "title": "Working with Text Data",
    "section": "",
    "text": "Code\nwith open(\"the-verdict.txt\",\"r\", encoding=\"utf-8\") as f:\n    raw_text = f.read()\nprint(f\"Total number of character: {len(raw_text)}\")\n\n\nTotal number of character: 20479\n\n\n\n\n\n Back to top"
  },
  {
    "objectID": "misc/working-with-text/SimpleTokenizer.html",
    "href": "misc/working-with-text/SimpleTokenizer.html",
    "title": "Converting tokens into token IDs",
    "section": "",
    "text": "with open(\"the-verdict.txt\",\"r\", encoding=\"utf-8\") as f:\n    raw_text = f.read()\nprint(f\"Total number of character: {len(raw_text)}\")\n\nTotal number of character: 20479\n\n\n\nprint(raw_text[:99])\n\nI HAD always thought Jack Gisburn rather a cheap genius--though a good fellow enough--so it was no \n\n\n\n# Simplest tokenization using regex\n\nimport re\ntext = \"Hello world, I am learning AI.\"\n\n# tokenization scheme 1\nresult = re.split(r'(\\s)',text)\nprint(result)\n\n['Hello', ' ', 'world,', ' ', 'I', ' ', 'am', ' ', 'learning', ' ', 'AI.']\n\n\n\n# tokenization scheme 2\nresult = re.split(r'[,.]|\\s',text)\nprint(result)\n\n['Hello', 'world', '', 'I', 'am', 'learning', 'AI', '']\n\n\n\n# tokenization scheme 3: removing whitespaces\nresult = [item for item in result if item.strip()]\nprint(result)\n\n['Hello', 'world', 'I', 'am', 'learning', 'AI']\n\n\n\ntext = \"Hello, world. Is this-- a test?\"\nresult = re.split(r'([,.:;?_!\"()\\']|--|\\s)', text)\nresult = [item.strip() for item in result if item.strip()]\nprint(result)\n\n['Hello', ',', 'world', '.', 'Is', 'this', '--', 'a', 'test', '?']\n\n\n\n# apply the basic tokenizer to the initial raw text\npreprocessed = re.split(r'([,.?_!\"()\\']|--|\\s)', raw_text)\npreprocessed = [item.strip() for item in preprocessed if item.strip()]\nprint(len(preprocessed))\n\n4649\n\n\n\nprint(preprocessed[:30])\n\n['I', 'HAD', 'always', 'thought', 'Jack', 'Gisburn', 'rather', 'a', 'cheap', 'genius', '--', 'though', 'a', 'good', 'fellow', 'enough', '--', 'so', 'it', 'was', 'no', 'great', 'surprise', 'to', 'me', 'to', 'hear', 'that', ',', 'in']\n\n\n\n# convert from Python string to integers (token IDs)\n\n# create a list of unique tokens and sort aphabetically to create vocabulary\nall_words = sorted(set(preprocessed))\nvocab_size = len(all_words)\nprint(vocab_size)\n\n1159\n\n\n\nvocab = {token:integer for integer, token in enumerate(all_words)}\nfor i, item in enumerate(vocab.items()):\n    print(item)\n    if i&gt;20:\n        break\n\n('!', 0)\n('\"', 1)\n(\"'\", 2)\n('(', 3)\n(')', 4)\n(',', 5)\n('--', 6)\n('.', 7)\n(':', 8)\n(';', 9)\n('?', 10)\n('A', 11)\n('Ah', 12)\n('Among', 13)\n('And', 14)\n('Are', 15)\n('Arrt', 16)\n('As', 17)\n('At', 18)\n('Be', 19)\n('Begin', 20)\n('Burlington', 21)\n\n\n\n# implementing a simple tokenizer\n\nclass SimpleTokenizerv1:\n    def __init__(self,vocab):\n        self.str_to_int = vocab\n        self.int_to_str = {i:s for s,i in vocab.items()}\n\n    def encode(self,text):\n        preprocessed = re.split(r'([,.?_!\"()\\']|--|\\s)', text)\n        preprocessed = [item.strip() for item in preprocessed if item.strip()]\n        ids = [self.str_to_int[s] for s in preprocessed]\n        return ids\n\n    def decode(self,ids):\n        text = \" \".join([self.int_to_str[i] for i in ids])\n        text = re.sub(r'\\s+([,.?!\"()\\'])', r'\\1', text)\n        return text\n\n\ntokenizer = SimpleTokenizerv1(vocab)\ntext = \"\"\"\"It's the last he painted, you know,\" \n           Mrs. Gisburn said with pardonable pride.\"\"\"\nids = tokenizer.encode(text)\nprint(ids)\n\n[1, 58, 2, 872, 1013, 615, 541, 763, 5, 1155, 608, 5, 1, 69, 7, 39, 873, 1136, 773, 812, 7]\n\n\n\ntokenizer.decode(ids)\n\n'\" It\\' s the last he painted, you know,\" Mrs. Gisburn said with pardonable pride.'\n\n\n\ntokenizer.decode(tokenizer.encode(text))\n\n'\" It\\' s the last he painted, you know,\" Mrs. Gisburn said with pardonable pride.'\n\n\n\n# problem with unknown words (words that are not in the vocabulary)\n\n# text = \"I like AI.\"\n#tokenizer.encode(text) # throws a KeyError\n\n\nAdding speical context tokens\n\n# modifying the tokenizer to handle unknown words (could be an exercise)\n\nall_tokens = sorted(list(set(preprocessed)))\nall_tokens.extend([\"&lt;|endoftext|&gt;\",\"&lt;|unk|&gt;\"])\nvocab = {token:integer for integer,token in enumerate(all_tokens)}\nprint(len(vocab.items()))\n\n1161\n\n\n\nfor i, item in enumerate(list(vocab.items())[-5:]):\n    print(item)\n\n('younger', 1156)\n('your', 1157)\n('yourself', 1158)\n('&lt;|endoftext|&gt;', 1159)\n('&lt;|unk|&gt;', 1160)\n\n\n\nclass SimpleTokenizerv2:\n    def __init__(self,vocab):\n        self.str_to_int = vocab\n        self.int_to_str = {i:s for s,i in vocab.items()}\n\n    def encode(self,str):\n        preprocessed = re.split(r'([,.?_!\"()\\']|--|\\s)', text)\n        preprocessed = [item.strip() for item in preprocessed if item.strip()]\n        preprocessed = [item if item in self.str_to_int else \"&lt;|unk|&gt;\" for item in preprocessed]\n        ids = [self.str_to_int[s] for s in preprocessed]\n        return ids\n\n    def decode(self,ids):\n        text = \" \".join([self.int_to_str[i] for i in ids])\n        text = re.sub(r'\\s+([,.?!\"()\\'])', r'\\1', text)\n        return text\n\n\ntext1 = \"Hello world. I am learning AI.\"\ntext2 = \"In the sunlit terraces of the palace.\"\ntext = \" &lt;|endoftext|&gt; \".join((text1,text2))\nprint(text)\n\nHello world. I am learning AI. &lt;|endoftext|&gt; In the sunlit terraces of the palace.\n\n\n\ntokenizer = SimpleTokenizerv2(vocab)\nprint(tokenizer.encode(text))\n\n[1160, 1160, 7, 55, 155, 1160, 1160, 7, 1159, 57, 1013, 981, 1009, 738, 1013, 1160, 7]\n\n\n\nprint(tokenizer.decode(tokenizer.encode(text)))\n\n&lt;|unk|&gt; &lt;|unk|&gt;. I am &lt;|unk|&gt; &lt;|unk|&gt;. &lt;|endoftext|&gt; In the sunlit terraces of the &lt;|unk|&gt;.\n\n\n\n\n\n\n Back to top"
  },
  {
    "objectID": "misc/working-with-text/misc.html",
    "href": "misc/working-with-text/misc.html",
    "title": "Misc",
    "section": "",
    "text": "Simple Tokenizer\n\n\n\n\n Back to top"
  },
  {
    "objectID": "resources/resources.html",
    "href": "resources/resources.html",
    "title": "Resources",
    "section": "",
    "text": "Books\nBlogs\nCourses\nFundamental Papers\nGitHub repository\nResearch and Paper Writing / Graduate life\n\n\n\n\n Back to top",
    "crumbs": [
      "Misc",
      "Others",
      "Resources"
    ]
  },
  {
    "objectID": "resources/papers.html",
    "href": "resources/papers.html",
    "title": "Papers",
    "section": "",
    "text": "sorted in random order\n\nWord2Vec | Mikolov, T., Chen, K., Corrado, G., & Dean, J. (2013). Efficient Estimation of Word Representations in Vector Space. International Conference on Learning Representations.\nBERT | Devlin, J., Chang, M.-W., Lee, K., & Toutanova, K. (2019). BERT: Pre-training of Deep Bidirectional Transformers for Language Understanding. Proceedings of the 2019 Conference of the North, 4171–4186.\nTransformer | Vaswani, A., Shazeer, N. M., Parmar, N., Uszkoreit, J., Jones, L., Gomez, A. N., Kaiser, L., & Polosukhin, I. (2017).Attention is All you Need. Neural Information Processing Systems, 5998–6008.\nCLIP | Radford, A., Kim, J. W., Hallacy, C., Ramesh, A., Goh, G., Agarwal, S., Sastry, G., Askell, A., Mishkin, P., Clark, J., Krueger, G., & Sutskever, I. (2021). Learning Transferable Visual Models From Natural Language Supervision. International Conference on Machine Learning, 8748–8763.\nAlexNet | Krizhevsky, A., Sutskever, I., & Hinton, G. E. (2012). ImageNet Classification with Deep Convolutional Neural Networks. Conference on Neural Information Processing Systems (NeurIPS), 1106–1114.\n\n\n\n\n Back to top"
  },
  {
    "objectID": "resources/books.html",
    "href": "resources/books.html",
    "title": "Books and other preprint materials",
    "section": "",
    "text": "“I do believe something very magical can happen when you read a good book.” - J.K. Rowling"
  },
  {
    "objectID": "resources/books.html#probability-and-statistics",
    "href": "resources/books.html#probability-and-statistics",
    "title": "Books and other preprint materials",
    "section": "Probability and Statistics",
    "text": "Probability and Statistics\n\nIntroduction to Probability for Data Science by Stanley H. Chan\nWasserman, L. (2013). All of statistics: a concise course in statistical inference. Springer Science & Business Media."
  },
  {
    "objectID": "resources/books.html#others",
    "href": "resources/books.html#others",
    "title": "Books and other preprint materials",
    "section": "Others",
    "text": "Others\n\nMathematics for Machine Learning by Garret Thomas"
  },
  {
    "objectID": "resources/books.html#nlp",
    "href": "resources/books.html#nlp",
    "title": "Books and other preprint materials",
    "section": "NLP",
    "text": "NLP\n\nSpeech and Language Processing (3rd ed. draft) by Dan Jurafsky and James H. Martin"
  },
  {
    "objectID": "resources/courses.html#nlp",
    "href": "resources/courses.html#nlp",
    "title": "Courses",
    "section": "NLP",
    "text": "NLP\n\n\n\nDifficulty\nCourse\nUniversity\nAdditional Resources\nLecture\n\n\n\n\nMedium\nCS224N: Natural Language Processing with Deep Learning\nStanford University\n\n[ ]\n\n\nMedium\nCS224U: Natural Language Understanding\nStanford University\n\n[ ]\n\n\n\n\nLLMs\n\n\n\nDifficulty\nCourse\nUniversity\nAdditional Resources\nLecture\n\n\n\n\nMedium\nCOS 597G: Understanding Large Language Models\nPrinceton University\n(No lecture video)\n[ ]\n\n\nMedium\nCOS584: Advanced Natural Language Processing\nPrinceton University\n(No lecture video)\n[ ]\n\n\nMedium\nCS324: Large Language Models\nStanford University\n\n[ ]\n\n\nMedium\nCS 329X: Human-Centered NLP\nStanford University"
  },
  {
    "objectID": "resources/courses.html#computer-vision",
    "href": "resources/courses.html#computer-vision",
    "title": "Courses",
    "section": "Computer Vision",
    "text": "Computer Vision\n\n\n\nDifficulty\nCourse\nUniversity\nAdditional Resources\nLecture\n\n\n\n\nMedium\nCS231N: Deep Learning for Computer Vision\nStanford University\n\n[ ]"
  },
  {
    "objectID": "resources/courses.html#multimodal",
    "href": "resources/courses.html#multimodal",
    "title": "Courses",
    "section": "Multimodal",
    "text": "Multimodal"
  },
  {
    "objectID": "resources/blogs.html",
    "href": "resources/blogs.html",
    "title": "Blogs",
    "section": "",
    "text": "Fabian Pedregosa’s blog, technical, math-rigorous\nhttps://leimao.github.io/blog/\nhttps://francisbach.com/\nhttps://davidstutz.de/category/blog/\nhttps://ai.stanford.edu/~gwthomas/notes/index.html\nhttps://andrewcharlesjones.github.io/menu/blog.html\n\n\n\n\n Back to top"
  },
  {
    "objectID": "misc/misc.html",
    "href": "misc/misc.html",
    "title": "Misc",
    "section": "",
    "text": "Order By\n       Default\n         \n          Date - Oldest\n        \n         \n          Date - Newest\n        \n         \n          Title\n        \n     \n  \n    \n      \n      \n    \n\n\n\n\n\nDate\n\n\nTitle\n\n\nDescription\n\n\n\n\n\n\n \n\n\nKL Divergence\n\n\n\n\n\n\n\n \n\n\nFisher Information\n\n\n\n\n\n\n\n \n\n\nCentral Limit Theorem\n\n\n\n\n\n\n\n \n\n\nScore Matching Reading\n\n\n\n\n\n\n\nAug 20, 2024\n\n\nVariational Autoencoders - Fundamental concepts\n\n\nIn this article we’re going to explore Variational Autoencoder (VAE), a class of generative models. This is the first part of a two-series blogpost that cover the fundamnetal concepts behind VAE.\n\n\n\n\nAug 14, 2024\n\n\nSpelled-out Introduction to Simple Self-Attention mechanism, in code\n\n\nThe backbone mechanism of almost all modern LLMs. This is an excerpt from an excellent book entitled Build a Large Language Model (From Scratch) by Sebastian Raschka.\n\n\n\n\nAug 14, 2024\n\n\nTokenization and Word Embeddings\n\n\nThe two essential steps before training any language models. This is an excerpt from an excellent book entitled Build a Large Language Model (From Scratch) by Sebastian Raschka.\n\n\n\n\n\nNo matching items\n\n Back to top",
    "crumbs": [
      "Misc",
      "Others",
      "Misc"
    ]
  },
  {
    "objectID": "misc/attention/Attention (non-trainable weights).html",
    "href": "misc/attention/Attention (non-trainable weights).html",
    "title": "Spelled-out Introduction to Simple Self-Attention mechanism, in code",
    "section": "",
    "text": "Introduction\nThe Attention mechanism itself can easily become a comprehensive topic. There are four different variants of the attention mechanism, including:\n\nSimplified self-attention\nSelf-attention\nCasual attention\nMulti-head attention.\n\n\n\nCapturing data dependencies with Attention mechanisms\nThe Attention mechanism is a foundation to every LLM that are based on the Transformer architecture. In self-attention, the self refers to the fact that the Attention mechanism is able to compute the attention weights by relating different positions within a single input sequence. It assesses and learns the relationships and dependencies between various parts of the input itself.\n\n\nSimple Attention mechanism without trainable weights\nIn self-attention, context vectors play a crucial role. Their purpose is to create enriched representations of each element in an input sequence (like a sentence) by incorporating information from all other elements in the sequence.\n\nimport torch\n\ninputs = torch.tensor(\n  [[0.43, 0.15, 0.89], # Your     (x^1)\n   [0.55, 0.87, 0.66], # journey  (x^2)\n   [0.57, 0.85, 0.64], # starts   (x^3)\n   [0.22, 0.58, 0.33], # with     (x^4)\n   [0.77, 0.25, 0.10], # one      (x^5)\n   [0.05, 0.80, 0.55]] # step     (x^6)\n)\nThe very first step of implementing self-attention is to compute the attention scores \\(\\omega\\), which are determined by computing the dot product of the query with every other input token.\nquery = inputs[1]\n\nattn_scores_2 = torch.empty(inputs.shape[0])\n\nfor i, x_i in enumerate(inputs):\n    attn_scores_2[i] = torch.dot(x_i, query)\n    \nprint(attn_scores_2)\ntensor([0.9544, 1.4950, 1.4754, 0.8434, 0.7070, 1.0865])\n# understanding dot product\n\nresult = 0\n\nfor idx, element in enumerate(inputs[0]):\n    result += inputs[0][idx]*query[idx]\n\nprint(result)\nprint(torch.dot(inputs[0],query))\ntensor(0.9544)\ntensor(0.9544)\nAfter calculating the attention weights, the next step is to normalize these attention weights so that the sum of these weights equals 1. In so doing, we can have nice probabilistic interpretation of the attention weights as well as maintain training stability for an LLM. In code, we could achieve this step as follows:\nattn_weights_2_tmp = attn_scores_2 / attn_scores_2.sum()\nprint(\"Attention weights: \\t\", attn_weights_2_tmp)\nprint(\"Sum: \\t\\t\\t\", attn_weights_2_tmp.sum())\nAttention weights:   tensor([0.1455, 0.2278, 0.2249, 0.1285, 0.1077, 0.1656])\nSum:             tensor(1.0000)\nIn practice, it’s more common to instead use the softmax function for normalization because it is better at handling extreme values and offers more favorable gradient properties during training.\ndef softmax_naive(x):\n    return torch.exp(x)/torch.exp(x).sum(dim=0)\n\nattn_weights_2_naive = softmax_naive(attn_scores_2)\nprint(\"Attention weights: \", attn_weights_2_naive)\nprint(\"Sum: \", attn_weights_2_naive.sum())\nAttention weights:  tensor([0.1385, 0.2379, 0.2333, 0.1240, 0.1082, 0.1581])\nSum:  tensor(1.)\nNow that we have the attention weights, the next step is to calculate the context vector \\(z\\) b multiplying the embedded input tokens \\(x^{(i)}\\) with the corresponding attention weights and summing the resulting vectors.\nattn_weights_2 = torch.softmax(attn_scores_2, dim=0)\n\nquery = inputs[1]\ncontext_vec_2 = torch.zeros(query.shape)\n\nfor i, x_i in enumerate(inputs):\n    context_vec_2 += attn_weights_2[i] * x_i\n\nprint(context_vec_2)\ntensor([0.4419, 0.6515, 0.5683])\n\n\nComputing attention weights for all input tokens\nimport time\nattn_scores = torch.empty(6,6)\n\nstart_time = time.time()\n\n# naive for loops\nfor i, x_i in enumerate(inputs):\n    for j, x_j in enumerate(inputs):\n        attn_scores[i,j] = torch.dot(x_i, x_j)\n        \nend_time = time.time()\n\nprint(attn_scores)\n\nexecution_time = round(end_time - start_time,4)\nprint(f\"Execution Time: {execution_time} seconds\")\ntensor([[0.9995, 0.9544, 0.9422, 0.4753, 0.4576, 0.6310],\n        [0.9544, 1.4950, 1.4754, 0.8434, 0.7070, 1.0865],\n        [0.9422, 1.4754, 1.4570, 0.8296, 0.7154, 1.0605],\n        [0.4753, 0.8434, 0.8296, 0.4937, 0.3474, 0.6565],\n        [0.4576, 0.7070, 0.7154, 0.3474, 0.6654, 0.2935],\n        [0.6310, 1.0865, 1.0605, 0.6565, 0.2935, 0.9450]])\nExecution Time: 0.0044 seconds\nFor loop is slow, using two for loops is even slower. A smarter way to compute the attention weights is to use matrix multiplication as follows:\nstart_time = time.time()\nattn_scores  = inputs @ inputs.T\nend_time = time.time()\n\nprint(attn_scores)\n\nexecution_time = round(end_time - start_time,4)\nprint(f\"Execution Time: {execution_time} seconds\")\ntensor([[0.9995, 0.9544, 0.9422, 0.4753, 0.4576, 0.6310],\n        [0.9544, 1.4950, 1.4754, 0.8434, 0.7070, 1.0865],\n        [0.9422, 1.4754, 1.4570, 0.8296, 0.7154, 1.0605],\n        [0.4753, 0.8434, 0.8296, 0.4937, 0.3474, 0.6565],\n        [0.4576, 0.7070, 0.7154, 0.3474, 0.6654, 0.2935],\n        [0.6310, 1.0865, 1.0605, 0.6565, 0.2935, 0.9450]])\nExecution Time: 0.0004 seconds\nNice, we are now 10 times faster than using two for loops. Next, we compute the attention weights via the softmax function.\nattn_weights = torch.softmax(attn_scores, dim=1)\nprint(attn_weights)\ntensor([[0.2098, 0.2006, 0.1981, 0.1242, 0.1220, 0.1452],\n        [0.1385, 0.2379, 0.2333, 0.1240, 0.1082, 0.1581],\n        [0.1390, 0.2369, 0.2326, 0.1242, 0.1108, 0.1565],\n        [0.1435, 0.2074, 0.2046, 0.1462, 0.1263, 0.1720],\n        [0.1526, 0.1958, 0.1975, 0.1367, 0.1879, 0.1295],\n        [0.1385, 0.2184, 0.2128, 0.1420, 0.0988, 0.1896]])\n#verify that the rows sum to 1\n\nprint(\"All rows sum: \", attn_weights.sum(dim=1))\nAll rows sum:  tensor([1.0000, 1.0000, 1.0000, 1.0000, 1.0000, 1.0000])\nThe final step is to compute the context vector. Recall that we can do so by multiplying the attention weights with the embedded vector of the inputs.\nall_context_vecs = attn_weights @ inputs\nprint(all_context_vecs)\ntensor([[0.4421, 0.5931, 0.5790],\n        [0.4419, 0.6515, 0.5683],\n        [0.4431, 0.6496, 0.5671],\n        [0.4304, 0.6298, 0.5510],\n        [0.4671, 0.5910, 0.5266],\n        [0.4177, 0.6503, 0.5645]])\n\n\n\n\n Back to top"
  },
  {
    "objectID": "misc/posts/attention/Attention (non-trainable weights).html",
    "href": "misc/posts/attention/Attention (non-trainable weights).html",
    "title": "Spelled-out Introduction to Simple Self-Attention mechanism, in code",
    "section": "",
    "text": "Introduction\nThe Attention mechanism itself can easily become a comprehensive topic. There are four different variants of the attention mechanism, including:\n\nSimplified self-attention\nSelf-attention\nCasual attention\nMulti-head attention.\n\n\n\nCapturing data dependencies with Attention mechanisms\nThe Attention mechanism is a foundation to every LLM that are based on the Transformer architecture. In self-attention, the self refers to the fact that the Attention mechanism is able to compute the attention weights by relating different positions within a single input sequence. It assesses and learns the relationships and dependencies between various parts of the input itself.\n\n\nSimple Attention mechanism without trainable weights\nIn self-attention, context vectors play a crucial role. Their purpose is to create enriched representations of each element in an input sequence (like a sentence) by incorporating information from all other elements in the sequence.\n\nimport torch\n\ninputs = torch.tensor(\n  [[0.43, 0.15, 0.89], # Your     (x^1)\n   [0.55, 0.87, 0.66], # journey  (x^2)\n   [0.57, 0.85, 0.64], # starts   (x^3)\n   [0.22, 0.58, 0.33], # with     (x^4)\n   [0.77, 0.25, 0.10], # one      (x^5)\n   [0.05, 0.80, 0.55]] # step     (x^6)\n)\nThe very first step of implementing self-attention is to compute the attention scores \\(\\omega\\), which are determined by computing the dot product of the query with every other input token.\nquery = inputs[1]\n\nattn_scores_2 = torch.empty(inputs.shape[0])\n\nfor i, x_i in enumerate(inputs):\n    attn_scores_2[i] = torch.dot(x_i, query)\n    \nprint(attn_scores_2)\ntensor([0.9544, 1.4950, 1.4754, 0.8434, 0.7070, 1.0865])\n# understanding dot product\n\nresult = 0\n\nfor idx, element in enumerate(inputs[0]):\n    result += inputs[0][idx]*query[idx]\n\nprint(result)\nprint(torch.dot(inputs[0],query))\ntensor(0.9544)\ntensor(0.9544)\nAfter calculating the attention weights, the next step is to normalize these attention weights so that the sum of these weights equals 1. In so doing, we can have nice probabilistic interpretation of the attention weights as well as maintain training stability for an LLM. In code, we could achieve this step as follows:\nattn_weights_2_tmp = attn_scores_2 / attn_scores_2.sum()\nprint(\"Attention weights: \\t\", attn_weights_2_tmp)\nprint(\"Sum: \\t\\t\\t\", attn_weights_2_tmp.sum())\nAttention weights:   tensor([0.1455, 0.2278, 0.2249, 0.1285, 0.1077, 0.1656])\nSum:             tensor(1.0000)\nIn practice, it’s more common to instead use the softmax function for normalization because it is better at handling extreme values and offers more favorable gradient properties during training.\ndef softmax_naive(x):\n    return torch.exp(x)/torch.exp(x).sum(dim=0)\n\nattn_weights_2_naive = softmax_naive(attn_scores_2)\nprint(\"Attention weights: \", attn_weights_2_naive)\nprint(\"Sum: \", attn_weights_2_naive.sum())\nAttention weights:  tensor([0.1385, 0.2379, 0.2333, 0.1240, 0.1082, 0.1581])\nSum:  tensor(1.)\nNow that we have the attention weights, the next step is to calculate the context vector \\(z\\) b multiplying the embedded input tokens \\(x^{(i)}\\) with the corresponding attention weights and summing the resulting vectors.\nattn_weights_2 = torch.softmax(attn_scores_2, dim=0)\n\nquery = inputs[1]\ncontext_vec_2 = torch.zeros(query.shape)\n\nfor i, x_i in enumerate(inputs):\n    context_vec_2 += attn_weights_2[i] * x_i\n\nprint(context_vec_2)\ntensor([0.4419, 0.6515, 0.5683])\n\n\nComputing attention weights for all input tokens\nimport time\nattn_scores = torch.empty(6,6)\n\nstart_time = time.time()\n\n# naive for loops\nfor i, x_i in enumerate(inputs):\n    for j, x_j in enumerate(inputs):\n        attn_scores[i,j] = torch.dot(x_i, x_j)\n        \nend_time = time.time()\n\nprint(attn_scores)\n\nexecution_time = round(end_time - start_time,4)\nprint(f\"Execution Time: {execution_time} seconds\")\ntensor([[0.9995, 0.9544, 0.9422, 0.4753, 0.4576, 0.6310],\n        [0.9544, 1.4950, 1.4754, 0.8434, 0.7070, 1.0865],\n        [0.9422, 1.4754, 1.4570, 0.8296, 0.7154, 1.0605],\n        [0.4753, 0.8434, 0.8296, 0.4937, 0.3474, 0.6565],\n        [0.4576, 0.7070, 0.7154, 0.3474, 0.6654, 0.2935],\n        [0.6310, 1.0865, 1.0605, 0.6565, 0.2935, 0.9450]])\nExecution Time: 0.0044 seconds\nFor loop is slow, using two for loops is even slower. A smarter way to compute the attention weights is to use matrix multiplication as follows:\nstart_time = time.time()\nattn_scores  = inputs @ inputs.T\nend_time = time.time()\n\nprint(attn_scores)\n\nexecution_time = round(end_time - start_time,4)\nprint(f\"Execution Time: {execution_time} seconds\")\ntensor([[0.9995, 0.9544, 0.9422, 0.4753, 0.4576, 0.6310],\n        [0.9544, 1.4950, 1.4754, 0.8434, 0.7070, 1.0865],\n        [0.9422, 1.4754, 1.4570, 0.8296, 0.7154, 1.0605],\n        [0.4753, 0.8434, 0.8296, 0.4937, 0.3474, 0.6565],\n        [0.4576, 0.7070, 0.7154, 0.3474, 0.6654, 0.2935],\n        [0.6310, 1.0865, 1.0605, 0.6565, 0.2935, 0.9450]])\nExecution Time: 0.0004 seconds\nNice, we are now 10 times faster than using two for loops. Next, we compute the attention weights via the softmax function.\nattn_weights = torch.softmax(attn_scores, dim=1)\nprint(attn_weights)\ntensor([[0.2098, 0.2006, 0.1981, 0.1242, 0.1220, 0.1452],\n        [0.1385, 0.2379, 0.2333, 0.1240, 0.1082, 0.1581],\n        [0.1390, 0.2369, 0.2326, 0.1242, 0.1108, 0.1565],\n        [0.1435, 0.2074, 0.2046, 0.1462, 0.1263, 0.1720],\n        [0.1526, 0.1958, 0.1975, 0.1367, 0.1879, 0.1295],\n        [0.1385, 0.2184, 0.2128, 0.1420, 0.0988, 0.1896]])\n#verify that the rows sum to 1\n\nprint(\"All rows sum: \", attn_weights.sum(dim=1))\nAll rows sum:  tensor([1.0000, 1.0000, 1.0000, 1.0000, 1.0000, 1.0000])\nThe final step is to compute the context vector. Recall that we can do so by multiplying the attention weights with the embedded vector of the inputs.\nall_context_vecs = attn_weights @ inputs\nprint(all_context_vecs)\ntensor([[0.4421, 0.5931, 0.5790],\n        [0.4419, 0.6515, 0.5683],\n        [0.4431, 0.6496, 0.5671],\n        [0.4304, 0.6298, 0.5510],\n        [0.4671, 0.5910, 0.5266],\n        [0.4177, 0.6503, 0.5645]])\n\n\n\n\n Back to top"
  },
  {
    "objectID": "misc/posts/working-with-text/SimpleTokenizer.html",
    "href": "misc/posts/working-with-text/SimpleTokenizer.html",
    "title": "Tokenization and Word Embeddings",
    "section": "",
    "text": "with open(\"data/chap2/the-verdict.txt\",\"r\", encoding=\"utf-8\") as f:\n    raw_text = f.read()\nprint(f\"Total number of character: {len(raw_text)}\")\nTotal number of character: 20479\nprint(raw_text[:99])\nI HAD always thought Jack Gisburn rather a cheap genius--though a good fellow enough--so it was no \n# Simplest tokenization using regex\n\nimport re\ntext = \"Hello world, I am learning AI.\"\n\n# tokenization scheme 1\nresult = re.split(r'(\\s)',text)\nprint(result)\n['Hello', ' ', 'world,', ' ', 'I', ' ', 'am', ' ', 'learning', ' ', 'AI.']\n# tokenization scheme 2\nresult = re.split(r'[,.]|\\s',text)\nprint(result)\n['Hello', 'world', '', 'I', 'am', 'learning', 'AI', '']\n# tokenization scheme 3: removing whitespaces\nresult = [item for item in result if item.strip()]\nprint(result)\n['Hello', 'world', 'I', 'am', 'learning', 'AI']\ntext = \"Hello, world. Is this-- a test?\"\nresult = re.split(r'([,.:;?_!\"()\\']|--|\\s)', text)\nresult = [item.strip() for item in result if item.strip()]\nprint(result)\n['Hello', ',', 'world', '.', 'Is', 'this', '--', 'a', 'test', '?']\n# apply the basic tokenizer to the initial raw text\npreprocessed = re.split(r'([,.?_!\"()\\']|--|\\s)', raw_text)\npreprocessed = [item.strip() for item in preprocessed if item.strip()]\nprint(len(preprocessed))\n4649\nprint(preprocessed[:30])\n['I', 'HAD', 'always', 'thought', 'Jack', 'Gisburn', 'rather', 'a', 'cheap', 'genius', '--', 'though', 'a', 'good', 'fellow', 'enough', '--', 'so', 'it', 'was', 'no', 'great', 'surprise', 'to', 'me', 'to', 'hear', 'that', ',', 'in']\n\nConverting tokens into token IDs\n# convert from Python string to integers (token IDs)\n\n# create a list of unique tokens and sort aphabetically to create vocabulary\nall_words = sorted(set(preprocessed))\nvocab_size = len(all_words)\nprint(vocab_size)\n1159\nvocab = {token:integer for integer, token in enumerate(all_words)}\nfor i, item in enumerate(vocab.items()):\n    print(item)\n    if i&gt;20:\n        break\n('!', 0)\n('\"', 1)\n(\"'\", 2)\n('(', 3)\n(')', 4)\n(',', 5)\n('--', 6)\n('.', 7)\n(':', 8)\n(';', 9)\n('?', 10)\n('A', 11)\n('Ah', 12)\n('Among', 13)\n('And', 14)\n('Are', 15)\n('Arrt', 16)\n('As', 17)\n('At', 18)\n('Be', 19)\n('Begin', 20)\n('Burlington', 21)\n# implementing a simple tokenizer\n\nclass SimpleTokenizerv1:\n    def __init__(self,vocab):\n        self.str_to_int = vocab\n        self.int_to_str = {i:s for s,i in vocab.items()}\n\n    def encode(self,text):\n        preprocessed = re.split(r'([,.?_!\"()\\']|--|\\s)', text)\n        preprocessed = [item.strip() for item in preprocessed if item.strip()]\n        ids = [self.str_to_int[s] for s in preprocessed]\n        return ids\n\n    def decode(self,ids):\n        text = \" \".join([self.int_to_str[i] for i in ids])\n        text = re.sub(r'\\s+([,.?!\"()\\'])', r'\\1', text)\n        return text\ntokenizer = SimpleTokenizerv1(vocab)\ntext = \"\"\"\"It's the last he painted, you know,\" \n           Mrs. Gisburn said with pardonable pride.\"\"\"\nids = tokenizer.encode(text)\nprint(ids)\n[1, 58, 2, 872, 1013, 615, 541, 763, 5, 1155, 608, 5, 1, 69, 7, 39, 873, 1136, 773, 812, 7]\ntokenizer.decode(ids)\n'\" It\\' s the last he painted, you know,\" Mrs. Gisburn said with pardonable pride.'\ntokenizer.decode(tokenizer.encode(text))\n'\" It\\' s the last he painted, you know,\" Mrs. Gisburn said with pardonable pride.'\n# problem with unknown words (words that are not in the vocabulary)\n\ntext = \"I like AI.\"\ntokenizer.encode(text) # throws a KeyError\n---------------------------------------------------------------------------\n\nKeyError                                  Traceback (most recent call last)\n\nCell In[58], line 4\n      1 # problem with unknown words (words that are not in the vocabulary)\n      3 text = \"I like AI.\"\n----&gt; 4 tokenizer.encode(text)\n\n\nCell In[46], line 11, in SimpleTokenizerv1.encode(self, text)\n      9 preprocessed = re.split(r'([,.?_!\"()\\']|--|\\s)', text)\n     10 preprocessed = [item.strip() for item in preprocessed if item.strip()]\n---&gt; 11 ids = [self.str_to_int[s] for s in preprocessed]\n     12 return ids\n\n\nKeyError: 'AI'\n\n\nAdding speical context tokens\n# modifying the tokenizer to handle unknown words (could be an exercise)\n\nall_tokens = sorted(list(set(preprocessed)))\nall_tokens.extend([\"&lt;|endoftext|&gt;\",\"&lt;|unk|&gt;\"])\nvocab = {token:integer for integer,token in enumerate(all_tokens)}\nprint(len(vocab.items()))\n1161\nfor i, item in enumerate(list(vocab.items())[-5:]):\n    print(item)\n('younger', 1156)\n('your', 1157)\n('yourself', 1158)\n('&lt;|endoftext|&gt;', 1159)\n('&lt;|unk|&gt;', 1160)\nclass SimpleTokenizerv2:\n    def __init__(self,vocab):\n        self.str_to_int = vocab\n        self.int_to_str = {i:s for s,i in vocab.items()}\n\n    def encode(self,str):\n        preprocessed = re.split(r'([,.?_!\"()\\']|--|\\s)', text)\n        preprocessed = [item.strip() for item in preprocessed if item.strip()]\n        preprocessed = [item if item in self.str_to_int else \"&lt;|unk|&gt;\" for item in preprocessed]\n        ids = [self.str_to_int[s] for s in preprocessed]\n        return ids\n\n    def decode(self,ids):\n        text = \" \".join([self.int_to_str[i] for i in ids])\n        text = re.sub(r'\\s+([,.?!\"()\\'])', r'\\1', text)\n        return text\ntext1 = \"Hello world. I am learning AI.\"\ntext2 = \"In the sunlit terraces of the palace.\"\ntext = \" &lt;|endoftext|&gt; \".join((text1,text2))\nprint(text)\nHello world. I am learning AI. &lt;|endoftext|&gt; In the sunlit terraces of the palace.\ntokenizer = SimpleTokenizerv2(vocab)\nprint(tokenizer.encode(text))\n[1160, 1160, 7, 55, 155, 1160, 1160, 7, 1159, 57, 1013, 981, 1009, 738, 1013, 1160, 7]\nprint(tokenizer.decode(tokenizer.encode(text)))\n&lt;|unk|&gt; &lt;|unk|&gt;. I am &lt;|unk|&gt; &lt;|unk|&gt;. &lt;|endoftext|&gt; In the sunlit terraces of the &lt;|unk|&gt;.\n\n\n\n\n\n Back to top"
  },
  {
    "objectID": "ainews.html",
    "href": "ainews.html",
    "title": "AI News",
    "section": "",
    "text": "Papers\n\nModel Mixtral dropped Pixtral 12B, their first-ever multimodal model: Tweet.\nNLP MMToM-QA: Multimodal Theory of Mind Question Answering at ACL2024, Outstanding Paper Award.\n\n\n\nPress\n\nPodcast 10 Sep, 2024: Decoding Google Gemini with Jeff Dean, a podcast of Google’s Chief Scientist Jeff Dean, hosted by Prof. Hannah Fry\nEvent Jeff Dean and other prestigious guests are arriving to Vietnam to attend the GenAI Summit.\nPodcast 14 Aug, 2024: Unreasonably Effective AI, a podcast by Google DeepMind’s CEO Demis Hassabis, hosted by Prof. Hannah Fry\n\n\n\nConferences\n\nList of accepted papers at Coference on Language Modeling (COLM) 2024 is now available. Visit here.\n11 Aug, 2024: The 62nd Annual Meeting of the Association for Computational Linguistics (ACL 2024).\n\nACL is not an AI conference.\nGloVe wins the test-of-time award.\n3rd Workshop on Advances in Language and Vision Research (ALVR)\n\n\n\n\n\n\n\n Back to top"
  },
  {
    "objectID": "memes.html",
    "href": "memes.html",
    "title": "Memes",
    "section": "",
    "text": "Back to top"
  },
  {
    "objectID": "resources/repo.html",
    "href": "resources/repo.html",
    "title": "Blogs",
    "section": "",
    "text": "PyTorch Tutorial for Deep Learning Researchers\n\n\n\n\n\n Back to top"
  },
  {
    "objectID": "module1.html#project-handout-and-material",
    "href": "module1.html#project-handout-and-material",
    "title": "Module 1 - Introduction to Python Programming",
    "section": "Project handout and material",
    "text": "Project handout and material\n\n\n\n\n\n\n\n\n\n\n\nDate\nType\nContent\nInstructor\nHandout\nFurther Reading\n\n\n\n\n22/05/23\nProject handout\nObject Detection with YOLOv8 Project\nTA Thắng\n[solution]\n\n\n\n22/05/23\nProject handout\nData Manipulation and Crawling Project\nTA Thắng\n[solution]\n\n\n\n22/05/23\nProject handout\nChatGPT Applications Project\nTA Thái\n[solution]\n\n\n\n24/05/23\nProject Tutorial\nImage Project: Yolov8 for Object Detection\nTA Thắng\n230524 - M01PT01\n\n\n\n26/05/23\nProject Tutorial\nData Manipulation using Python Libraries\nTA Thắng\n230526 - M01PT02\n\n\n\n28/05/23\nProject Tutorial\nText Project: ChatGPT-based Application\nTA Thái\n230528 - M01PT03\n\n\n\n23/05/23\nPython Support\nFor, List and Dictionary\nTA Tiềm\n230523 - M02CR01",
    "crumbs": [
      "Resources",
      "Course content",
      "Module 1"
    ]
  },
  {
    "objectID": "module1.html#competition-training-1",
    "href": "module1.html#competition-training-1",
    "title": "Module 1 - Introduction to Python Programming",
    "section": "Competition training",
    "text": "Competition training\n\n\n\n\n\n\n\n\n\n\n\nDate\nType\nContent\nInstructor\nHandout\nFurther Reading\n\n\n\n\n11/05/23\nCompetition\nData Visualization\nTA Hùng\n230511 - M01AC01\n\n\n\n18/05/23\nCompetition\nCompetition tasks and metrics\nTA Hùng\n230518 - M01AC02\n\n\n\n25/05/23\nCompetition\nDesign Validation\nTA Hùng\n230525 - M01AC03",
    "crumbs": [
      "Resources",
      "Course content",
      "Module 1"
    ]
  },
  {
    "objectID": "module1.html#extra-class-algorithms-and-complexity",
    "href": "module1.html#extra-class-algorithms-and-complexity",
    "title": "Module 1 - Introduction to Python Programming",
    "section": "Extra class: Algorithms and Complexity",
    "text": "Extra class: Algorithms and Complexity\n\n\n\n\n\n\n\n\n\n\n\nDate\nType\nContent\nInstructor\nHandout\nFurther Reading\n\n\n\n\n06/05/23\nAlgorithms & Complexity\nBig-O (Time analysis)\nTA Thái\n230506 - M01EC01\n\n\n\n13/05/23\nAlgorithms & Complexity\nBrute-force Exhaustive\nTA Thái\n230513 - M01EC02\n\n\n\n20/05/23\nAlgorithms & Complexity\nRecursion and Two Pointer\nTA Thái\n230520 - M01EC03\n\n\n\n27/05/23\nAlgorithms & Complexity\nDynamic Programming\nTA Thái\n230527 - M01EC04",
    "crumbs": [
      "Resources",
      "Course content",
      "Module 1"
    ]
  },
  {
    "objectID": "module2.html#competition",
    "href": "module2.html#competition",
    "title": "Module 2 - Calculus and Linear Algebra for AI",
    "section": "Competition",
    "text": "Competition\n\n\n\n\n\n\n\n\n\n\n\nDate\nType\nContent\nInstructor\nHandout\nFurther Reading\n\n\n\n\n01/06/23\nCompetition\nHyper-parameter optimization\nTA Hùng\n230601 - M02AC01\n\n\n\n08/06/23\nCompetition\nEnsembling (Voting, Averaging, and Stacked Generalization)\nTA Hùng\n230608 - M02AC02\n\n\n\n15/06/23\nCompetition\nAugmentation Strategies and Albumentation\nTA Hùng\n230615 - M02AC03\n\n\n\n22/06/23\nCompetition\nNestquant\nTA Hùng\n230622 - M02AC04\n\n\n\n29/06/23\nCompetition\nOOD-CV\nTA Hùng\n230629 - M02AC05\n\n\n\n06/07/23\nCompetition\nVision Transformer\nTA Hùng\n230706 - M02AC06",
    "crumbs": [
      "Resources",
      "Course content",
      "Module 2"
    ]
  },
  {
    "objectID": "module2.html#project-tutorial",
    "href": "module2.html#project-tutorial",
    "title": "Module 2 - Calculus and Linear Algebra for AI",
    "section": "Project Tutorial",
    "text": "Project Tutorial\n\n\n\n\n\n\n\n\n\n\n\nDate\nType\nContent\nInstructor\nHandout\nFurther Reading\n\n\n\n\n09/07/23\nProject Tutorial\nSVD and its Applications\nDr. Đình Vinh\n230709 - M02ET01\n\n\n\n10/07/23\nProject handout\nImage Project: Depth Information Reconstruction\nTA Thắng\n[proposal]\n\n\n\n10/07/23\nProject handout\nText Project: Text Retrieval (using Pretrained Embedding)\nTA Thắng\n[solution]\n\n\n\n10/07/23\nProject handout\nTabular Data Project: Sales Prediction (Linear and non-linear regression)\nTA Thái\n[proposal]\n\n\n\n12/07/23\nProject Tutorial\nTabular Data Project: Sales Prediction (Linear and non-linear regression)\nTA Thái\n230716 - M02PT03\n\n\n\n14/07/23\nProject Tutorial\nText Project: Text Retrieval (using Pretrained Embedding)\nTA Thắng\n230714 - M02PT02\n\n\n\n16/07/23\nProject Tutorial\nImage Project: Depth Information Reconstruction\nTA Thắng\n230712 - M02PT01",
    "crumbs": [
      "Resources",
      "Course content",
      "Module 2"
    ]
  },
  {
    "objectID": "resources/courses.html",
    "href": "resources/courses.html",
    "title": "Courses",
    "section": "",
    "text": "Difficulty\nCourse\nUniversity\nAdditional Resources\nLecture\n\n\n\n\nMedium\nStat110: Probability\nHarvard University\nLectures\n[x]\n\n\nEasy\nCS109: Probability for Computer Scientists\nStanford University\nLectures\n[x]\n\n\nMedium\nStats 116: Introduction to Probability\nStanford University\n\n[ ]\n\n\nMedium\nMath 131B: Introduction to Probability and Statistics\nUC Irvine\nLectures\n[x]\n\n\nDifficult\nStats 300B: Theory of Statistics II\nStanford University\n\n[ ]\n\n\nDifficult\nStatistics 305a: Applied Statistics (Linear Models and More)\nStanford University\n\n[ ]\n\n\n\n\n\n\n\n\n\nDifficulty\nCourse\nUniversity\nAdditional Resources\nLecture\n\n\n\n\nExtreme\nEE364A: Convex Optimization I\nStanford University\nLecture\n\n\n\nDifficult\nEE364m: The Mathematics of Convexity\nStanford University\n\n[ ]"
  },
  {
    "objectID": "resources/courses.html#probability-and-statistics",
    "href": "resources/courses.html#probability-and-statistics",
    "title": "Courses",
    "section": "",
    "text": "Difficulty\nCourse\nUniversity\nAdditional Resources\nLecture\n\n\n\n\nMedium\nStat110: Probability\nHarvard University\nLectures\n[x]\n\n\nEasy\nCS109: Probability for Computer Scientists\nStanford University\nLectures\n[x]\n\n\nMedium\nStats 116: Introduction to Probability\nStanford University\n\n[ ]\n\n\nMedium\nMath 131B: Introduction to Probability and Statistics\nUC Irvine\nLectures\n[x]\n\n\nDifficult\nStats 300B: Theory of Statistics II\nStanford University\n\n[ ]\n\n\nDifficult\nStatistics 305a: Applied Statistics (Linear Models and More)\nStanford University\n\n[ ]"
  },
  {
    "objectID": "resources/courses.html#optimization",
    "href": "resources/courses.html#optimization",
    "title": "Courses",
    "section": "",
    "text": "Difficulty\nCourse\nUniversity\nAdditional Resources\nLecture\n\n\n\n\nExtreme\nEE364A: Convex Optimization I\nStanford University\nLecture\n\n\n\nDifficult\nEE364m: The Mathematics of Convexity\nStanford University\n\n[ ]"
  },
  {
    "objectID": "resources/books.html#optimization",
    "href": "resources/books.html#optimization",
    "title": "Books and other preprint materials",
    "section": "Optimization",
    "text": "Optimization\n\nConvex Optimization by Stephen Boyd and Lieven Vandenberghe\nIntroductory Lectures on Stochastic Convex Optimization by John C. Duchi"
  },
  {
    "objectID": "module3.html#project-tutorial",
    "href": "module3.html#project-tutorial",
    "title": "Module 3 - Probability and Statistics for AI",
    "section": "",
    "text": "Date\nType\nContent\nInstructor\nHandout\nFurther Reading\n\n\n\n\n17/08/23\nAssignment\nBig Data Frameworks (1)\nDr. Đình Vinh\n\n\n\n\n17/08/23\nAssignment\nBig Data Frameworks (1)\nDr. Đình Vinh\n\n\n\n\n17/08/23\nProject tutorial - PT\nBig Data Frameworks (1)\nDr. Đình Vinh\n230818\n[LSFS and MapReduce]\n\n\n20/08/23\n\nBig Data Frameworks (2)\nDr. Đình Vinh\n230820\n\n\n\n22/08/23\n\nImage Data Project: Image Retrieval\nTA Thắng\n\n\n\n\n23/08/23\n\nBig Data Frameworks (3)\nDr. Đình Vinh\n230823\n[C9, Feng 2021]\n\n\n25/08/23\n\nImage Data Project: Image Retrieval\nTA Thắng\n230825\n[VIT paper (Dosovitskiy et al., 2021)]",
    "crumbs": [
      "Resources",
      "Course content",
      "Module 3"
    ]
  },
  {
    "objectID": "module3.html#extra-class-introduction-to-nlp",
    "href": "module3.html#extra-class-introduction-to-nlp",
    "title": "Module 3 - Probability and Statistics for AI",
    "section": "",
    "text": "Date\nType\nContent\nInstructor\nHandout\nFurther Reading\n\n\n\n\n29/07/23\nNLP Introduction\nIntroduction, Preprocessing\nTA Thái\n230729\n\n\n\n05/08/23\nNLP Introduction\nPreprocessing, Tokenization\nTA Thái\n230805\n[note] [C2, Jurafsky et al., 2023]\n\n\n12/08/23\nNLP Introduction\nStatistical Language Model\nTA Thái\n230812\n[C3, Jurafsky et al., 2023] [Andrej Karpathy’s Lecture]\n\n\n18/08/23\nNLP Introduction\nPart-of-Speech Tagging\nTA Thái\n230819\n[CA Jurafsky et al., 2023]\n\n\n26/08/23\nNLP Introduction\nConstituency Parsing\nTA Thái\n230826",
    "crumbs": [
      "Resources",
      "Course content",
      "Module 3"
    ]
  },
  {
    "objectID": "module3.html#main-lesson",
    "href": "module3.html#main-lesson",
    "title": "Module 3 - Probability and Statistics for AI",
    "section": "",
    "text": "Date\nType\nContent\nInstructor\nHandout\nFurther Reading\n\n\n\n\n18/07/23\nAssignment\nProbability Exercise\nDr. Đình Vinh\n\n\n\n\n19/07/23\nMain Lesson\nBasic Probability, Histogram and Image Enhancement\nDr. Vinh\n230719\n\n\n\n21/07/23\nMain Lesson\nNaive Bayes Classifier\nDr. Vinh\n230721\n\n\n\n23/07/23\nExercise Session\nTA-Exercise\nDr. Đình Vinh\n230723\n\n\n\n31/07/23\nAssignment\nStatistic Exercise\nDr. Đình Vinh\nSolution\n\n\n\n26/07/23\nMain Lesson\nBasic Statistics and Correlation Coefficient (Basic Tracking)\nDr. Vinh\n230726\n\n\n\n28/07/23\nMain Lesson\nTemplate Matching (Cosine Similarity vs. Correlation Coefficient)\nDr. Đình Vinh\n230728\n\n\n\n30/07/23\nExercise Session\nTA-Exercise\nDr. Đình Vinh\n230730\n\n\n\n31/07/23\nAssignment\nGenetic Algorithms and its Applications Exercise\nDr. Đình Vinh\n\n\n\n\n02/08/23\nMain Lesson\nRandomness and Genetic Algorithms\nDr. Vinh\n230802\n\n\n\n04/08/23\nMain Lesson\nGenetic Algorithms (Optimization and Linear Regression)\nDr. Vinh\n230804\n\n\n\n06/08/23\nExercise Session\nTA-Exercise\nDr. Đình Vinh\n230806\n\n\n\n07/08/23\nAssignment\nGenetic Algorithms and its Applications Exercise\nDr. Đình Vinh\n\n\n\n\n07/08/23\nExercise Session\nData Analysis Exercise\nTA Thắng\n[solution]\n\n\n\n09/08/23\nMain Lesson\nData visualization and Data Analysis (1)\nDr. Vinh\n230809\n[note]\n\n\n11/08/23\nMain Lesson\nData visualization and Data Analysis (2)\nDr. Vinh\n230811\n\n\n\n13/08/23\nExercise Session\nTA-Exercise (Polar)\nTA Thắng\n230813\n[Polar doc]",
    "crumbs": [
      "Resources",
      "Course content",
      "Module 3"
    ]
  },
  {
    "objectID": "module3.html#python-tutorial-session",
    "href": "module3.html#python-tutorial-session",
    "title": "Module 3 - Probability and Statistics for AI",
    "section": "",
    "text": "Date\nType\nContent\nInstructor\nHandout\nFurther Reading\n\n\n\n\n18/07/23\nPython Support\nProbability\nTA Tiềm\n230718\n\n\n\n25/07/23\nPython Support\nStatistics\nTA Bảo\n230725\n\n\n\n01/08/23\nPython Support\nGenetic Algorithms\nTA Tiềm\n230801\n\n\n\n08/08/23\nPython Support\nPandas (1)\nTA Bảo\n230808\n\n\n\n14/08/23\nPython Support\nPandas (2)\nTA Bảo\n230815\n[C7, McKinney 2023]",
    "crumbs": [
      "Resources",
      "Course content",
      "Module 3"
    ]
  },
  {
    "objectID": "module3.html#competition-training",
    "href": "module3.html#competition-training",
    "title": "Module 3 - Probability and Statistics for AI",
    "section": "",
    "text": "Date\nType\nContent\nInstructor\nHandout\nFurther Reading\n\n\n\n\n20/07/23\nCompetition\nTricks to improve performance\nTA Hùng\n230720\n\n\n\n27/07/23\nCompetition\nHCM AI Challenge\nTA Hùng\n230727\n\n\n\n03/08/23\nCompetition\nWeb API + Docker\nTA Hùng\n230803\n\n\n\n17/08/23\nCompetition\nOnnx\nTA Hùng\n230817\n\n\n\n24/08/23\nCompetition\nSemi-supervised Learning\nTA Hùng\n230824\n[Lil’Log] [GitHub repo] [Laine et al., 2016] [Tarvainen et al., 2017]",
    "crumbs": [
      "Resources",
      "Course content",
      "Module 3"
    ]
  },
  {
    "objectID": "resources/books.html#linear-algebra",
    "href": "resources/books.html#linear-algebra",
    "title": "Books and other preprint materials",
    "section": "Linear Algebra",
    "text": "Linear Algebra\n\nLinear Algebra and Learning from Data by Gilbert Strang"
  },
  {
    "objectID": "resources/books.html#information-theory",
    "href": "resources/books.html#information-theory",
    "title": "Books and other preprint materials",
    "section": "Information Theory",
    "text": "Information Theory\n\nInformation Theory and Statistics by John C. Duchi\nInformation-Theoretic Foundations for Machine Learning by Hong Jun Jeon and Benjamin Van Roy"
  },
  {
    "objectID": "misc/posts/vae/vae.html",
    "href": "misc/posts/vae/vae.html",
    "title": "Variational Autoencoders - Fundamental concepts",
    "section": "",
    "text": "Probabilistic modeling is one of the most fundamental ways in which we reason about the world. This paper spearheaded the integration of deep learning with scalable probabilistic inference (amortized mean-field variational inference via a so-called reparameterization trick), giving rise to the Variational Autoencoder (VAE). The lasting value of this work is rooted in its elegance. The principles used to develop VAEs deepened our understanding of the interplay between deep learning and probabilistic modeling, and sparked the development of many subsequent interesting probabilistic models and encoding approaches."
  },
  {
    "objectID": "misc/posts/vae/vae.html#proxy-distributions",
    "href": "misc/posts/vae/vae.html#proxy-distributions",
    "title": "Variational Autoencoders - Fundamental concepts",
    "section": "Proxy distributions",
    "text": "Proxy distributions\nSince we don’t have access to the two high-dimensional distributions associated with the encoder \\(p(\\mathbf{z} \\vert \\mathbf{x})\\) and the decoder \\(p(\\mathbf{x} \\vert \\mathbf{z})\\), we need to come up with a way to approximate it. Consider the following two distributions:\n\n\\(q_{\\phi}(\\mathbf{z} \\vert \\mathbf{x})\\), which is the proxy for \\(p(\\mathbf{z} \\vert \\mathbf{x})\\) and we assume this to be a Gaussian distribution.\n\\(p_{\\theta}(\\mathbf{x} \\vert \\mathbf{z})\\), which is the proxy for \\(p(\\mathbf{x} \\vert \\mathbf{z})\\), and we also assume this to be a Gaussian distribution.\n\n![[Screenshot 2024-08-11 at 19.31.17.png]]\nHere, \\(\\phi\\) and \\(\\theta\\) are the parameters that we need to optimize. Proxy distributions are ditributions that are approximate of other true distributions."
  },
  {
    "objectID": "misc/posts/vae/vae.html#encoder",
    "href": "misc/posts/vae/vae.html#encoder",
    "title": "Variational Autoencoders - Fundamental concepts",
    "section": "Encoder",
    "text": "Encoder\nTo train the VAE, we need to know the ground truth pair \\((\\mathbf{x},\\mathbf{z})\\). We know that \\(\\mathbf{z}\\) is generated from the distribution \\(q_{\\phi}(\\mathbf{z} \\vert \\mathbf{x})\\), and we also know that \\(q_{\\phi}(\\mathbf{z} \\vert \\mathbf{x})\\) is also a Gaussian. Assume that this Gaussian has mean \\(\\boldsymbol{\\mu}\\) and variance \\(\\sigma^{2}\\mathbf{I}\\). Now, the difficult part is how to obtain \\(\\boldsymbol{\\mu}\\) and covariance \\(\\sigma^2\\mathbf{I}\\) from the input image \\(\\mathbf{x}\\). The answer is, we will construct a neural network such that \\[\\boldsymbol{\\mu} = \\underbrace{\\boldsymbol{\\mu}_\\phi(\\mathbf{x})}_{\\text{neural network}}\\] \\[\\sigma^{2}= \\underbrace{\\sigma^{2}_{\\phi}(\\mathbf{x})}_{\\text{neural network}}\\] Now, the \\(\\ell^{th}\\) training sample of the training set \\(\\mathbf{z}^{(\\ell)}\\) can be sampled from the Gaussian distribution: \\[\\mathbf{z}^{(\\ell)} \\sim q_{\\phi}(\\mathbf{z} \\vert \\mathbf{x}^{(\\ell)}),\\quad \\text{where } q_{\\phi}(\\mathbf{z} \\vert \\mathbf{x}^{(\\ell)})=\\mathcal{N}(\\mathbf{z} \\vert \\boldsymbol{\\mu}_\\phi(\\mathbf{x}^{(\\ell)}), \\sigma^{2}_{\\phi}(\\mathbf{x}^{(\\ell)}))\\] Here, note that \\(\\boldsymbol{\\mu}_\\phi(\\mathbf{x}^{(\\ell)})\\) and \\(\\sigma^{2}_{\\phi}(\\mathbf{x}^{(\\ell)})\\) are functions of \\(\\mathbf{x}^{(\\ell)}\\), so for every different \\(\\mathbf{x}^{(\\ell)}\\), we need to come up with a different Gaussian."
  },
  {
    "objectID": "misc/posts/vae/vae.html#beta-vae",
    "href": "misc/posts/vae/vae.html#beta-vae",
    "title": "Variational Autoencoders - Fundamental concepts",
    "section": "\\(\\beta\\)-VAE",
    "text": "\\(\\beta\\)-VAE\nIt is often the case that VAE will generate blurry images. However, this is not the case for models that optimize the exact likelihood, such as PixelCNNs or flow models. Consider the common case where the decoder is a Gaussian with fixed variance: \\[\\log p_{\\theta}(\\mathbf{x} \\vert \\mathbf{z}) = -\\dfrac{1}{-2\\sigma^{2}} \\|\\mathbf{x} - \\text{decode}_{\\theta}(\\mathbf{z})\\|_{2}^{2}+ \\text{const}.\\] Let \\(e_{\\phi}(\\mathbf{x}) = \\mathbb{E}[q_{\\phi}(\\mathbf{z} \\vert \\mathbf{x})]\\) be the encoding of \\(\\mathbf{x}\\) and \\(\\mathcal{X}(\\mathbf{z}) = \\{\\mathbf{x}: e_{\\phi}(\\mathbf{x}) = \\mathbf{z}\\}\\) be the set of inputs that get mapped to \\(\\mathbf{z}\\). The main reason why the decoder produces blurry images is that, for a fixed inference network, the optimal setting of the generator parameters when using squared construction loss is to ensure \\(d_{\\theta}(\\mathbf{z}) = \\mathbf{E}[\\mathbf{x}: \\mathbf{x} \\in \\mathcal{X}(\\mathbf{z})]\\), which is the average of all inputs \\(\\mathbf{x}\\) which map to that \\(\\mathbf{z}\\)."
  },
  {
    "objectID": "misc/posts/vae/vae.html#autoencoder",
    "href": "misc/posts/vae/vae.html#autoencoder",
    "title": "Variational Autoencoders - Fundamental concepts",
    "section": "Autoencoder",
    "text": "Autoencoder\nWhile VAEs (Kingma and Welling 2013) and the deterministic autoencoder (AE) are very similar, there are two main differences:\n\nKingma, Diederik P., and M. Welling. 2013. “Auto-Encoding Variational Bayes.” In International Conference on Learning Representations. Vol. abs/1312.6114. 1. Explorandum Ltd.\n\nin the AE, the objective is the log likelihood of the reconstruction without any KL term\nThe encoding is deterministic, so that the encoder network just needs to compute \\(\\mathbb{E}[\\mathbf{z} \\vert \\mathbf{x}]\\) and not \\(\\text{Var}[\\mathbf{z} \\vert \\mathbf{x}]\\).\n\nThe main advantage of VAE over a deterministic autoencoder is that it defines a proper generative model that can create natural-looking novel images by decoding prior samples \\(\\mathbf{z} \\sim \\mathcal{N}(0, \\mathbf{I})\\). This is in constrast with the autoencoder, where it only knows how to decode the latent codes derived from the training set, so it does poorly on random inputs, as shown in Figure\n\n\n\ncelebA produced by Autoencoder, VAE, and beta-VAE"
  },
  {
    "objectID": "misc/posts/vae/vae.html#decoder",
    "href": "misc/posts/vae/vae.html#decoder",
    "title": "Variational Autoencoders - Fundamental concepts",
    "section": "Decoder",
    "text": "Decoder\nThe job of the decoder network is to reconstruct the image \\(\\mathbf{x}\\) given the latent vector \\(\\mathbf{z}\\), which is encoded from the original image via the encoder. \\[\\hat{\\mathbf{x}} = \\text{decode}_\\theta(\\mathbf{z})\\] One assumption that we’ll make is that the error between the reconstructed image \\(\\hat{\\mathbf{x}}\\) and the original input image \\(\\mathbf{x}\\) is Gaussian, i.e. \\[(\\hat{\\mathbf{x}}-\\mathbf{x}) \\sim \\mathcal{N}(0, \\sigma^{2}_\\text{dec})\\] It then follows that the distribution \\(p_{\\theta}(\\mathbf{x} \\vert \\mathbf{z})\\) is: \\[\n\\begin{aligned}\n\\log p_{\\theta}(\\mathbf{x} \\vert \\mathbf{z})\n&= \\log \\mathcal{N}(\\mathbf{x} \\vert \\text{decode}_{\\theta}(\\mathbf{x}), \\sigma^{2}_\\text{dec}\\mathbf{I}) \\\\\n&= \\log \\dfrac{1}{\\sqrt{(2\\pi \\sigma^{2}_\\text{dec})^{D}}} \\exp \\left(- \\dfrac{\\|\\mathbf{x} - \\text{decode}_{\\theta}(\\mathbf{z})\\|^2}{2 \\sigma^{2}_\\text{dec}}\\right)\\\\\n&= - \\dfrac{\\|\\mathbf{x} - \\text{decode}_{\\theta}(\\mathbf{z})\\|^2}{2 \\sigma^{2}_\\text{dec}}-\\log\\sqrt{(2\\pi\\sigma^2_\\text{dec})^D}\\end{aligned}\n\\] where \\(D\\) is the dimension of \\(\\mathbf{x}\\). Through this derivation, we know that maximizing the likelihood term in ELBO is equivalent to minimizing the \\(\\ell_{2}\\) loss between the ground truth image and the reconstructed image, which makes a lot of sense."
  },
  {
    "objectID": "misc/posts/vae/vae.html#jensens-inequality",
    "href": "misc/posts/vae/vae.html#jensens-inequality",
    "title": "Variational Autoencoders - Fundamental concepts",
    "section": "Jensen’s inequality",
    "text": "Jensen’s inequality\nJensen’s inequality is a useful concept for any convex function that has many applications in generative models. A convex function can be defined as follows:\n\n\n\n\n\n\nDefinition of convex function (Boyd and Vandenberghe 2004)\n\n\n\nA function \\(f: \\mathbb{R}^n \\mapsto \\mathbb{R}\\) is said to be convex if its domain \\(\\textbf{dom}f\\) is a convex set and if for all \\(x,y \\in \\textbf{dom}f\\), \\(\\theta \\in [0,1]\\), we have:\n\\[f(\\theta x + (1-\\theta) y) \\leq \\theta f(x) + (1-\\theta) f(y)\\]\n\n\n\nBoyd, Stephen, and Lieven Vandenberghe. 2004. Convex Optimization. Cambridge University Press.\nFor example, consider the figure below where the figure on the left is the graph of a convex function \\(f(x) = x^2\\), the line (sometimes also referred to as chord) between any two points \\(x,y\\) on the graph lies above the graph, i.e. every chord is below the function. In contrast, the function on the right does not have such property so it is not a convex function. The definition that we have for a convex function is just a generalization and mathematical expression of this property.\n\n\nGiven a convex function \\(f\\) and a random variable \\(X\\), Jensen’s inequality states that:\n\n\n\n\n\n\nJensen’s inequality\n\n\n\n\\[\\mathbb{E}[f(X)] \\geq f(\\mathbb{E}[X])\\]\n\n\nLater in this section, we will apply the Jensen’s inequality to derive the Evidence Lower Bound, which is the workhorse of VAE."
  },
  {
    "objectID": "misc/posts/vae/vae.html#kl-divergence",
    "href": "misc/posts/vae/vae.html#kl-divergence",
    "title": "Variational Autoencoders - Fundamental concepts",
    "section": "KL Divergence",
    "text": "KL Divergence\nSometimes it is useful for us to have a measure \\(D\\) to quantify the difference between two distributions. We require that measure to have the following properties (Murphy 2023):\n\nContinuous in its arguments\nNonnegativity\nPermutation invariant\nMonotonic\nSatisfy the natural chain rule\n\nKL divergence turns out to be that useful tool and it is defined as follows:\n\n\n\n\n\n\nKL Divergence\n\n\n\n\\[D_{KL} = \\int p(x) \\log \\dfrac{p(x)}{q(x)} dx\\]\n\n\nInterested readers can refer to (Murphy 2023) to see the proof that KL Divergence satisfies all of the desiderata.\n\nMurphy, Kevin P. 2023. Probabilistic Machine Learning: Advanced Topics. MIT Press. http://probml.github.io/book2."
  },
  {
    "objectID": "misc/posts/vae/vae.html#tip-convex-function",
    "href": "misc/posts/vae/vae.html#tip-convex-function",
    "title": "Variational Autoencoders - Fundamental concepts",
    "section": "",
    "text": "A function \\(f: \\mathbb{R}^n \\mapsto \\mathbb{R}\\) is said to be convex if its domain \\(\\textbf{dom}f\\) is a convex set and if for all \\(x,y \\in \\textbf{dom}f\\), \\(\\theta \\in [0,1]\\), we have:\n\\[f(\\theta x + (1-\\theta) y) \\leq \\theta f(x) + (1-\\theta) f(y)\\]"
  },
  {
    "objectID": "misc/fisher-information/fisher-information.html",
    "href": "misc/fisher-information/fisher-information.html",
    "title": "Fisher Information",
    "section": "",
    "text": "#information-theory #statistics\n\nIntroduction\nFisher information provides a way to measure the amount of information that a random variable contains about some parameter \\(\\theta\\) (such as the true mean) of the random variable’s assumed probability distribution. # Preliminaries A standard problem in statistical estimation is to determine the parameters of a distribution from a sample of data drawn from that distribution. Let \\(X_{1}, \\cdots, X_{n}\\) be drawn i.i.d. from \\(\\mathcal{N}(\\theta,1)\\). Suppose we wish to estimate \\(\\theta\\) from a sample of size \\(n\\). We guess that the sample mean \\(\\bar{X}_{n} = \\dfrac{1}{n}\\sum\\limits X_i\\) is the best estimate of \\(\\theta\\) and we can indeed show that \\(\\bar{X}_{n}\\) is the minimum MSE unbiased estimator.\nLet \\(\\{f(x;\\theta)\\}, \\theta \\in \\Theta\\) denote an indexed family of densities so that \\(f(x;\\theta) \\geq 0, \\int f(x;\\theta)dx=1\\) for all \\(\\theta \\in \\Theta\\), where \\(\\Theta\\) is called the parameter set.\n\n[!definition] Estimator An estimator for \\(\\theta\\) for a sample size \\(n\\) is a function \\(T\\): \\(\\mathcal{X}^{n}\\rightarrow \\Theta\\).\n\nAn estimator is meant to approximate the value of the parameter, therefore it is desirable to have some idea of the goodness of the approximator. We call the difference \\(T-\\theta\\) the error of the estimator. This error itself is a random variable.\n\n[!definition] Bias The bias of an estimator \\(T(X_{1},X_{2},\\cdots, X_{n})\\) for the parameter \\(\\theta\\) is the expected value of the error of the estimator, i.e. \\[\\mathbb{E}_{\\theta}[T(X_{1}, X_{2},\\cdots, X_{n})-\\theta]\\]\n\nThe estimator is said to be unbiased if the bias is 0 for all \\(\\theta \\in \\Theta\\).\n\n[!definition] Consistency of probability An estimator \\(T(X_{1},X_{2},\\cdots, X_{n})\\) for \\(\\theta\\) is said to be consistent in probability if \\(T(X_{1},X_{2},\\cdots, X_{n}) \\rightarrow \\theta\\) in probability as \\(n\\rightarrow \\theta\\).\n\nWhile consistency is a desirable asymptotic property, we are also interested in the behavior for small sample size.\n\n[!definition] Dominate An estimator \\(T_{1}(X_{1},X_{2},\\cdots, X_{n})\\) is said to dominate another estimator \\(T_{2}(X_{1},X_{2},\\cdots, X_n)\\) if for all \\(\\theta\\) we have: \\[\\mathbb{E}[T_{1}(X_{1},X_{2}, \\cdots, X_{n})-\\theta ]^{2}\\leq \\mathbb{E}[T_{2}(X_{1},X_{2}, \\cdots, X_{n})-\\theta ]^{2}\\]\n\nThis raises a natural question: Is there a best estimator of \\(\\theta\\) that dominates every other estimator ? To answer this, we derive the Cramér-Rao lower bound on the MSE of any estimator.\n\n[!definition] Score The score \\(V\\) is a random variable defined by: \\[V = \\dfrac{\\partial}{\\partial \\theta} \\log f(X;\\theta)\\]\n\nThe mean value of the score is: \\[\n\\begin{aligned}\n\\mathbb{E}[V] &= \\int \\dfrac{\\partial}{\\partial \\theta}\\log f(x;\\theta) f(x;\\theta)dx \\\\\n&= \\int \\dfrac{\\frac{\\partial}{\\partial \\theta}f(x;\\theta)}{f(x;\\theta)} f(x;\\theta)dx \\\\\n&= \\int \\dfrac{\\partial}{\\partial \\theta}f(x;\\theta)dx \\\\\n&= \\dfrac{\\partial}{\\partial\\theta}\\int f(x;\\theta)dx = 0\n\\end{aligned}\n\\] therefore \\(\\mathbb{E}[V]^{2}= var(V)\\). The variance of the score has a special significance.\n\n[!definition] Fisher information The Fisher information \\(J(\\theta)\\) is the variance of the score: \\[J(\\theta) = \\mathbb{E}_{\\theta}\\left[\\dfrac{\\partial}{\\partial \\theta}\\log f(X;\\theta)\\right]^2\\]\n\nIf we consider a sample of \\(n\\) random variable \\(X_{1},X_{2},\\cdots, X_{n}\\) drawn i.i.d. from \\(f(x;\\theta)\\), we have: \\[\nf(x_{1},x_{2},\\cdots, x_{n;\\theta)}= \\prod_{i=1}^{n}f(x_{i};\\theta)\n\\] and the score function is the sum of the individual score functions: \\[\n\\begin{aligned}\nV(X_{1}, X_{2}, \\cdots, X_{n})\n&= \\dfrac{\\partial}{\\partial \\theta} \\log f(X_{1},X_{2}, \\cdots, X_{n}; \\theta) \\\\\n&= \\sum\\limits_{i=1}^{n}\\dfrac{\\partial}{\\partial \\theta} \\log f(X_{i}; \\theta) \\\\\n&= \\sum\\limits_{i=1}^{n}V(X_{i})\n\\end{aligned}\n\\] where the \\(V(X_{i})\\) are i.i.d. with zero mean, therefore the \\(n\\)-sample Fisher information is: \\[\n\\begin{aligned}\nJ_{n}(\\theta) &= \\mathbb{E}_{\\theta}\\left[\\dfrac{\\partial}{\\partial \\theta} \\log f(X_{1},X_{2},\\cdots, X_{n};\\theta)\\right]^{2} \\\\\n&= \\mathbb{E}_{\\theta}[V^{2}(X_{1},X_{2},\\cdots, X_{n})] \\\\&= \\mathbb{E}_{\\theta}\\left(\\sum\\limits_{i=1}^{n}V(X_{i})\\right)^{2} \\\\\n&= \\sum\\limits_{i=1}^{n}\\mathbb{E}_{\\theta}[V^{2}(X_{i})] = nJ(\\theta)\n\\end{aligned}\n\\]\n\n\n\n\n\n Back to top"
  },
  {
    "objectID": "misc/posts/fisher-information/fisher-information.html",
    "href": "misc/posts/fisher-information/fisher-information.html",
    "title": "Fisher Information",
    "section": "",
    "text": "Introduction\nFisher information provides a way to measure the amount of information that a random variable contains about some parameter \\(\\theta\\) (such as the true mean) of the random variable’s assumed probability distribution.\n\n\nPreliminaries\nA standard problem in statistical estimation is to determine the parameters of a distribution from a sample of data drawn from that distribution. Let \\(X_{1}, \\cdots, X_{n}\\) be drawn i.i.d. from \\(\\mathcal{N}(\\theta,1)\\). Suppose we wish to estimate \\(\\theta\\) from a sample of size \\(n\\). We guess that the sample mean \\(\\bar{X}_{n} = \\dfrac{1}{n}\\sum\\limits X_i\\) is the best estimate of \\(\\theta\\) and we can indeed show that \\(\\bar{X}_{n}\\) is the minimum MSE unbiased estimator.\nLet \\(\\{f(x;\\theta)\\}, \\theta \\in \\Theta\\) denote an indexed family of densities so that \\(f(x;\\theta) \\geq 0, \\int f(x;\\theta)dx=1\\) for all \\(\\theta \\in \\Theta\\), where \\(\\Theta\\) is called the parameter set.\n\n[!definition] Estimator An estimator for \\(\\theta\\) for a sample size \\(n\\) is a function \\(T\\): \\(\\mathcal{X}^{n}\\rightarrow \\Theta\\).\n\nAn estimator is meant to approximate the value of the parameter, therefore it is desirable to have some idea of the goodness of the approximator. We call the difference \\(T-\\theta\\) the error of the estimator. This error itself is a random variable.\n\n[!definition] Bias The bias of an estimator \\(T(X_{1},X_{2},\\cdots, X_{n})\\) for the parameter \\(\\theta\\) is the expected value of the error of the estimator, i.e. \\[\\mathbb{E}_{\\theta}[T(X_{1}, X_{2},\\cdots, X_{n})-\\theta]\\]\n\nThe estimator is said to be unbiased if the bias is 0 for all \\(\\theta \\in \\Theta\\).\n\n[!definition] Consistency of probability An estimator \\(T(X_{1},X_{2},\\cdots, X_{n})\\) for \\(\\theta\\) is said to be consistent in probability if \\(T(X_{1},X_{2},\\cdots, X_{n}) \\rightarrow \\theta\\) in probability as \\(n\\rightarrow \\theta\\).\n\nWhile consistency is a desirable asymptotic property, we are also interested in the behavior for small sample size.\n\n[!definition] Dominate An estimator \\(T_{1}(X_{1},X_{2},\\cdots, X_{n})\\) is said to dominate another estimator \\(T_{2}(X_{1},X_{2},\\cdots, X_n)\\) if for all \\(\\theta\\) we have: \\[\\mathbb{E}[T_{1}(X_{1},X_{2}, \\cdots, X_{n})-\\theta ]^{2}\\leq \\mathbb{E}[T_{2}(X_{1},X_{2}, \\cdots, X_{n})-\\theta ]^{2}\\]\n\nThis raises a natural question: Is there a best estimator of \\(\\theta\\) that dominates every other estimator ? To answer this, we derive the Cramér-Rao lower bound on the MSE of any estimator.\n\n[!definition] Score The score \\(V\\) is a random variable defined by: \\[V = \\dfrac{\\partial}{\\partial \\theta} \\log f(X;\\theta)\\]\n\nThe mean value of the score is: \\[\n\\begin{aligned}\n\\mathbb{E}[V] &= \\int \\dfrac{\\partial}{\\partial \\theta}\\log f(x;\\theta) f(x;\\theta)dx \\\\\n&= \\int \\dfrac{\\frac{\\partial}{\\partial \\theta}f(x;\\theta)}{f(x;\\theta)} f(x;\\theta)dx \\\\\n&= \\int \\dfrac{\\partial}{\\partial \\theta}f(x;\\theta)dx \\\\\n&= \\dfrac{\\partial}{\\partial\\theta}\\int f(x;\\theta)dx = 0\n\\end{aligned}\n\\] therefore \\(\\mathbb{E}[V]^{2}= var(V)\\). The variance of the score has a special significance.\n\n[!definition] Fisher information The Fisher information \\(J(\\theta)\\) is the variance of the score: \\[J(\\theta) = \\mathbb{E}_{\\theta}\\left[\\dfrac{\\partial}{\\partial \\theta}\\log f(X;\\theta)\\right]^2\\]\n\nIf we consider a sample of \\(n\\) random variable \\(X_{1},X_{2},\\cdots, X_{n}\\) drawn i.i.d. from \\(f(x;\\theta)\\), we have: \\[\nf(x_{1},x_{2},\\cdots, x_{n;\\theta)}= \\prod_{i=1}^{n}f(x_{i};\\theta)\n\\] and the score function is the sum of the individual score functions: \\[\n\\begin{aligned}\nV(X_{1}, X_{2}, \\cdots, X_{n})\n&= \\dfrac{\\partial}{\\partial \\theta} \\log f(X_{1},X_{2}, \\cdots, X_{n}; \\theta) \\\\\n&= \\sum\\limits_{i=1}^{n}\\dfrac{\\partial}{\\partial \\theta} \\log f(X_{i}; \\theta) \\\\\n&= \\sum\\limits_{i=1}^{n}V(X_{i})\n\\end{aligned}\n\\] where the \\(V(X_{i})\\) are i.i.d. with zero mean, therefore the \\(n\\)-sample Fisher information is: \\[\n\\begin{aligned}\nJ_{n}(\\theta) &= \\mathbb{E}_{\\theta}\\left[\\dfrac{\\partial}{\\partial \\theta} \\log f(X_{1},X_{2},\\cdots, X_{n};\\theta)\\right]^{2} \\\\\n&= \\mathbb{E}_{\\theta}[V^{2}(X_{1},X_{2},\\cdots, X_{n})] \\\\&= \\mathbb{E}_{\\theta}\\left(\\sum\\limits_{i=1}^{n}V(X_{i})\\right)^{2} \\\\\n&= \\sum\\limits_{i=1}^{n}\\mathbb{E}_{\\theta}[V^{2}(X_{i})] = nJ(\\theta)\n\\end{aligned}\n\\]\n\n\n\n\n\n Back to top"
  },
  {
    "objectID": "misc/posts/score-matching-ref/sm-ref.html",
    "href": "misc/posts/score-matching-ref/sm-ref.html",
    "title": "Score Matching Reading",
    "section": "",
    "text": "https://jmlr.org/papers/volume6/hyvarinen05a/hyvarinen05a.pdf\nhttps://andrewcharlesjones.github.io/journal/21-score-matching.html\nhttps://random-walks.org/book/papers/score-matching/score-matching.html\nInterpretation and Generalization of Score Matching\nA Study of the Theoretical Foundations of Variational and Score Matching-based Diffusion Models\nhttps://jmtomczak.github.io/blog/16/16_score_matching.html\nhttps://fanpu.io/blog/2023/score-based-diffusion-models/\nhttps://francisbach.com/integration-by-parts-randomized-smoothing-score-functions/\nhttps://github.com/Ending2015a/toy_gradlogp\nRegularized estimation of image statistics by score matching. DP Kingma, Y Cun - Advances in neural information processing systems, 2010\n\n\n\n Back to top"
  },
  {
    "objectID": "misc/posts/clt/clt.html",
    "href": "misc/posts/clt/clt.html",
    "title": "Central Limit Theorem",
    "section": "",
    "text": "We know from the [[llns | Law of Large Numbers]] that \\(\\hat{X}_{n}-\\mu \\rightarrow 0\\) with probability 1. But what does the distribution look like ?\nThe Central Limit Theorem tells us that: \\[\n\\dfrac{\\sqrt{n} (\\hat{X}_{n}-\\mu)}{\\sigma} \\rightarrow \\mathcal{N}(0,1)\n\\] Equivalently, we can write this as: \\[\n\\dfrac{\\sum\\limits_{j=1}^{n}X_{j} - n\\mu}{\\sqrt{n} \\sigma} \\rightarrow \\mathcal{N}(0,1)\n\\]\nProof. Without loss generality, assume that \\(\\mu = 0\\) and \\(\\sigma = 1\\) and that the Moment Generating Function \\(M(t)\\) of \\(X\\) exists. Let \\(S_{n}=\\sum\\limits_{j=1}^{n}X_{j}\\). Show that that MGF of \\(\\dfrac{S_{n}}{\\sqrt{n}}\\) goes to the MGF of \\(\\mathcal{N}(0,1)\\). \\[\n\\mathbb{E}\\left(\\exp \\left(t \\dfrac{S_{n}}{\\sqrt{n}}\\right)\\right) = \\mathbb{E} \\left(\\exp \\left(t \\dfrac{X_{1}}{\\sqrt{n}}\\right)\\right) \\cdots \\left(\\exp \\left(t \\dfrac{X_{n}}{\\sqrt{n}}\\right)\\right) = \\left(M\\left(\\dfrac{t}{\\sqrt{n}}\\right)\\right)^{n}\n\\]\nTaking the log and the limit of the MGF, we obtain that: \\[\n\\begin{aligned}\n\\underset{y \\rightarrow 0}{\\lim} n \\log M\\left(\\frac{t}{\\sqrt{n}}\\right)\n&= \\underset{y \\rightarrow 0}{\\lim} \\dfrac{\\log M(\\frac{t}{\\sqrt{n}})}{\\frac{1}{n}} && (\\text{simple rewrite})\\\\\n&= \\underset{y \\rightarrow 0}{\\lim} \\dfrac{\\log M(yt)}{y^{2}} && (\\text{for $y = 1/\\sqrt{n} $})\\\\\n&= \\underset{y \\rightarrow 0}{\\lim} \\dfrac{tM'(yt)}{2y M(yt)} && (\\text{using L'Hôpital's rule}) \\\\\n&= \\dfrac{t}{2} \\underset{y \\rightarrow 0}{\\lim} \\dfrac{M'(yt)}{y} && (M(0)=1) \\\\\n& = \\dfrac{t^{2}}{2} \\underset{y \\rightarrow 0}{\\lim} \\dfrac{M''(yt)}{1} = \\frac{t^{2}}{2} && (\\text{using  L'Hôpital's rule again and $M''(0)=1$})\n\\end{aligned}\n\\]\n\n\nApplications\n\nIf we flip a fair coin 100 times. What is the probability that it lands more than 55 heads ?\nIf we flip a fair coin 400 times. What is the probability that it lands more than 220 heads ?\n\nSolution: (a) Let \\(X_{j}\\) be the result of the \\(j^{th}\\) flip, so that \\(X_{j}= 1\\) for heads and \\(X_{j}=0\\) for tails. The total number of head is therefore: \\[\nS = X_{1}+ X_{2} + \\cdots + X_{100}\n\\] We know that \\(\\mathbb{E}[X_{j}] = 0.5\\) and \\(\\text{Var}(X_{j}) = 1/4\\). Since \\(n=100\\), we have: \\[\\mathbb{E}[S] = 50 \\qquad \\text{Var}(S) = 25 \\qquad \\sigma_{S}=5\\] Now, the Central Limit Theorem says that \\(Z = \\dfrac{S-\\mathbb{E}[S]}{\\sigma_{S}}\\) is approximately \\(\\mathcal{N}(0,1)\\). We get: \\[\nP(S &gt; 55) = P\\left(\\dfrac{S-50}{5} &gt; \\dfrac{55-50}{5}\\right) \\approx P(Z&gt;1) = 0.16\n\\] (b) Going through an analogous process as we did in (a), using the Central Limit Theorem, we get: \\[\nP(S &gt; 220) = P\\left(\\dfrac{S-200}{10} &gt; \\dfrac{220-200}{10}\\right) \\approx P(Z&gt;2) = 0.025\n\\]\n\n[!remark] Although 55/100 is exactly equal to 220/400, the probability of more than 55 heads in 100 flips is larger than the probability of more than 220 heads in 400 flips. This is due to the [[llns | Law of Large Numbers]].\n\n\n\nHow large does \\(n\\) need to be ?\n\n\nReferences\n\n\n\n\n Back to top"
  },
  {
    "objectID": "misc/posts/kl-divergence/kl-divergence.html",
    "href": "misc/posts/kl-divergence/kl-divergence.html",
    "title": "KL Divergence",
    "section": "",
    "text": "#prob-stats #divergence # KL Divergence # Introduction\nThe Kullback-Leibler Divergence, or KL Divergence, is defined as follows:"
  },
  {
    "objectID": "misc/posts/kl-divergence/kl-divergence.html#nonnegativity",
    "href": "misc/posts/kl-divergence/kl-divergence.html#nonnegativity",
    "title": "KL Divergence",
    "section": "Nonnegativity",
    "text": "Nonnegativity\n\\(D_{KL}(p \\| q) \\geq 0\\) with equality iff \\(p=q\\).\nBefore proving this, first recall the [[jensen-inequality |Jensen’s inequality]], which states the following for any convex function \\(f\\): \\[\nf\\left(\\sum\\limits_{i=1}^{n}\\lambda_{i}x_{i}\\right)\\leq \\sum\\limits_{i=1}^{n}\\lambda_{i}f(x_{i})\n\\] where \\(\\lambda_{i} \\geq 0\\) and \\(\\sum\\limits_{i=1}^{n}\\lambda_{i}=1\\).\nProof \\[\\begin{aligned}\nD_{\\mathrm{KL}}(P\\|Q)& =\\sum_{x\\in\\mathcal{X}}P(x)\\log\\frac{P(x)}{Q(x)} && \\text{(by definition for discrete case)}  \\\\\n&=-\\sum_{x\\in\\mathcal{X}}P(x)\\log\\frac{Q(x)}{P(x)} && \\text{(changing the sign of the $\\log$)}\\\\\n&=E\\bigg[-\\log\\frac{Q(x)}{P(x)}\\bigg] && \\text{(by Law of Unconscious stastistician)}\\\\\n&\\geq-\\log E\\Big[\\frac{Q(x)}{P(x)}\\Big] && \\text{(by Jensen's inequality)}\\\\\n&=-\\log\\Big(\\sum_{x\\in\\mathcal{X}}P(x)\\frac{Q(x)}{P(x)}\\Big) && \\text{(deriving the expectation term)}\\\\\n&=-\\log\\sum_{x\\in\\mathcal{X}}Q(x) && \\text{(cancelling out the $P(x)$)}\\\\\n&=-\\log1 =0\n\\end{aligned}\\] # KL Divergence and MLE\nSuppose we want to find the distribution \\(q\\) that is as closes as possible to \\(p\\), as measured by KL Divergence: \\[q^{*}= \\underset{q}{\\text{argmin}} D_{KL}(p \\| q) = \\underset{q}{\\text{argmin}} \\int p(x) \\log p(x)dx - \\int p(x) \\log q(x)dx\\]\nSuppose we are given the training set \\(\\{x^{(i)}; i=1,\\cdots,m\\}\\). Let \\(\\hat{P}(x) = \\dfrac{1}{m} \\sum\\limits_{i=1}^{m}1\\{x^{(i)}=x\\}\\) be the empirical distribution and suppose we have some family of distributions \\(P_{\\theta}\\). We can actually show that finding the maximum likelihood estimate for the parameter \\(\\theta\\) is equivalent to finding \\(P_{\\theta}\\) with minimal KL Divergence from \\(\\hat{P}\\), i.e.: \\[\n\\underset{\\theta}{\\text{argmin}} D_{KL}(\\hat{P} \\| P_{\\theta}) = \\underset{\\theta}{\\text{argmax}} \\sum\\limits_{i=1}^{m}\\log P_{\\theta}(x^{(i)})\n\\] # Approximating KL Divergence\nhttp://joschu.net/blog/kl-approx.html\nSometimes we can not compute analytically the sum over \\(x\\) for various reasons, including (1) high computation cost and memory; (2) no closed-form expression; (3) code simplification (by just storing the log probability rather than the whole distribution itself).\nOne of the most common approach to estimate sums or integrals is to use [[Monte-Carlo estimate]]. Given the samples \\(x_{1}, x_{2}, \\cdots \\sim q\\), the question is how can we construct a good estimate ?\nA good estimator is one that is unbiased (it has the right mean) and has low variance.\nAll \\(f\\)-divergences with differentiable \\(f\\) look like KL divergence up to second order when \\(q\\) is close to \\(p\\). For a parameterized distribution \\(p_\\theta\\): \\[\nD_{f}(p_{0},p_{\\theta}) = \\dfrac{f''(1)}{2} \\theta^{T}F\\theta + O(\\theta^3)\n\\] where \\(F\\) is the Fisher information matrix for \\(p_{\\theta}\\) evaluated at \\(p_{\\theta}= p_{0}\\)."
  },
  {
    "objectID": "misc/posts/kl-divergence/kl-divergence.html#approximating-kl-divergence-with-fisher-information-matrix",
    "href": "misc/posts/kl-divergence/kl-divergence.html#approximating-kl-divergence-with-fisher-information-matrix",
    "title": "KL Divergence",
    "section": "Approximating KL Divergence with Fisher information matrix",
    "text": "Approximating KL Divergence with Fisher information matrix\nLet \\(p_{\\theta}(x)\\) and \\(p_{\\theta'}(x)\\) be two distributions, where \\(\\theta' = \\theta + \\delta\\). We can measure how close the second distribution is to the first in terms of their predictive distribution (comparing \\(\\theta\\) and \\(\\theta\\)’ in the parameter space): \\[D_{KL}(p_{\\theta} \\| p_{\\theta'}) = \\mathbb{E}_{p_{\\theta}(x)}[\\log p_{\\theta}(x) - \\log p_{\\theta'}(x)]\\] Approximate with a second-order Taylor series expansion: \\[D_{KL}(p_{\\theta}\\| p_{\\theta'}) \\approx  -\\delta^{T}\\mathbb{E}[\\nabla \\log  p_{\\theta}(x)] - \\dfrac{1}{2}\\delta^{T}\\mathbb{E}[\\nabla^{2}\\log p_{\\theta}(x)]\\delta\\] Since \\(\\mathbb{E}[\\nabla \\log p_\\theta (x)] = 0\\), we have: \\[D_{KL}(p_{\\theta}\\| p_{\\theta'}) \\approx \\dfrac{1}{2} \\delta^{T}F(\\theta)\\delta\\] where F is the Fisher information matrix defined as: \\[F = -\\mathbb{E}[\\nabla^{2}\\log p_{\\theta}(x)] = \\mathbb{E}[(\\nabla \\log p_{\\theta}(x))(\\log p_{\\theta}(x))^{T}]\\] # KL Divergence and exponential families\nIn this section, we’ll show that the KL between two distributions in the exponential family has a nice closed form. Consider \\(p(x)\\) with a natural parameter \\(\\eta\\), base measure \\(h(x)\\) and sufficient statistics \\(\\mathcal{T}(x)\\), i.e. \\[p(x) = h(x) \\exp [\\eta^{T}\\mathcal{T}(x) - A(\\eta)]\\] where \\(A(\\eta) = \\log \\int h(x) \\exp(\\eta^{T}\\mathcal{T}(x))dx\\) is the log-partition function and is a convex function of \\(\\eta\\).\nThe KL divergence between two exponential family distributions from the same family is as follows: \\[\n\\begin{aligned}\nD_{KL}(p(x | \\eta_{1})\\| p(x | \\eta_{2})) &= \\mathbb{E}_{\\eta_{1}}[(\\eta_{1}-\\eta_{2})^{T}\\mathcal{T}(x) - A(\\eta_{1})+ A(\\eta_{2})] \\\\\n&= (\\eta_{1}-\\eta_{2})^{T}\\mu_{1}-A(\\eta_{1})+A(\\eta_{2})\n\\end{aligned}\n\\]"
  },
  {
    "objectID": "module4.html#main-lessons",
    "href": "module4.html#main-lessons",
    "title": "Module 4 - Machine Learning and Data Science",
    "section": "Main Lessons",
    "text": "Main Lessons\n\n\n\n\n\n\n\n\n\n\n\nDate\nType\nContent\nInstructor\nHandout\nFurther Reading\n\n\n\n\n\n\n\n\n\n\n\n\n28/08/23\nAssignment\nK-Nearest Neighbor and Decision Tree Exercise\nTA Thái\n[solution]\n\n\n\n30/08/23\nMain Lesson\nK-Nearest Neighbors\nDr. Đình Vinh\n230830\n[note] [CS4780 Lecture 3]\n\n\n01/09/23\nMain Lesson\nDecision Tree for Classification\nDr. Đình Vinh\n230901\n\n\n\n03/09/23\nExercise Session\nTA Exercise\nTA Thái\n230903\n\n\n\n06/09/23\nMain Lesson\nDecision Tree for Regression\nDr. Đình Vinh\n230906\n[Choi et al., 2017]\n\n\n08/09/23\nMain Lesson\nRandom Forest\nDr. Đình Vinh\n230908\n[Louppe et al., 2015]\n\n\n10/09/23\nExercise Session\nTA Exercise\nTA Thắng\n230910\n\n\n\n11/09/23\nAssignment\nXGBoost\nTA Khoa\n[solution]\n\n\n\n13/09/23\nMain Lesson\nBasic XGBoost: Understanding Gradient Boost and AdaBoost\nDr. Đình Vinh\n230913\n[note] [Trevor Lecture]\n\n\n15/09/23\nMain Lesson\nAdvanced XGBoost: Fully XGBoost and its mathematics\nDr. Đình Vinh\n230915\n\n\n\n17/09/23\nExercise Session\nTA Exercise\nTA Khoa\n230917\n\n\n\n18/09/23\nAssignment\nSupport Vector Machine Exercise\nTA Thắng\n[solution]\n\n\n\n20/09/23\nMain Lesson\nReview on Tree-based Approaches and Discussion\nDr. Đình Vinh\n230920\n\n\n\n22/09/23\nMain Lesson\nSupport Vector Machine (1)\nDr. Đình Vinh\n230922\n[CS229 Note] [MLCB]\n\n\n24/09/23\nMain Lesson\nSupport Vector Machine (2)\nDr. Đình Vinh\n230924\n\n\n\n27/09/23\nExercise Session\nTA-Exercise\nTA Thắng\n230927\n\n\n\n29/09/23\nProject Tutorial\nTabular Data Project: Heart Disease Prediction\nDr. Đình Vinh\n230929\n\n\n\n01/10/23\nProject Tutorial\nImage Project: Object Detection\nTA Thắng\n231001\n[HoG paper]",
    "crumbs": [
      "Resources",
      "Course content",
      "Module 4"
    ]
  },
  {
    "objectID": "module4.html#python-tutorial",
    "href": "module4.html#python-tutorial",
    "title": "Module 4 - Machine Learning and Data Science",
    "section": "Python Tutorial",
    "text": "Python Tutorial\n\n\n\n\n\n\n\n\n\n\n\nDate\nType\nContent\nInstructor\nHandout\nFurther Reading\n\n\n\n\n\n\n\n\n\n\n\n\n29/08/23\nPreview\nK-Nearest Neighbors (Warm-up)\nDr. Vinh\n230829\n[STAT451 Note]\n\n\n05/09/23\nPython Tutorial\nDecision Tree\n\n230905\n\n\n\n12/09/23\nPython Tutorial\nRandom Forest and AdaBoost (Warm-up)\n\n230912\n\n\n\n21/09/23\nPython Tutorial\nSVM\nTA Thắng\n230921\n[CS4780 Note]",
    "crumbs": [
      "Resources",
      "Course content",
      "Module 4"
    ]
  },
  {
    "objectID": "module4.html#extra-class-advanced-nlp",
    "href": "module4.html#extra-class-advanced-nlp",
    "title": "Module 4 - Machine Learning and Data Science",
    "section": "Extra class: Advanced NLP",
    "text": "Extra class: Advanced NLP\n\n\n\n\n\n\n\n\n\n\n\nDate\nType\nContent\nInstructor\nHandout\nFurther Reading\n\n\n\n\n\n\n\n\n\n\n\n\n09/09/23\nAdvanced NLP\nFoundations for an NLP topic\nTA Thái\n230909\n[note] [CS324 note]\n\n\n16/09/23\nAdvanced NLP\nLanguage Models and Prompting Techniques\n\n230916\n\n\n\n23/09/23\nAdvanced NLP\nParameter-Efficient Fine-tuning\n\n230923\n[Houlsby et al., 2019] [COS597G Slides]\n\n\n30/09/23\nAdvanced NLP\nEfficient Fine-tuning of quantized LLMs\n\n230930\n[LoRA paper]",
    "crumbs": [
      "Resources",
      "Course content",
      "Module 4"
    ]
  },
  {
    "objectID": "misc/posts/blogs/vae/vae.html",
    "href": "misc/posts/blogs/vae/vae.html",
    "title": "Variational Autoencoders - Fundamental concepts",
    "section": "",
    "text": "Variational Autoencoder, or VAE, is one of the most important class of the generative model family, where later generateive models are built upon. The paper by Kingma and Welling (2013) which proposes the VAE model has recently won the ICLR 2024 Test of Time award, which indicates the important impact of VAE model. To quote their comment:\n\nProbabilistic modeling is one of the most fundamental ways in which we reason about the world. This (VAE) paper spearheaded the integration of deep learning with scalable probabilistic inference (amortized mean-field variational inference via a so-called reparameterization trick), giving rise to the Variational Autoencoder (VAE). The lasting value of this work is rooted in its elegance. The principles used to develop VAEs deepened our understanding of the interplay between deep learning and probabilistic modeling, and sparked the development of many subsequent interesting probabilistic models and encoding approaches.\n\n\n\n\n\n\n\nNote\n\n\n\nThis article is an excerpt from Tutorial on Diffusion Models for Imaging and Vision by (Chan 2024) with some add-ons and modifications."
  },
  {
    "objectID": "misc/posts/blogs/vae/vae.html#jensens-inequality",
    "href": "misc/posts/blogs/vae/vae.html#jensens-inequality",
    "title": "Variational Autoencoders - Fundamental concepts",
    "section": "Jensen’s inequality",
    "text": "Jensen’s inequality\nJensen’s inequality is a useful concept for any convex function that has many applications in generative models. A convex function can be defined as follows:\n\n\n\n\n\n\nDefinition of convex function (Boyd and Vandenberghe 2004)\n\n\n\nA function \\(f: \\mathbb{R}^n \\mapsto \\mathbb{R}\\) is said to be convex if its domain \\(\\textbf{dom}f\\) is a convex set and if for all \\(x,y \\in \\textbf{dom}f\\), \\(\\theta \\in [0,1]\\), we have:\n\\[f(\\theta x + (1-\\theta) y) \\leq \\theta f(x) + (1-\\theta) f(y)\\]\n\n\nFor example, consider the figure below where the figure on the left is the graph of a convex function \\(f(x) = x^2\\), the line (sometimes also referred to as chord) between any two points \\(x,y\\) on the graph lies above the graph, i.e. every chord is below the function. In contrast, the function on the right does not have such property so it is not a convex function. The definition that we have for a convex function is just a generalization and mathematical expression of this property.\n\n\nGiven a convex function \\(f\\) and a random variable \\(X\\), Jensen’s inequality states that:\n\n\n\n\n\n\nJensen’s inequality\n\n\n\n\\[\\mathbb{E}[f(X)] \\geq f(\\mathbb{E}[X])\\]\n\n\nThe probabilistic proof of this inequality is given in Section 7. Later in this section, we will apply the Jensen’s inequality to derive the Evidence Lower Bound, which is the workhorse of VAE."
  },
  {
    "objectID": "misc/posts/blogs/vae/vae.html#kl-divergence",
    "href": "misc/posts/blogs/vae/vae.html#kl-divergence",
    "title": "Variational Autoencoders - Fundamental concepts",
    "section": "KL Divergence",
    "text": "KL Divergence\nSometimes it is useful for us to have a measure \\(D\\) to quantify the difference between two distributions. We require that measure to have the following properties (Murphy 2023):\n\nContinuous in its arguments\nNonnegativity\nPermutation invariant\nMonotonic\nSatisfy the natural chain rule\n\nKL divergence turns out to be that useful tool and it is defined as follows:\n\n\n\n\n\n\nKL Divergence\n\n\n\n\\[D_{KL} = \\int p(x) \\log \\dfrac{p(x)}{q(x)} dx\\]\n\n\nInterested readers can refer to (Murphy 2023) to see the proof that KL Divergence satisfies all of the desiderata. Here, we’ll only give a proof of the nonnegativity of the KL Divergence in Section 8 because this property will be useful later when we derive the ELBO in Section 4."
  },
  {
    "objectID": "misc/posts/blogs/vae/vae.html#autoencoder",
    "href": "misc/posts/blogs/vae/vae.html#autoencoder",
    "title": "Variational Autoencoders - Fundamental concepts",
    "section": "Autoencoder",
    "text": "Autoencoder\nAutoencoder was traditionally used for dimensionality reduction or feature learning Hinton and Salakhutdinov (2006). This is the case because learning an undercomplete representation forces the autoencoder to capture the most salient features of the training data .It was not until recently when theoretical connections between autoencoder and latent variable models were established that brought autoencoder to the forefront of generative models.\nWhile VAEs (Kingma and Welling 2013) and the deterministic autoencoder (AE) are very similar, there are two main differences:\n\nin the AE, the objective is the log likelihood of the reconstruction without any KL term\nThe encoding is deterministic, so that the encoder network just needs to compute \\(\\mathbb{E}[\\mathbf{z} \\vert \\mathbf{x}]\\) and not \\(\\text{Var}[\\mathbf{z} \\vert \\mathbf{x}]\\).\n\nThe main advantage of VAE over a deterministic autoencoder is that it defines a proper generative model that can create natural-looking novel images by decoding prior samples \\(\\mathbf{z} \\sim \\mathcal{N}(0, \\mathbf{I})\\). This is in constrast with the autoencoder, where it only knows how to decode the latent codes derived from the training set, so it does poorly on random inputs, as shown in Figure Figure 1.\n\n\n\n\n\n\nFigure 1: celebA produced by Autoencoder, VAE, and beta-VAE"
  },
  {
    "objectID": "misc/posts/blogs/vae/vae.html#proxy-distributions",
    "href": "misc/posts/blogs/vae/vae.html#proxy-distributions",
    "title": "Variational Autoencoders - Fundamental concepts",
    "section": "Proxy distributions",
    "text": "Proxy distributions\nSince we don’t have access to the two high-dimensional distributions associated with the encoder \\(p(\\mathbf{z} \\vert \\mathbf{x})\\) and the decoder \\(p(\\mathbf{x} \\vert \\mathbf{z})\\), we need to come up with a way to approximate it. Consider the following two distributions:\n\n\\(q_{\\phi}(\\mathbf{z} \\vert \\mathbf{x})\\), which is the proxy for \\(p(\\mathbf{z} \\vert \\mathbf{x})\\) and we assume this to be a Gaussian distribution.\n\\(p_{\\theta}(\\mathbf{x} \\vert \\mathbf{z})\\), which is the proxy for \\(p(\\mathbf{x} \\vert \\mathbf{z})\\), and we also assume this to be a Gaussian distribution.\n\n\n\n\n\n\nHere, \\(\\phi\\) and \\(\\theta\\) are the parameters that we need to optimize. Proxy distributions are ditributions that are approximate of other true distributions."
  },
  {
    "objectID": "misc/posts/blogs/vae/vae.html#encoder",
    "href": "misc/posts/blogs/vae/vae.html#encoder",
    "title": "Variational Autoencoders - Fundamental concepts",
    "section": "Encoder",
    "text": "Encoder\nTo train the VAE, we need to know the ground truth pair \\((\\mathbf{x},\\mathbf{z})\\). We know that \\(\\mathbf{z}\\) is generated from the distribution \\(q_{\\phi}(\\mathbf{z} \\vert \\mathbf{x})\\), and we also know that \\(q_{\\phi}(\\mathbf{z} \\vert \\mathbf{x})\\) is also a Gaussian. Assume that this Gaussian has mean \\(\\boldsymbol{\\mu}\\) and variance \\(\\sigma^{2}\\mathbf{I}\\). Now, the difficult part is how to obtain \\(\\boldsymbol{\\mu}\\) and covariance \\(\\sigma^2\\mathbf{I}\\) from the input image \\(\\mathbf{x}\\). The answer is, we will construct a neural network such that \\[\\boldsymbol{\\mu} = \\underbrace{\\boldsymbol{\\mu}_\\phi(\\mathbf{x})}_{\\text{neural network}}\\] \\[\\sigma^{2}= \\underbrace{\\sigma^{2}_{\\phi}(\\mathbf{x})}_{\\text{neural network}}\\] Now, the \\(\\ell^{th}\\) training sample of the training set \\(\\mathbf{z}^{(\\ell)}\\) can be sampled from the Gaussian distribution: \\[\\mathbf{z}^{(\\ell)} \\sim q_{\\phi}(\\mathbf{z} \\vert \\mathbf{x}^{(\\ell)}),\\quad \\text{where } q_{\\phi}(\\mathbf{z} \\vert \\mathbf{x}^{(\\ell)})=\\mathcal{N}(\\mathbf{z} \\vert \\boldsymbol{\\mu}_\\phi(\\mathbf{x}^{(\\ell)}), \\sigma^{2}_{\\phi}(\\mathbf{x}^{(\\ell)}))\\] Here, note that \\(\\boldsymbol{\\mu}_\\phi(\\mathbf{x}^{(\\ell)})\\) and \\(\\sigma^{2}_{\\phi}(\\mathbf{x}^{(\\ell)})\\) are functions of \\(\\mathbf{x}^{(\\ell)}\\), so for every different \\(\\mathbf{x}^{(\\ell)}\\), we need to come up with a different Gaussian."
  },
  {
    "objectID": "misc/posts/blogs/vae/vae.html#decoder",
    "href": "misc/posts/blogs/vae/vae.html#decoder",
    "title": "Variational Autoencoders - Fundamental concepts",
    "section": "Decoder",
    "text": "Decoder\nThe job of the decoder network is to reconstruct the image \\(\\mathbf{x}\\) given the latent vector \\(\\mathbf{z}\\), which is encoded from the original image via the encoder. \\[\\hat{\\mathbf{x}} = \\text{decode}_\\theta(\\mathbf{z})\\] One assumption that we’ll make is that the error between the reconstructed image \\(\\hat{\\mathbf{x}}\\) and the original input image \\(\\mathbf{x}\\) is Gaussian, i.e. \\[(\\hat{\\mathbf{x}}-\\mathbf{x}) \\sim \\mathcal{N}(0, \\sigma^{2}_\\text{dec})\\] It then follows that the distribution \\(p_{\\theta}(\\mathbf{x} \\vert \\mathbf{z})\\) is: \\[\n\\begin{aligned}\n\\log p_{\\theta}(\\mathbf{x} \\vert \\mathbf{z})\n&= \\log \\mathcal{N}(\\mathbf{x} \\vert \\text{decode}_{\\theta}(\\mathbf{x}), \\sigma^{2}_\\text{dec}\\mathbf{I}) \\\\\n&= \\log \\dfrac{1}{\\sqrt{(2\\pi \\sigma^{2}_\\text{dec})^{D}}} \\exp \\left(- \\dfrac{\\|\\mathbf{x} - \\text{decode}_{\\theta}(\\mathbf{z})\\|^2}{2 \\sigma^{2}_\\text{dec}}\\right)\\\\\n&= - \\dfrac{\\|\\mathbf{x} - \\text{decode}_{\\theta}(\\mathbf{z})\\|^2}{2 \\sigma^{2}_\\text{dec}}-\\log\\sqrt{(2\\pi\\sigma^2_\\text{dec})^D}\\end{aligned}\n\\] where \\(D\\) is the dimension of \\(\\mathbf{x}\\). Through this derivation, we know that maximizing the likelihood term in ELBO is equivalent to minimizing the \\(\\ell_{2}\\) loss between the ground truth image and the reconstructed image, which makes a lot of sense."
  },
  {
    "objectID": "misc/blog.html",
    "href": "misc/blog.html",
    "title": "Blogs",
    "section": "",
    "text": "Variational Autoencoders - Fundamental concepts\n\n\n\n\n\nIn this article we’re going to explore Variational Autoencoder (VAE), a class of generative models. This is the first part of a two-series blogpost that cover the fundamnetal concepts behind VAE.\n\n\n\n\n\nAug 20, 2024\n\n\n\n\n\n\nNo matching items\n\n Back to top"
  },
  {
    "objectID": "module4.html#python-support",
    "href": "module4.html#python-support",
    "title": "Module 4 - Machine Learning and Data Science",
    "section": "Python Support",
    "text": "Python Support\n\n\n\n\n\n\n\n\n\n\n\nDate\nType\nContent\nInstructor\nHandout\nFurther Reading\n\n\n\n\n\n\n\n\n\n\n\n\n29/08/23\nPython Support\nK-Nearest Neighbors (Warm-up)\nDr. Vinh\n230829\n[STAT451 Note]\n\n\n05/09/23\nPython Support\nDecision Tree\nDr. Vinh\n230905\n\n\n\n12/09/23\nPython Support\nRandom Forest and AdaBoost (Warm-up)\nDr. Vinh\n230912\n\n\n\n21/09/23\nPython Support\nSVM\nTA Thắng\n230921\n[CS4780 Note]",
    "crumbs": [
      "Resources",
      "Course content",
      "Module 4"
    ]
  },
  {
    "objectID": "module5.html#main-lessons",
    "href": "module5.html#main-lessons",
    "title": "Module 5 - Deep Learning Prerequisites",
    "section": "Main lessons",
    "text": "Main lessons\n\n\n\n\n\n\n\n\n\n\n\nDate\nType\nContent\nInstructor\nHandout\nFurther Reading\n\n\n\n\n02/10/23\nAssignment\nLogistic Regression Exercise\nTA Thắng\n[solution]\n\n\n\n04/10/23\nMain Lesson\nFrom Linear Regression to Logistic Regression\nDr. Vinh\n231004\n\n\n\n06/10/23\nMain Lesson\nLogistic Regression - Vectorization and Applications\nDr. Vinh\n231006\n[Jurafsky et al., C5, 2023]\n\n\n08/10/23\nExercise Session\nTA Exercise\nTA Thắng\n231008\n\n\n\n09/10/23\nAssignment\nSoftmax Regression Exercise\nTA Thắng\n[solution]\n\n\n\n11/10/23\nMain Lesson\nSoftmax Regression (Multiclass Classification)\nDr. Vinh\n231011\n\n\n\n13/10/23\nMain Lesson\nPytorch Framework (Implementation for regression)\nDr. Vinh\n231013\n\n\n\n15/10/23\nExercise Session\nTA Exercise\nTA Khoa\n231015\n\n\n\n16/10/23\nAssignment\nMLP Exercise\nTA Thắng\n[Exercise]\n\n\n\n18/10/23\nMain Lesson\nMultilayer Perceptron\nDr. Vinh\n231018\n\n\n\n20/10/23\nMain Lesson\nActivations and Initializers\nDr. Vinh\n231020\n\n\n\n22/10/23\nExercise Session\nTA Exercise\nTA Khoa\n231022\n\n\n\n25/10/23\nMain Lesson\nOptimizers for Neural Network (1)\nDr. Vinh\n231025\n\n\n\n27/10/23\nMain Lesson\nOptimizers for Neural Networks (2)\nDr. Vinh\n231027\n\n\n\n29/10/23\nExercise Session\nTA-Exercise (Optimization methods)\nTA Khoa\n231029\n\n\n\n01/11/23\nProject Tutorial\nText data: Sentiment Analysis\nTA Thái\n231101\n\n\n\n03/11/23\nProject Tutorial\nTime-series Data Project: Music Genre Classification\nTA Bảo\n231103\n\n\n\n05/11/23\nProject Tutorial\nImage Data Project: Gradient Vanishing in MLP\nTA Khoa\n231105",
    "crumbs": [
      "Resources",
      "Course content",
      "Module 5"
    ]
  },
  {
    "objectID": "module5.html#python-tutorial-session",
    "href": "module5.html#python-tutorial-session",
    "title": "Module 5 - Deep Learning Prerequisites",
    "section": "Python Tutorial Session",
    "text": "Python Tutorial Session\n\n\n\n\n\n\n\n\n\n\n\nDate\nType\nContent\nInstructor\nHandout\nFurther Reading\n\n\n\n\n03/10/23\nPreview\nLogistic Regression\nTA Thái\n231003\n\n\n\n10/10/23\nPreview\nSoftmax Regression\n\n231010\n\n\n\n17/10/23\nPreview\nMultilayer Perceptron\n\n231017\n\n\n\n24/10/23\nPreview\nSGD+Momentum\n\n231024",
    "crumbs": [
      "Resources",
      "Course content",
      "Module 5"
    ]
  },
  {
    "objectID": "module5.html#introduction-to-computer-vision",
    "href": "module5.html#introduction-to-computer-vision",
    "title": "Module 5 - Deep Learning Prerequisites",
    "section": "Introduction to Computer Vision",
    "text": "Introduction to Computer Vision\n\n\n\n\n\n\n\n\n\n\n\nDate\nType\nContent\nInstructor\nHandout\nFurther Reading\n\n\n\n\n07/10/23\nComputer Vision\nIntroduction to CV and Background subtraction\nDr. Đình Vinh\n231007\n\n\n\n14/10/23\nComputer Vision\nLane Detection\n\n231014\n\n\n\n21/10/23\nComputer Vision\nImage Stitching (panorama)\n\n231021\n\n\n\n28/10/23\nComputer Vision\nFace Detection\n\n231028\n\n\n\n04/11/23\nComputer Vision\nObject Tracking using Mean Shift/Cam Shift\n\n231104",
    "crumbs": [
      "Resources",
      "Course content",
      "Module 5"
    ]
  },
  {
    "objectID": "module5.html#competition-training",
    "href": "module5.html#competition-training",
    "title": "Module 5 - Deep Learning Prerequisites",
    "section": "Competition Training",
    "text": "Competition Training\n\n\n\n\n\n\n\n\n\n\n\nDate\nType\nContent\nInstructor\nHandout\nFurther Reading\n\n\n\n\n05/10/23\nCompetition\nIntroduction to Imbalance Data\nTA Hùng\n231005\n\n\n\n12/10/23\nCompetition\nModel Evaluation\nTA Hùng\n231012\n\n\n\n19/10/23\nCompetition\nKalapa Challenge\nTA Hùng\n231019\n\n\n\n02/11/23\nCompetition\nData Sampling\nTA Hùng\n231102",
    "crumbs": [
      "Resources",
      "Course content",
      "Module 5"
    ]
  },
  {
    "objectID": "module6.html#main-lessons",
    "href": "module6.html#main-lessons",
    "title": "Module 6 - Introduction to Deep Learning",
    "section": "",
    "text": "Date\nType\nContent\nInstructor\nHandout\nFurther Reading\n\n\n\n\n06/11/23\nAssignment\nConvolutional Neural Network Exercise\nTA Thái\n[solution]\n\n\n\n08/11/23\nMain Lesson\nBasic CNN (1)\nDr. Vinh\n231108\n[CS231N Note] [CNN Explainer]\n\n\n10/11/23\nMain Lesson\nBasic CNN (2)\nDr. Vinh\n231110\n\n\n\n12/11/23\nExercise Session\nTA-Exercise\nTA Thắng\n231112\n\n\n\n13/11/23\nAssignment\nConvolutional Neural Network Exercise\nTA Thắng\n[solution]\n\n\n\n15/11/23\nMain Lesson\nCNN Training\nDr. Vinh\n231115\n[VGG Paper]\n\n\n17/11/23\nMain Lesson\nCNN Generalization\nDr. Vinh\n231117\n\n\n\n19/11/23\nExercise Session\nTA-Exercise\nTA Thắng\n231119\n\n\n\n20/11/23\nAssignment\nPretrained Models for Image Exercise\nTA Thắng\n[solution]\n\n\n\n22/11/23\nMain Lesson\nAdvanced CNN Architecture\nDr. Vinh\n231122\n\n\n\n24/11/23\nMain Lesson\nTransfer Learning for CNN\nDr. Vinh\n231124\n\n\n\n26/11/23\nExercise Session\nTA-Exercise\nTA Thái\n231126\n\n\n\n27/11/23\nAssignment\nRecurrent Neural Network Exercise\nTA Thắng\n\n\n\n\n29/11/23\nMain Lesson\nBasic RNN/LSTM (Time series and Text)\nDr. Vinh\n231129\n\n\n\n01/12/23\nMain Lesson\nBasic RNN/LSTM (Time series and Text)\nDr. Vinh\n231201\n\n\n\n03/12/23\nExercise Session\nTA-Exercise\nTA Thắng\n231203\n\n\n\n04/12/23\nAssignment\nTransformer Application Exercise\n\n\n\n\n\n06/12/23\nMain Lesson\nTransformer (Encoder - Text Classification)\nDr. Vinh\n231206\nBERT Readings\n\n\n08/12/23\nMain Lesson\nTransformer for Image and Time series Data\nDr. Vinh\n231208\n\n\n\n13/12/23\nProject Tutorial\nImage Project: OCR with YOLOv8 and CNN (Scene Text Recognition)\nTA Thắng\n231213\n\n\n\n15/12/23\nProject Tutorial\nImage-text Project: VQA\nTA Thắng\n231215\n\n\n\n17/12/23\nProject Tutorial\nTime-series forecasting project\nDr. Đình Vinh\n231217",
    "crumbs": [
      "Resources",
      "Course content",
      "Module 6"
    ]
  },
  {
    "objectID": "module6.html#competition-training",
    "href": "module6.html#competition-training",
    "title": "Module 6 - Introduction to Deep Learning",
    "section": "",
    "text": "Date\nType\nContent\nInstructor\nHandout\nFurther Reading\n\n\n\n\n09/11/23\nCompetition\nZalo AI\nTA Hùng\n231109\n\n\n\n16/11/23\nCompetition\nZalo AI\n\n231116",
    "crumbs": [
      "Resources",
      "Course content",
      "Module 6"
    ]
  },
  {
    "objectID": "module6.html#python-review",
    "href": "module6.html#python-review",
    "title": "Module 6 - Introduction to Deep Learning",
    "section": "",
    "text": "Date\nType\nContent\nInstructor\nHandout\nFurther Reading\n\n\n\n\n07/11/23\nPreview\nIntroduction to CNN\nTA Thái\n231107\n\n\n\n14/11/23\nPreview\nAdvanced CNN\n\n231114\n\n\n\n21/11/23\nPreview\nCNN and its variants\n\n231121\n\n\n\n28/11/23\nPreview\nIntroduction to Transfer Learning\n\n231128\n\n\n\n05/12/23\nPreview\nIntroduction to Transformer\n\n231205\nGenerative AI exists because of the transformer",
    "crumbs": [
      "Resources",
      "Course content",
      "Module 6"
    ]
  },
  {
    "objectID": "module6.html#extra-class-research-and-paper-writing",
    "href": "module6.html#extra-class-research-and-paper-writing",
    "title": "Module 6 - Introduction to Deep Learning",
    "section": "",
    "text": "Date\nType\nContent\nInstructor\nHandout\nFurther Reading\n\n\n\n\n11/11/23\nResearch and Paper\nResearch Idea - Brainstorming (1)\nDr. Đình Vinh\n231111\n\n\n\n18/11/23\nResearch and Paper\nResearch Idea - Brainstorming (2)\n\n231118\n\n\n\n25/11/23\nResearch and Paper\nHow to do Research (1)\n\n231125\n\n\n\n02/12/23\nResearch and Paper\nHow to do Research (2)\n\n231202\n\n\n\n09/12/23\nResearch and Paper\nHow to do Research (3)\n\n231209",
    "crumbs": [
      "Resources",
      "Course content",
      "Module 6"
    ]
  },
  {
    "objectID": "module6.html#seminar",
    "href": "module6.html#seminar",
    "title": "Module 6 - Introduction to Deep Learning",
    "section": "",
    "text": "Date\nType\nContent\nInstructor\nHandout\nFurther Reading\n\n\n\n\n10/12/23\nSeminar\nTransformers for Time series data\nDr. Vinh\n231210-1\n\n\n\n17/12/23\nSeminar\nScholarship and Feature Extraction in Time Series Data",
    "crumbs": [
      "Resources",
      "Course content",
      "Module 6"
    ]
  },
  {
    "objectID": "module7.html#main-lessons",
    "href": "module7.html#main-lessons",
    "title": "Module 7 - Advanced Model Architectures for Computer Vision",
    "section": "",
    "text": "Type\nContent\nInstructor\nHandout\nFurther Reading\n\n\n\n\nAssignment\nDomain Conversion Exercise\nTA Khoa\n\n\n\n\nMain Lesson\nDomain Conversion - Denoising and Segmentation\nDr. Vinh\n231220-23M07MC\n\n\n\nMain Lesson\nDomain Conversion - Colorization and Super Resolution\nDr. Vinh\n231222-23M07MC\n\n\n\nExercise Session\nTA Exercise\nTA Khoa\n231224-23M07MC\n\n\n\nAssignment\nObject Detection Project\nTA Hùng\n\n\n\n\nMain Lesson\nObject Detection (1)\nDr. Đình Vinh\n231227-23M07MC\n\n\n\nMain Lesson\nObject Detection (2)\nDr. Đình Vinh\n231229-23M07MC\n\n\n\nExercise Session\nTA Exercise\nTA Hùng\n231231-23M07MC\n\n\n\nMain Lesson\nObject Detection\nDr. Đình Vinh\n240103-23M07MC\n\n\n\nMain Lesson\nObject Detection\nDr. Đình Vinh\n240105-23M07MC\n\n\n\nExercise Session\nTA-Exercise\nTA Hùng\n240107-23M07MC\n\n\n\nAssignment\nImbalanced Data Exercise\n\n\n\n\n\nMain Lesson\nAdvanced Topic: Imbalanced Data\nDr. Vinh\n240110-23M07MC\n\n\n\nMain Lesson\nAdvanced Topic: Self/semi-supervised Learning\nDr. Hưng\n240112-23M07MC\n\n\n\nMain Lesson\nAdvanced Topic: Knowledge Distillation\nDr. Hưng\n240114-23M07MC\n\n\n\nProject tutorial\nImage Project: Tracking by Detection\nTA Thắng\n240117-23M07MC\n\n\n\nProject tutorial\nImage Project: Medical Image Analysis (1)\nTA Huy\n240119-23M07MC\n\n\n\nProject tutorial\nImage Project: Medical Image Analysis (2)\nTA Huy\n240121-23M07MC",
    "crumbs": [
      "Resources",
      "Course content",
      "Module 7"
    ]
  },
  {
    "objectID": "module7.html#lesspn-preview",
    "href": "module7.html#lesspn-preview",
    "title": "Module 7 - Advanced Model Architectures for Computer Vision",
    "section": "",
    "text": "Type\nContent (click for recording)\nInstructor\nHandout\nFurther Reading\n\n\n\n\nPreview\nUNet\nTA Thái\n231219-23M07-EC2\n\n\n\n\nObject Detection using Pretrained Models\n\n231226-23M07-EC2\n\n\n\n\nYolov1\n\n240102-23M07-EC2\n\n\n\n\nIntroduction to Imbalanced Data\n\n240109-23M07-EC2\n\n\n\nHow to write a research paper (cont.)\n\n\n\n\n\n\nResearch and Paper\nGroup Report (1)\nDr. Đình Vinh\nN/A\n\n\n\nResearch and Paper\nGroup Report (2)\n\nN/A\n\n\n\nResearch and Paper\nHow to Write a Paper (1)\n\n240106-23M07-EC1\n\n\n\nResearch and Paper\nHow to Write a Paper (2)\n\n240113-23M07-EC1\n\n\n\nSeminars\n\n\n\n\n\n\nSeminar\nAdvanced UNet and LLMs Introduction\nDr. Vinh\n231224-23M07-EC3\n\n\n\nSeminar\nSeminar: Multimodal Language Models\n\n231231-23M07-EC3\n\n\n\nSeminar\nSeminar: Visual Instruction Tuning (LlaVa) and QLoRA\n\n240107-23M07-EC3\n\n\n\nSeminar\nData Augmentation and Imbalanced Data\n\n240114-23M07-EC3\n\n\n\nSeminar\nRNN-based Forecasting and Toolformer\n\n240121-23M07-EC3",
    "crumbs": [
      "Resources",
      "Course content",
      "Module 7"
    ]
  },
  {
    "objectID": "module7.html#lesson-preview",
    "href": "module7.html#lesson-preview",
    "title": "Module 7 - Advanced Model Architectures for Computer Vision",
    "section": "",
    "text": "Type\nContent\nInstructor\nHandout\nFurther Reading\n\n\n\n\nPreview\nUNet\nTA Thái\n231219-23M07-EC2\n\n\n\nPreview\nObject Detection using Pretrained Models\nTA Thái\n231226-23M07-EC2\n\n\n\nPreview\nYolov1\nTA Thái\n240102-23M07-EC2\n\n\n\nPreview\nIntroduction to Imbalanced Data\nTA Thái\n240109-23M07-EC2",
    "crumbs": [
      "Resources",
      "Course content",
      "Module 7"
    ]
  },
  {
    "objectID": "module7.html#extra-class-research-and-paper-writing",
    "href": "module7.html#extra-class-research-and-paper-writing",
    "title": "Module 7 - Advanced Model Architectures for Computer Vision",
    "section": "",
    "text": "Type\nContent\nInstructor\nHandout\nFurther Reading\n\n\n\n\nResearch & Paper\nGroup Report (1)\nDr. Đình Vinh\nN/A\n\n\n\nResearch & Paper\nGroup Report (2)\nDr. Đình Vinh\nN/A\n\n\n\nResearch & Paper\nHow to Write a Paper (1)\nDr. Đình Vinh\n240106-23M07-EC1\n\n\n\nResearch & Paper\nHow to Write a Paper (2)\nDr. Đình Vinh\n240113-23M07-EC1",
    "crumbs": [
      "Resources",
      "Course content",
      "Module 7"
    ]
  },
  {
    "objectID": "module7.html#seminars",
    "href": "module7.html#seminars",
    "title": "Module 7 - Advanced Model Architectures for Computer Vision",
    "section": "",
    "text": "Type\nContent\nInstructor\nHandout\nFurther Reading\n\n\n\n\nSeminar\nAdvanced UNet and LLMs Introduction\nDr. Vinh\n231224-23M07-EC3\n\n\n\nSeminar\nSeminar: Multimodal Language Models\n\n231231-23M07-EC3\n\n\n\nSeminar\nSeminar: Visual Instruction Tuning (LlaVa) and QLoRA\n\n240107-23M07-EC3\n\n\n\nSeminar\nData Augmentation and Imbalanced Data\n\n240114-23M07-EC3\n\n\n\nSeminar\nRNN-based Forecasting and Toolformer\n\n240121-23M07-EC3",
    "crumbs": [
      "Resources",
      "Course content",
      "Module 7"
    ]
  },
  {
    "objectID": "module5.html#extra-class-introduction-to-computer-vision",
    "href": "module5.html#extra-class-introduction-to-computer-vision",
    "title": "Module 5 - Deep Learning Prerequisites",
    "section": "Extra class: Introduction to Computer Vision",
    "text": "Extra class: Introduction to Computer Vision\n\n\n\n\n\n\n\n\n\n\n\nDate\nType\nContent\nInstructor\nHandout\nFurther Reading\n\n\n\n\n07/10/23\nComputer Vision\nIntroduction to CV and Background subtraction\nDr. Đình Vinh\n231007\n\n\n\n14/10/23\nComputer Vision\nLane Detection\nDr. Đình Vinh\n231014\n\n\n\n21/10/23\nComputer Vision\nImage Stitching (panorama)\nDr. Đình Vinh\n231021\n\n\n\n28/10/23\nComputer Vision\nFace Detection\nDr. Đình Vinh\n231028\n\n\n\n04/11/23\nComputer Vision\nObject Tracking using Mean Shift/Cam Shift\nDr. Đình Vinh\n231104",
    "crumbs": [
      "Resources",
      "Course content",
      "Module 5"
    ]
  },
  {
    "objectID": "module6.html#extra-class-research-paper-writing",
    "href": "module6.html#extra-class-research-paper-writing",
    "title": "Module 6 - Introduction to Deep Learning",
    "section": "",
    "text": "Date\nType\nContent\nInstructor\nHandout\nFurther Reading\n\n\n\n\n11/11/23\nResearch & Paper\nResearch Idea - Brainstorming (1)\nDr. Đình Vinh\n231111\n\n\n\n18/11/23\nResearch & Paper\nResearch Idea - Brainstorming (2)\n\n231118\n\n\n\n25/11/23\nResearch & Paper\nHow to do Research (1)\n\n231125\n\n\n\n02/12/23\nResearch & Paper\nHow to do Research (2)\n\n231202\n\n\n\n09/12/23\nResearch & Paper\nHow to do Research (3)\n\n231209",
    "crumbs": [
      "Resources",
      "Course content",
      "Module 6"
    ]
  },
  {
    "objectID": "module6.html#competitiontraining",
    "href": "module6.html#competitiontraining",
    "title": "Module 6 - Introduction to Deep Learning",
    "section": "",
    "text": "Date\nType\nContent\nInstructor\nHandout\nFurther Reading\n\n\n\n\n09/11/23\nCompetition\nZalo AI\nTA Hùng\n231109\n\n\n\n16/11/23\nCompetition\nZalo AI\n\n231116",
    "crumbs": [
      "Resources",
      "Course content",
      "Module 6"
    ]
  },
  {
    "objectID": "module8.html#extra-class-mlops",
    "href": "module8.html#extra-class-mlops",
    "title": "Module 8 - Applications of Deep Learning for NLP",
    "section": "",
    "text": "Type\nContent\nInstructor\nHandout\nFurther Reading\n\n\n\n\nMLOps\nModel Quantization\nTA Bách\n240302-23M09-EC1",
    "crumbs": [
      "Resources",
      "Course content",
      "Module 8"
    ]
  },
  {
    "objectID": "module8.html#seminar",
    "href": "module8.html#seminar",
    "title": "Module 8 - Applications of Deep Learning for NLP",
    "section": "",
    "text": "Type\nContent\nInstructor\nHandout\nFurther Reading\n\n\n\n\nSeminar\nCLIP\nTA Đức\n240128-23M08-EC3\n\n\n\nSeminar\nDetecting violation of helmet rule for motorcyclists (CVPRW2024)\nDr. Vinh\n240204-23M08-EC3\n\n\n\nSeminar\nDirect Preference Optimization\nDr. Vinh\n240225-23M08-EC3\n\n\n\nSeminar\nStudy and Job in the USA",
    "crumbs": [
      "Resources",
      "Course content",
      "Module 8"
    ]
  },
  {
    "objectID": "module8.html#extra-class-introduction-to-large-language-models-llms",
    "href": "module8.html#extra-class-introduction-to-large-language-models-llms",
    "title": "Module 8 - Applications of Deep Learning for NLP",
    "section": "",
    "text": "Type\nContent\nInstructor\nHandout\nFurther Reading\n\n\n\n\nLLMs\nIntroduction to Text Generation\nDr. Vinh\n240220-23M08-EC2",
    "crumbs": [
      "Resources",
      "Course content",
      "Module 8"
    ]
  },
  {
    "objectID": "module9.html#extra-class-introduction-to-large-language-models-llms",
    "href": "module9.html#extra-class-introduction-to-large-language-models-llms",
    "title": "Module 9 - Generative Models",
    "section": "",
    "text": "Type\nContent\nInstructor\nHandout\nFurther Reading\n\n\n\n\nLLMs\nTraining a minChatGPT\nTA Thái\n240406-23M10-EC1\n\n\n\nLLMs\nLLM Finetuning for Math Solver\nTA Thắng\n240413-23M10-EC1",
    "crumbs": [
      "Resources",
      "Course content",
      "Module 9"
    ]
  },
  {
    "objectID": "module9.html#extra-class-mlops",
    "href": "module9.html#extra-class-mlops",
    "title": "Module 9 - Generative Models",
    "section": "",
    "text": "Type\nContent\nInstructor\nHandout\nFurther Reading\n\n\n\n\nMLOps\nModel Pruning\nTA Bách\n240309-23M09-EC1\n\n\n\nMLOps\nMobile Deployment\nDr. Đình Vinh\n240316-23M09-EC1\n\n\n\nMLOps\nWeb Deployment\nDr. Đình Vinh\n240323-23M09-EC1\n\n\n\nMLOps\nDeployment as a Service (API)\nTA Thắng\n240330-23M09-EC1",
    "crumbs": [
      "Resources",
      "Course content",
      "Module 9"
    ]
  },
  {
    "objectID": "module9.html#seminar",
    "href": "module9.html#seminar",
    "title": "Module 9 - Generative Models",
    "section": "",
    "text": "Type\nContent\nInstructor\nHandout\nFurther Reading\n\n\n\n\nSeminar\nSeminar: XAI\nDr. Anh Nguyen\n240405-23M09-MC",
    "crumbs": [
      "Resources",
      "Course content",
      "Module 9"
    ]
  },
  {
    "objectID": "module10.html#extra-class-introduction-to-large-language-models-llms",
    "href": "module10.html#extra-class-introduction-to-large-language-models-llms",
    "title": "Module 10 - Reinforcement Learning, GNN, and LLMs",
    "section": "",
    "text": "Type\nContent\nInstructor\nHandout\nFurther Reading\n\n\n\n\nExtra class\nLLM RAG for Applications\nTA Bách\n240421-23M10-MC\n\n\n\nExtra class\nLLMs for Multimodal Data\nTA Thái\n240427-23M10-EC1\n[Modaverse] [BLIP-2] [NExT-GPT]\n\n\nExtra class\nLLM Deployment with LangChain\nTA Thắng\n240504-23M10-EC1",
    "crumbs": [
      "Resources",
      "Course content",
      "Module 10"
    ]
  },
  {
    "objectID": "module9.html#main-lessons",
    "href": "module9.html#main-lessons",
    "title": "Module 9 - Generative Models",
    "section": "",
    "text": "Type\nContent\nInstructor\nHandout\nFurther Reading\n\n\n\n\nMain Lesson \n\n\n\n\n\n\nAssignment\nStyle Transfer Exercise\nTA Khoa\n\n\n\n\nMain Lesson\nBasic Style Transfer\nDr. Vinh\n240306-23M09-MC\n\n\n\nMain Lesson\nMultimodal Style Transfer\nDr. Vinh\n240308-23M09-MC\n\n\n\nExercise Session\nTA Exercise\nTA Khoa\n240310-23M09-MC\n\n\n\nExercise Session\nTA Exercise\nTA Thái\n240317-23M09-MC\n\n\n\nMain Lesson\nGAN and DCGAN\nDr. Vinh\n240320-23M09-MC\n\n\n\nMain Lesson\nPix2Pix and CycleGAN\nDr. Vinh\n240322-23M09-MC\n\n\n\nExercise Session\nText to Image Synthesis with DCGAN\nTA Thái\n240324-23M09-MC\n\n\n\nAssignment\nImage Inpainting with DDPMs\nTA Thái\n[solution]\n\n\n\nMain Lesson\nDiffusion Models (1)\nDr. Đình Vinh\n240327-23M09-MC\nTutorial on Diffusion Models for Imaging and Vision\n\n\nMain Lesson\nDiffusion Models (2)\nDr. Đình Vinh\n240329-23M09-MC\n\n\n\nExercise Session\nDiffusion-based Image Inpainting\nTA Thái\n240331-23M09-MC\n\n\n\nMain Lesson\nProject: VAE-based Image Colorization\nDr. Tài\n240403-23M09-MC\n\n\n\nProject tutorial - PT\nProject: Diffusion-based Image Colorization\nDr. Tài\n240407-23M09-MC\n\n\n\nAssignment\nText to Image Synthesis with Stable Diffusion (and CLIP)\nTA Thái\n\n\n\n\nMain Lesson\nStable Diffusion\nDr. Đình Vinh\n240410-23M09-MC\n\n\n\nMain Lesson\nIntroduction to OpenAI’s Sora\nDr. Đình Vinh\n240412-23M09-MC\n\n\n\nProject tutorial - PT\nText to Image Synthesis with Stable Diffusion (and CLIP)\nTA Thái\n240414-23M09-MC",
    "crumbs": [
      "Resources",
      "Course content",
      "Module 9"
    ]
  },
  {
    "objectID": "module10.html#main-lessons",
    "href": "module10.html#main-lessons",
    "title": "Module 10 - Reinforcement Learning, GNN, and LLMs",
    "section": "",
    "text": "Type\nContent\nInstructor\nHandout\nFurther Reading\n\n\n\n\nMain Lesson\nAssignment: Reinforcement Learning Exercise\nTA Thắng\n\n\n\n\nPreview\nIntroduction to Reinforcement Learning\nTA Thuận\n240416-23M10-EC2\n\n\n\nMain Lesson\nReinforcement Learning (CartPole)\nDr. Hoàng\n240417-23M10-MC\n\n\n\nMain Lesson\nReinforcement Learning (Deep Deterministic Policy Gradient)\nDr. Hoàng\n240419-23M10-MC\n\n\n\nExercise\nTA Exercise\nTA Thắng\n240421-23M10-MC\n\n\n\nAssignment\nPoint Cloud Techniques and Applications Project\nDr. Tuấn\n\n\n\n\nAssignment\nMultimodal Large Language Models Exercise\nTA Thái\n\n\n\n\nMain Lesson\nClassification for 3D Point Cloud Data\nDr. Tuấn\n240424-23M10-MC\n\n\n\nMain Lesson\nAdvances in 3D Point Cloud Data\nDr. Tuấn\n240426-23M10-MC\n\n\n\nMain Lesson\nGNN Node Classification\nDr. Đình Vinh\n240428-23M10-MC\n\n\n\nMain Lesson\nGNN (Molecular Property Prediction)\nDr. Đình Vinh\n240501-23M10-MC\n\n\n\nExercise\nTA Exercise\nTA Đức\n240503-23M10-MC\n\n\n\nProject PT\nMultitasking networks for Vision\nTA Thái\n240505-23M10-MC\n[Exercise]\n\n\nProject PT\nVideoCLIP for Video Classification\nTA Đức\n240508-23M10-MC\n\n\n\nProject PT\nMulti-agent LLM\nTA Thắng\n240510-23M10-MC",
    "crumbs": [
      "Resources",
      "Course content",
      "Module 10"
    ]
  },
  {
    "objectID": "module8.html#main-lessons",
    "href": "module8.html#main-lessons",
    "title": "Module 8 - Applications of Deep Learning for NLP",
    "section": "",
    "text": "Type\nContent\nInstructor\nHandout\nFurther Reading\n\n\n\n\nMain lesson\n\n\n\n\n\n\nAssignment\nPOS and Medical NER Exercise\nTA Thái\n\n\n\n\nPreview\nIntroduction to POS Tagging\nTA Khoa\n240130-23M08-EC2\n\n\n\nMain Lesson\nFrom Text Classification to POS Tagging\nDr. Vinh\n240131-23M08MC\n\n\n\nMain Lesson\nNER for Medical Data\nDr. Vinh\n240202-23M08MC\n\n\n\nExercise Session - ES\nTA-Exercise\nTA Thái\n240204-23M08MC\n\n\n\nProject handout\nAspect-based Sentiment Analysis Project\nTA Thái\n240206-23M08MC\n\n\n\nProject tutorial\nText Project: Aspect Extraction and Content Classification (Text Classification + NER)\nTA Thái\n240206-23M08MC\n\n\n\nProject tutorial\nText Project: QA for Content Inquiry (Text Classification + NER)\nTA Thắng\n240216-23M08MC\n\n\n\nProject tutorial\nText Project: End-to-end Question Answering (Building a Searching System)\nTA Thắng\n240218-23M08MC\n\n\n\nMain Lesson\nText Generation\nDr. Vinh\n240221-23M08MC\n\n\n\nMain Lesson\nMachine Translation\nDr. Vinh\n240223-23M08MC\n\n\n\nExercise Session - ES\nTA Exercise (Neural Machine Translation)\nTA Thái\n240225-23M08MC\n\n\n\nProject handout\nNeural Machine Translation\nTA Thái\n[handout]\n\n\n\nProject tutorial\nPoem Generation Project\nTA Thắng\n240228-23M08MC\n\n\n\nProject tutorial\nLow-resource Machine Translation Project\nTA Thái\n240301-23M08MC\n\n\n\nProject tutorial\nText classification with Mamba Project\nTA Đức\n\n\n\n\nProject handout\nText Project: Poem Generation\nTA Thái\n240228-23M08MC\n\n\n\nProject handout\nText Project: Low-resource Machine Translation\nTA Thắng\n240301-23M08MC",
    "crumbs": [
      "Resources",
      "Course content",
      "Module 8"
    ]
  },
  {
    "objectID": "module8.html#extra-class-research-and-paper-writing",
    "href": "module8.html#extra-class-research-and-paper-writing",
    "title": "Module 8 - Applications of Deep Learning for NLP",
    "section": "",
    "text": "Type\nContent\nInstructor\nHandout\nFurther Reading\n\n\n\n\nResearch and Paper\nHow to write a paper (3)\nDr. Đình Vinh\n240127-23M08-EC1\n\n\n\nResearch and Paper\nHow to write a paper (4)\nDr. Đình Vinh\n240203-23M08-EC1",
    "crumbs": [
      "Resources",
      "Course content",
      "Module 8"
    ]
  },
  {
    "objectID": "seminars.html",
    "href": "seminars.html",
    "title": "Seminars",
    "section": "",
    "text": "Content\nInstructor\nHandout\nFurther Reading\n\n\n\n\nOnline Office Hour\nDr. Vinh\n\n\n\n\nTransformers for Time series data\nDr. Vinh\n231210-1\n\n\n\nScholarship and Feature Extraction in Time Series Data\n\n\n\n\n\nAdvanced UNet and LLMs Introduction\nDr. Vinh\n231224-23M07-EC3\n\n\n\nSeminar: Multimodal Language Models\n\n231231-23M07-EC3\n\n\n\nSeminar: Visual Instruction Tuning (LlaVa) and QLoRA\n\n240107-23M07-EC3\n\n\n\nData Augmentation and Imbalanced Data\n\n240114-23M07-EC3\n\n\n\nRNN-based Forecasting and Toolformer\n\n240121-23M07-EC3\n\n\n\nCLIP\nTA Đức\n240128-23M08-EC3\n\n\n\nDetecting violation of helmet rule for motorcyclists (CVPRW2024)\nDr. Vinh\n240204-23M08-EC3\n\n\n\nDirect Preference Optimization\nDr. Vinh\n240225-23M08-EC3\n\n\n\nStudy and Job in the USA\n\n\n\n\n\nSeminar: XAI\nDr. Anh Nguyen\n240405-23M09-MC\n\n\n\n\n\n\n\n Back to top"
  },
  {
    "objectID": "ml-glossary/entropy.html",
    "href": "ml-glossary/entropy.html",
    "title": "Entropy",
    "section": "",
    "text": "Entropy is the term that has its root from the field of information theory and it is extremely useful in machine learning. The entropy helps us quantify the average amount of surprise of a probability distribution. We first consider the concept of surprise.\n\nIntuition\nSurprise in probability\nImagine three of the following the cases:\n\nyou ask your friend to predict whether a coin will land head or tail when being tossed and your friend’s prediction is correct.\nyou ask your friend to predict what value would appear when you roll a dice and your friend’s prediction is correct.\nyou ask your friend to predict what value would appear when you roll a dice three times in a row, and your friend’s prediction is, again, correct for all the rolls.\n\nWe all know now that with those three cases, the extent of surprise differs, from scenario 1 having the least amount of surprise to scenario 3 being the most surprised situation. The question for us now is how to quantify this amount of surprise in a mathematical sense with the following properties:\n\nIt has an additive nature, i.e. the surprise value for correctly predicting the value of 3 dices in a row (as in scenario 3) should be larger than that of correctly predicting the value of 1 dice. In particular, when the probability value multiply, the amount of surprise should add up.\nIt have to be negatively proportional to the probability value \\(p\\). In other words, your amount of surprise should equals to 0 when the probability of some event happening is 1 (absolutely certain).\n\n\n\nEntropy definition\nLet \\(h(s)\\) denotes the surprisal of state \\(s\\), and \\(p_s\\) be the probability that the state \\(s\\) happen. The second desideratum is easy to satisfied, where \\(h(s) \\propto \\dfrac{1}{p_s}\\). For the first desideratum, we know from high school math that the \\(\\log()\\) have such property: \\[\\log \\left(\\prod_{i=1}^N s_i\\right) = \\sum_{i=1}^{N} \\log (s_i)\\] With that, we can now formally defined the surprise term:\n\n[!definition] Surprise \\[h(s)  = \\dfrac{1}{\\log (s)}\\]\n\nNow, we want to compute the average of the surprise term for the whole distribution, this is called entropy: \\[H = \\sum_{s} p_{s} \\log \\left(\\dfrac{1}{p_{s}}\\right)\\] In plain English, what we are doing with this formula is to sum over all possible states and multiply the probability of each state by the surprisal value.\n\n\nRelated concepts:\n[[Cross-entropy]] [[KL Divergence]]\n\n\n\n\n Back to top"
  },
  {
    "objectID": "ml-glossary/ml-glossary.html",
    "href": "ml-glossary/ml-glossary.html",
    "title": "ML Glossary",
    "section": "",
    "text": "Back to top"
  },
  {
    "objectID": "ml-glossary.html",
    "href": "ml-glossary.html",
    "title": "ML Glossary",
    "section": "",
    "text": "Back to top"
  },
  {
    "objectID": "modules/module3.html",
    "href": "modules/module3.html",
    "title": "Module 3 - Probability and Statistics for AI",
    "section": "",
    "text": "There’s no question that learning probability and statistics is a crucial step for anyone who wants to become an AI expert.",
    "crumbs": [
      "Resources",
      "Course content",
      "Module 3"
    ]
  },
  {
    "objectID": "modules/module3.html#main-lesson",
    "href": "modules/module3.html#main-lesson",
    "title": "Module 3 - Probability and Statistics for AI",
    "section": "Main lesson",
    "text": "Main lesson\n\n\n\n\n\n\n\n\n\n\n\n Date\n Type\n Content\n Instructor\n Handout\n Further Reading\n\n\n\n\n18/07/23\nAssignment\nDue: 23/07/2023 Probability Exercise\nDr. Đình Vinh\n[solution]\n\n\n\n19/07/23\nMain Lesson\nBasic Probability, Histogram and Image Enhancement\nDr. Vinh\n230719\n\n\n\n21/07/23\nMain Lesson\nNaive Bayes Classifier\nDr. Vinh\n230721\n\n\n\n23/07/23\nExercise Session\nTA-Exercise\nDr. Đình Vinh\n230723\n\n\n\n31/07/23\nAssignment\nStatistic Exercise\nDr. Đình Vinh\nSolution\n\n\n\n26/07/23\nMain Lesson\nBasic Statistics and Correlation Coefficient (Basic Tracking)\nDr. Vinh\n230726\n\n\n\n28/07/23\nMain Lesson\nTemplate Matching (Cosine Similarity vs. Correlation Coefficient)\nDr. Đình Vinh\n230728\n\n\n\n30/07/23\nExercise Session\nTA-Exercise\nDr. Đình Vinh\n230730\n\n\n\n31/07/23\nAssignment\nGenetic Algorithms and its Applications Exercise\nDr. Đình Vinh\n\n\n\n\n02/08/23\nMain Lesson\nRandomness and Genetic Algorithms\nDr. Vinh\n230802\n\n\n\n04/08/23\nMain Lesson\nGenetic Algorithms (Optimization and Linear Regression)\nDr. Vinh\n230804\n\n\n\n06/08/23\nExercise Session\nTA-Exercise\nDr. Đình Vinh\n230806\n\n\n\n07/08/23\nAssignment\nGenetic Algorithms and its Applications Exercise\nDr. Đình Vinh\n[solution]\n\n\n\n07/08/23\nExercise Session\nData Analysis Exercise\nTA Thắng\n[solution]\n\n\n\n09/08/23\nMain Lesson\nData visualization and Data Analysis (1)\nDr. Vinh\n230809\n[note]\n\n\n11/08/23\nMain Lesson\nData visualization and Data Analysis (2)\nDr. Vinh\n230811\n\n\n\n13/08/23\nExercise Session\nTA-Exercise (Polar)\nTA Thắng\n230813\n[Polar doc]",
    "crumbs": [
      "Resources",
      "Course content",
      "Module 3"
    ]
  },
  {
    "objectID": "modules/module3.html#project-tutorial",
    "href": "modules/module3.html#project-tutorial",
    "title": "Module 3 - Probability and Statistics for AI",
    "section": "Project Tutorial",
    "text": "Project Tutorial\n\n\n\n\n\n\n\n\n\n\n\n Date\n Type\n Content\n Instructor\n Handout\n Further Reading\n\n\n\n\n17/08/23\nAssignment\nBig Data Frameworks (1)\nDr. Đình Vinh\n\n\n\n\n17/08/23\nAssignment\nBig Data Frameworks (1)\nDr. Đình Vinh\n\n\n\n\n17/08/23\nProject tutorial - PT\nBig Data Frameworks (1)\nDr. Đình Vinh\n230818\n[LSFS and MapReduce]\n\n\n20/08/23\n\nBig Data Frameworks (2)\nDr. Đình Vinh\n230820\n\n\n\n22/08/23\n\nImage Data Project: Image Retrieval\nTA Thắng\n\n\n\n\n23/08/23\n\nBig Data Frameworks (3)\nDr. Đình Vinh\n230823\n[C9, Feng 2021]\n\n\n25/08/23\n\nImage Data Project: Image Retrieval\nTA Thắng\n230825\n[VIT paper (Dosovitskiy et al., 2021)]",
    "crumbs": [
      "Resources",
      "Course content",
      "Module 3"
    ]
  },
  {
    "objectID": "modules/module3.html#extra-class-introduction-to-nlp",
    "href": "modules/module3.html#extra-class-introduction-to-nlp",
    "title": "Module 3 - Probability and Statistics for AI",
    "section": "Extra class: Introduction to NLP",
    "text": "Extra class: Introduction to NLP\n\n\n\n\n\n\n\n\n\n\n\n Date\n Type\n Content\n Instructor\n Handout\n Further Reading\n\n\n\n\n29/07/23\nNLP Introduction\nIntroduction, Preprocessing\nTA Thái\n230729\n\n\n\n05/08/23\nNLP Introduction\nPreprocessing, Tokenization\nTA Thái\n230805\n[note] [C2, Jurafsky et al., 2023]\n\n\n12/08/23\nNLP Introduction\nStatistical Language Model\nTA Thái\n230812\n[C3, Jurafsky et al., 2023] [Andrej Karpathy’s Lecture]\n\n\n18/08/23\nNLP Introduction\nPart-of-Speech Tagging\nTA Thái\n230819\n[CA Jurafsky et al., 2023]\n\n\n26/08/23\nNLP Introduction\nConstituency Parsing\nTA Thái\n230826",
    "crumbs": [
      "Resources",
      "Course content",
      "Module 3"
    ]
  },
  {
    "objectID": "modules/module3.html#python-tutorial-session",
    "href": "modules/module3.html#python-tutorial-session",
    "title": "Module 3 - Probability and Statistics for AI",
    "section": "Python Tutorial Session",
    "text": "Python Tutorial Session\n\n\n\n\n\n\n\n\n\n\n\n Date\n Type\n Content\n Instructor\n Handout\n Further Reading\n\n\n\n\n18/07/23\nPython Support\nProbability\nTA Tiềm\n230718\n\n\n\n25/07/23\nPython Support\nStatistics\nTA Bảo\n230725\n\n\n\n01/08/23\nPython Support\nGenetic Algorithms\nTA Tiềm\n230801\n\n\n\n08/08/23\nPython Support\nPandas (1)\nTA Bảo\n230808\n\n\n\n14/08/23\nPython Support\nPandas (2)\nTA Bảo\n230815\n[C7, McKinney 2023]",
    "crumbs": [
      "Resources",
      "Course content",
      "Module 3"
    ]
  },
  {
    "objectID": "modules/module3.html#competition-training",
    "href": "modules/module3.html#competition-training",
    "title": "Module 3 - Probability and Statistics for AI",
    "section": "Competition Training",
    "text": "Competition Training\n\n\n\n\n\n\n\n\n\n\n\n Date\n Type\n Content\n Instructor\n Handout\n Further Reading\n\n\n\n\n20/07/23\nCompetition\nTricks to improve performance\nTA Hùng\n230720\n\n\n\n27/07/23\nCompetition\nHCM AI Challenge\nTA Hùng\n230727\n\n\n\n03/08/23\nCompetition\nWeb API + Docker\nTA Hùng\n230803\n\n\n\n17/08/23\nCompetition\nOnnx\nTA Hùng\n230817\n\n\n\n24/08/23\nCompetition\nSemi-supervised Learning\nTA Hùng\n230824\n[Lil’Log] [GitHub repo] [Laine et al., 2016] [Tarvainen et al., 2017]",
    "crumbs": [
      "Resources",
      "Course content",
      "Module 3"
    ]
  },
  {
    "objectID": "modules/module1.html",
    "href": "modules/module1.html",
    "title": "Module 1 - Introduction to Python Programming",
    "section": "",
    "text": "This website is still under construction. Some contents and functions might temporarily not be available or not working properly.",
    "crumbs": [
      "Resources",
      "Course content",
      "Module 1"
    ]
  },
  {
    "objectID": "modules/module1.html#projects",
    "href": "modules/module1.html#projects",
    "title": "Module 1 - Introduction to Python Programming",
    "section": "Projects",
    "text": "Projects\nRegarding the projects, there are three projects where you will respectively learn how to use YOLOv8, an object detection model as well as how to use Python to manipulate and crawl data from a website. Finally, the last project is about developing simple applications using ChatGPT. In particular, the three projects are:\n\nObject Detection with YOLOv8\nData Manipulation and Crawling\nChatGPT Applications",
    "crumbs": [
      "Resources",
      "Course content",
      "Module 1"
    ]
  },
  {
    "objectID": "modules/module1.html#competition-training",
    "href": "modules/module1.html#competition-training",
    "title": "Module 1 - Introduction to Python Programming",
    "section": "Competition Training",
    "text": "Competition Training\nAs for competition training, this module contains three lectures with the goal of teaching you the basic skills and knowledge you need before joining an AI competition, including visualizing data, knowledge about competition tasks and metrics, and design validation.",
    "crumbs": [
      "Resources",
      "Course content",
      "Module 1"
    ]
  },
  {
    "objectID": "modules/module1.html#extra-class",
    "href": "modules/module1.html#extra-class",
    "title": "Module 1 - Introduction to Python Programming",
    "section": "Extra class",
    "text": "Extra class\nThe central theme of the extra class for this module is about Algorithms and Complexity. In the age of AI, still, the knowledge about algorithms and their complexity including Big-O, Brute-force exhaustive, recursion, two pointer, and dynamic programming still plays an immensely important role.",
    "crumbs": [
      "Resources",
      "Course content",
      "Module 1"
    ]
  },
  {
    "objectID": "modules/module1.html#main-lessons",
    "href": "modules/module1.html#main-lessons",
    "title": "Module 1 - Introduction to Python Programming",
    "section": "Main Lessons",
    "text": "Main Lessons\n\n\n\n\n\n\n\n\n\n\n\n Date\n Type\n Content\n Instructor\n Handout\n Further Reading\n\n\n\n\n08/05/23\n\nOnline Office Hour\nDr. Vinh\n\n\n\n\n28/04/23\nMain Lesson\nSinh hoạt lớp\nDr. Phúc\nslide\n\n\n\n01/05/23\nAssignment\nBasic Python Exercise\nTA Khoa\n[solution]\n\n\n\n03/05/23\nMain Lesson\nBasic Python 1\nDr. Vinh\n230503 - M01ML01\nNote\n\n\n05/05/23\nMain Lesson\nBasic Python 2\nDr. Vinh\n230505 - M01ML02\nNote\n\n\n07/05/23\nExercise Session\nTA-Exercise\nTA Khoa\n230507 - M01ES01\n\n\n\n08/05/23\nAssignment\nData Structure Exercise\nDr. Vinh\n[solution]\n\n\n\n10/05/23\nMain Lesson\nData Structure\nDr. Đình Vinh\n230510 - M01ML03\n\n\n\n12/05/23\nMain Lesson\nData Structure\nDr. Đình Vinh\n230512 - M01ML04\n\n\n\n14/05/23\nExercise Session\nTA-Exercise\nDr. Vinh\n230514 - M01ES02\n\n\n\n14/05/23\nAssignment\nPython OOP Exercise\nDr. Đình Vinh\n[solution]\n\n\n\n17/05/23\nMain Lesson\nOOP with Python\nDr. Đình Vinh\n230517 - M01ML05\nPrevious offering by Dr. Vinh\n\n\n19/05/23\nMain Lesson\nOOP with Python\nDr. Đình Vinh\n230519 - M01ML06\n\n\n\n21/05/23\nExercise Session\nTA-Exercise\nDr. Vinh\n230521 - M01ES03",
    "crumbs": [
      "Resources",
      "Course content",
      "Module 1"
    ]
  },
  {
    "objectID": "modules/module1.html#project-handout-and-material",
    "href": "modules/module1.html#project-handout-and-material",
    "title": "Module 1 - Introduction to Python Programming",
    "section": "Project handout and material",
    "text": "Project handout and material\n\n\n\n\n\n\n\n\n\n\n\n Date\n Type\n Content\n Instructor\n Handout\n Further Reading\n\n\n\n\n22/05/23\nProject handout\nObject Detection with YOLOv8 Project\nTA Thắng\n[solution]\n\n\n\n22/05/23\nProject handout\nData Manipulation and Crawling Project\nTA Thắng\n[solution]\n\n\n\n22/05/23\nProject handout\nChatGPT Applications Project\nTA Thái\n[solution]\n\n\n\n24/05/23\nProject Tutorial\nImage Project: Yolov8 for Object Detection\nTA Thắng\n230524 - M01PT01\n\n\n\n26/05/23\nProject Tutorial\nData Manipulation using Python Libraries\nTA Thắng\n230526 - M01PT02\n\n\n\n28/05/23\nProject Tutorial\nText Project: ChatGPT-based Application\nTA Thái\n230528 - M01PT03\n\n\n\n23/05/23\nPython Support\nFor, List and Dictionary\nTA Tiềm\n230523 - M02CR01",
    "crumbs": [
      "Resources",
      "Course content",
      "Module 1"
    ]
  },
  {
    "objectID": "modules/module1.html#competition-training-1",
    "href": "modules/module1.html#competition-training-1",
    "title": "Module 1 - Introduction to Python Programming",
    "section": "Competition training",
    "text": "Competition training\n\n\n\n\n\n\n\n\n\n\n\n Date\n Type\n Content\n Instructor\n Handout\n Further Reading\n\n\n\n\n11/05/23\nCompetition\nData Visualization\nTA Hùng\n230511 - M01AC01\n\n\n\n18/05/23\nCompetition\nCompetition tasks and metrics\nTA Hùng\n230518 - M01AC02\n\n\n\n25/05/23\nCompetition\nDesign Validation\nTA Hùng\n230525 - M01AC03",
    "crumbs": [
      "Resources",
      "Course content",
      "Module 1"
    ]
  },
  {
    "objectID": "modules/module1.html#extra-class-algorithms-and-complexity",
    "href": "modules/module1.html#extra-class-algorithms-and-complexity",
    "title": "Module 1 - Introduction to Python Programming",
    "section": "Extra class: Algorithms and Complexity",
    "text": "Extra class: Algorithms and Complexity\n\n\n\n\n\n\n\n\n\n\n\n Date\n Type\n Content\n Instructor\n Handout\n Further Reading\n\n\n\n\n06/05/23\nAlgorithms & Complexity\nBig-O (Time analysis)\nTA Thái\n230506 - M01EC01\n\n\n\n13/05/23\nAlgorithms & Complexity\nBrute-force Exhaustive\nTA Thái\n230513 - M01EC02\n\n\n\n20/05/23\nAlgorithms & Complexity\nRecursion and Two Pointer\nTA Thái\n230520 - M01EC03\n\n\n\n27/05/23\nAlgorithms & Complexity\nDynamic Programming\nTA Thái\n230527 - M01EC04",
    "crumbs": [
      "Resources",
      "Course content",
      "Module 1"
    ]
  },
  {
    "objectID": "modules/module4.html",
    "href": "modules/module4.html",
    "title": "Module 4 - Machine Learning and Data Science",
    "section": "",
    "text": "Hi, welcome to Module 4: Machine Learning and Data Science of the AIO2023 course. The main lessons of this module involves around traditional machine learning algorithms, ranging from simple to complex one such as kNN, Decision tree, Random Forest, XGBoost, and Support Vector Machines (SVMs). There are two projects, including one where we’d work with tabular type of data for the problem of heart disease protection. The second project is on Object detection.\nAs of the extra class for this module, it aims is to provide you with the knowledge involving around Large Language Models (LLMs), from the most fundamental knowledge to more recent techniques such as Prompting and Fine-tuning.",
    "crumbs": [
      "Resources",
      "Course content",
      "Module 4"
    ]
  },
  {
    "objectID": "modules/module4.html#main-lessons",
    "href": "modules/module4.html#main-lessons",
    "title": "Module 4 - Machine Learning and Data Science",
    "section": "Main Lessons",
    "text": "Main Lessons\n\n\n\n\n\n\n\n\n\n\n\nDate\nType\nContent\nInstructor\nHandout\nFurther Reading\n\n\n\n\n\n\n\n\n\n\n\n\n28/08/23\nAssignment\nK-Nearest Neighbor and Decision Tree Exercise\nTA Thái\n[solution]\n\n\n\n30/08/23\nMain Lesson\nK-Nearest Neighbors\nDr. Đình Vinh\n230830\n[note] [CS4780 Lecture 3]\n\n\n01/09/23\nMain Lesson\nDecision Tree for Classification\nDr. Đình Vinh\n230901\n\n\n\n03/09/23\nExercise Session\nTA Exercise\nTA Thái\n230903\n\n\n\n06/09/23\nMain Lesson\nDecision Tree for Regression\nDr. Đình Vinh\n230906\n[Choi et al., 2017]\n\n\n08/09/23\nMain Lesson\nRandom Forest\nDr. Đình Vinh\n230908\n[Louppe et al., 2015]\n\n\n10/09/23\nExercise Session\nTA Exercise\nTA Thắng\n230910\n\n\n\n11/09/23\nAssignment\nXGBoost\nTA Khoa\n[solution]\n\n\n\n13/09/23\nMain Lesson\nBasic XGBoost: Understanding Gradient Boost and AdaBoost\nDr. Đình Vinh\n230913\n[note] [Trevor Lecture]\n\n\n15/09/23\nMain Lesson\nAdvanced XGBoost: Fully XGBoost and its mathematics\nDr. Đình Vinh\n230915\n\n\n\n17/09/23\nExercise Session\nTA Exercise\nTA Khoa\n230917\n\n\n\n18/09/23\nAssignment\nSupport Vector Machine Exercise\nTA Thắng\n[solution]\n\n\n\n20/09/23\nMain Lesson\nReview on Tree-based Approaches and Discussion\nDr. Đình Vinh\n230920\n\n\n\n22/09/23\nMain Lesson\nSupport Vector Machine (1)\nDr. Đình Vinh\n230922\n[CS229 Note] [MLCB]\n\n\n24/09/23\nMain Lesson\nSupport Vector Machine (2)\nDr. Đình Vinh\n230924\n\n\n\n27/09/23\nExercise Session\nTA-Exercise\nTA Thắng\n230927\n\n\n\n29/09/23\nProject Tutorial\nTabular Data Project: Heart Disease Prediction\nDr. Đình Vinh\n230929\n\n\n\n01/10/23\nProject Tutorial\nImage Project: Object Detection\nTA Thắng\n231001\n[HoG paper]",
    "crumbs": [
      "Resources",
      "Course content",
      "Module 4"
    ]
  },
  {
    "objectID": "modules/module4.html#python-support",
    "href": "modules/module4.html#python-support",
    "title": "Module 4 - Machine Learning and Data Science",
    "section": "Python Support",
    "text": "Python Support\n\n\n\n\n\n\n\n\n\n\n\nDate\nType\nContent\nInstructor\nHandout\nFurther Reading\n\n\n\n\n\n\n\n\n\n\n\n\n29/08/23\nPython Support\nK-Nearest Neighbors (Warm-up)\nDr. Vinh\n230829\n[STAT451 Note]\n\n\n05/09/23\nPython Support\nDecision Tree\nDr. Vinh\n230905\n\n\n\n12/09/23\nPython Support\nRandom Forest and AdaBoost (Warm-up)\nDr. Vinh\n230912\n\n\n\n21/09/23\nPython Support\nSVM\nTA Thắng\n230921\n[CS4780 Note]",
    "crumbs": [
      "Resources",
      "Course content",
      "Module 4"
    ]
  },
  {
    "objectID": "modules/module4.html#extra-class-advanced-nlp",
    "href": "modules/module4.html#extra-class-advanced-nlp",
    "title": "Module 4 - Machine Learning and Data Science",
    "section": "Extra class: Advanced NLP",
    "text": "Extra class: Advanced NLP\n\n\n\n\n\n\n\n\n\n\n\nDate\nType\nContent\nInstructor\nHandout\nFurther Reading\n\n\n\n\n\n\n\n\n\n\n\n\n09/09/23\nAdvanced NLP\nFoundations for an NLP topic\nTA Thái\n230909\n[note] [CS324 note]\n\n\n16/09/23\nAdvanced NLP\nLanguage Models and Prompting Techniques\n\n230916\n\n\n\n23/09/23\nAdvanced NLP\nParameter-Efficient Fine-tuning\n\n230923\n[Houlsby et al., 2019] [COS597G Slides]\n\n\n30/09/23\nAdvanced NLP\nEfficient Fine-tuning of quantized LLMs\n\n230930\n[LoRA paper]",
    "crumbs": [
      "Resources",
      "Course content",
      "Module 4"
    ]
  },
  {
    "objectID": "modules/module7.html",
    "href": "modules/module7.html",
    "title": "Module 7 - Advanced Model Architectures for Computer Vision",
    "section": "",
    "text": "Type\nContent\nInstructor\nHandout\nFurther Reading\n\n\n\n\nAssignment\nDomain Conversion Exercise\nTA Khoa\nDue: 24/12/2024 [solution]\n\n\n\nMain Lesson\nDomain Conversion - Denoising and Segmentation\nDr. Vinh\n231220-23M07MC\n\n\n\nMain Lesson\nDomain Conversion - Colorization and Super Resolution\nDr. Vinh\n231222-23M07MC\n\n\n\nExercise Session\nTA Exercise\nTA Khoa\n231224-23M07MC\n\n\n\nAssignment\nObject Detection Project\nTA Hùng\n\n\n\n\nMain Lesson\nObject Detection (1)\nDr. Đình Vinh\n231227-23M07MC\n\n\n\nMain Lesson\nObject Detection (2)\nDr. Đình Vinh\n231229-23M07MC\n\n\n\nExercise Session\nTA Exercise\nTA Hùng\n231231-23M07MC\n\n\n\nMain Lesson\nObject Detection\nDr. Đình Vinh\n240103-23M07MC\n\n\n\nMain Lesson\nObject Detection\nDr. Đình Vinh\n240105-23M07MC\n\n\n\nExercise Session\nTA-Exercise\nTA Hùng\n240107-23M07MC\n\n\n\nAssignment\nImbalanced Data Exercise\nDr. Vinh\n[solution]\n\n\n\nMain Lesson\nAdvanced Topic: Imbalanced Data\nDr. Vinh\n240110-23M07MC\n\n\n\nMain Lesson\nAdvanced Topic: Self/semi-supervised Learning\nDr. Hưng\n240112-23M07MC\n\n\n\nMain Lesson\nAdvanced Topic: Knowledge Distillation\nDr. Hưng\n240114-23M07MC\n\n\n\nProject tutorial\nImage Project: Tracking by Detection\nTA Thắng\n240117-23M07MC [Handout] [Solution]\n\n\n\nProject tutorial\nImage Project: Medical Image Analysis (1)\nTA Huy\n240119-23M07MC [Handout] [Solution]\n\n\n\nProject tutorial\nImage Project: Medical Image Analysis (2)\nTA Huy\n240121-23M07MC\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nType\nContent\nInstructor\nHandout\nFurther Reading\n\n\n\n\nPreview\nUNet\nTA Thái\n231219-23M07-EC2\n\n\n\nPreview\nObject Detection using Pretrained Models\nTA Thái\n231226-23M07-EC2\n\n\n\nPreview\nYolov1\nTA Thái\n240102-23M07-EC2\n\n\n\nPreview\nIntroduction to Imbalanced Data\nTA Thái\n240109-23M07-EC2\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nType\nContent\nInstructor\nHandout\nFurther Reading\n\n\n\n\nResearch & Paper\nGroup Report (1)\nDr. Đình Vinh\nN/A\n\n\n\nResearch & Paper\nGroup Report (2)\nDr. Đình Vinh\nN/A\n\n\n\nResearch & Paper\nHow to Write a Paper (1)\nDr. Đình Vinh\n240106-23M07-EC1\n\n\n\nResearch & Paper\nHow to Write a Paper (2)\nDr. Đình Vinh\n240113-23M07-EC1\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nType\nContent\nInstructor\nHandout\nFurther Reading\n\n\n\n\nSeminar\nAdvanced UNet and LLMs Introduction\nDr. Vinh\n231224-23M07-EC3\n\n\n\nSeminar\nSeminar: Multimodal Language Models\n\n231231-23M07-EC3\n\n\n\nSeminar\nSeminar: Visual Instruction Tuning (LlaVa) and QLoRA\n\n240107-23M07-EC3\n\n\n\nSeminar\nData Augmentation and Imbalanced Data\n\n240114-23M07-EC3\n\n\n\nSeminar\nRNN-based Forecasting and Toolformer\n\n240121-23M07-EC3",
    "crumbs": [
      "Misc",
      "Course content",
      "Module 7"
    ]
  },
  {
    "objectID": "modules/module7.html#main-lessons",
    "href": "modules/module7.html#main-lessons",
    "title": "Module 7 - Advanced Model Architectures for Computer Vision",
    "section": "",
    "text": "Type\nContent\nInstructor\nHandout\nFurther Reading\n\n\n\n\nAssignment\nDomain Conversion Exercise\nTA Khoa\nDue: 24/12/2024 [solution]\n\n\n\nMain Lesson\nDomain Conversion - Denoising and Segmentation\nDr. Vinh\n231220-23M07MC\n\n\n\nMain Lesson\nDomain Conversion - Colorization and Super Resolution\nDr. Vinh\n231222-23M07MC\n\n\n\nExercise Session\nTA Exercise\nTA Khoa\n231224-23M07MC\n\n\n\nAssignment\nObject Detection Project\nTA Hùng\n\n\n\n\nMain Lesson\nObject Detection (1)\nDr. Đình Vinh\n231227-23M07MC\n\n\n\nMain Lesson\nObject Detection (2)\nDr. Đình Vinh\n231229-23M07MC\n\n\n\nExercise Session\nTA Exercise\nTA Hùng\n231231-23M07MC\n\n\n\nMain Lesson\nObject Detection\nDr. Đình Vinh\n240103-23M07MC\n\n\n\nMain Lesson\nObject Detection\nDr. Đình Vinh\n240105-23M07MC\n\n\n\nExercise Session\nTA-Exercise\nTA Hùng\n240107-23M07MC\n\n\n\nAssignment\nImbalanced Data Exercise\nDr. Vinh\n[solution]\n\n\n\nMain Lesson\nAdvanced Topic: Imbalanced Data\nDr. Vinh\n240110-23M07MC\n\n\n\nMain Lesson\nAdvanced Topic: Self/semi-supervised Learning\nDr. Hưng\n240112-23M07MC\n\n\n\nMain Lesson\nAdvanced Topic: Knowledge Distillation\nDr. Hưng\n240114-23M07MC\n\n\n\nProject tutorial\nImage Project: Tracking by Detection\nTA Thắng\n240117-23M07MC [Handout] [Solution]\n\n\n\nProject tutorial\nImage Project: Medical Image Analysis (1)\nTA Huy\n240119-23M07MC [Handout] [Solution]\n\n\n\nProject tutorial\nImage Project: Medical Image Analysis (2)\nTA Huy\n240121-23M07MC",
    "crumbs": [
      "Misc",
      "Course content",
      "Module 7"
    ]
  },
  {
    "objectID": "modules/module7.html#lesson-preview",
    "href": "modules/module7.html#lesson-preview",
    "title": "Module 7 - Advanced Model Architectures for Computer Vision",
    "section": "",
    "text": "Type\nContent\nInstructor\nHandout\nFurther Reading\n\n\n\n\nPreview\nUNet\nTA Thái\n231219-23M07-EC2\n\n\n\nPreview\nObject Detection using Pretrained Models\nTA Thái\n231226-23M07-EC2\n\n\n\nPreview\nYolov1\nTA Thái\n240102-23M07-EC2\n\n\n\nPreview\nIntroduction to Imbalanced Data\nTA Thái\n240109-23M07-EC2",
    "crumbs": [
      "Misc",
      "Course content",
      "Module 7"
    ]
  },
  {
    "objectID": "modules/module7.html#extra-class-research-and-paper-writing",
    "href": "modules/module7.html#extra-class-research-and-paper-writing",
    "title": "Module 7 - Advanced Model Architectures for Computer Vision",
    "section": "",
    "text": "Type\nContent\nInstructor\nHandout\nFurther Reading\n\n\n\n\nResearch & Paper\nGroup Report (1)\nDr. Đình Vinh\nN/A\n\n\n\nResearch & Paper\nGroup Report (2)\nDr. Đình Vinh\nN/A\n\n\n\nResearch & Paper\nHow to Write a Paper (1)\nDr. Đình Vinh\n240106-23M07-EC1\n\n\n\nResearch & Paper\nHow to Write a Paper (2)\nDr. Đình Vinh\n240113-23M07-EC1",
    "crumbs": [
      "Misc",
      "Course content",
      "Module 7"
    ]
  },
  {
    "objectID": "modules/module7.html#seminars",
    "href": "modules/module7.html#seminars",
    "title": "Module 7 - Advanced Model Architectures for Computer Vision",
    "section": "",
    "text": "Type\nContent\nInstructor\nHandout\nFurther Reading\n\n\n\n\nSeminar\nAdvanced UNet and LLMs Introduction\nDr. Vinh\n231224-23M07-EC3\n\n\n\nSeminar\nSeminar: Multimodal Language Models\n\n231231-23M07-EC3\n\n\n\nSeminar\nSeminar: Visual Instruction Tuning (LlaVa) and QLoRA\n\n240107-23M07-EC3\n\n\n\nSeminar\nData Augmentation and Imbalanced Data\n\n240114-23M07-EC3\n\n\n\nSeminar\nRNN-based Forecasting and Toolformer\n\n240121-23M07-EC3",
    "crumbs": [
      "Misc",
      "Course content",
      "Module 7"
    ]
  },
  {
    "objectID": "modules/module9.html",
    "href": "modules/module9.html",
    "title": "Module 9 - Generative Models",
    "section": "",
    "text": "Type\nContent\nInstructor\nHandout\nFurther Reading\n\n\n\n\nMain Lesson \n\n\n\n\n\n\nAssignment\nStyle Transfer Exercise\nTA Khoa\n\n\n\n\nMain Lesson\nBasic Style Transfer\nDr. Vinh\n240306-23M09-MC\n\n\n\nMain Lesson\nMultimodal Style Transfer\nDr. Vinh\n240308-23M09-MC\n\n\n\nExercise Session\nTA Exercise\nTA Khoa\n240310-23M09-MC\n\n\n\nExercise Session\nTA Exercise\nTA Thái\n240317-23M09-MC\n\n\n\nMain Lesson\nGAN and DCGAN\nDr. Vinh\n240320-23M09-MC\n\n\n\nMain Lesson\nPix2Pix and CycleGAN\nDr. Vinh\n240322-23M09-MC\n\n\n\nExercise Session\nText to Image Synthesis with DCGAN\nTA Thái\n240324-23M09-MC\n\n\n\nAssignment\nImage Inpainting with DDPMs\nTA Thái\n[solution]\n\n\n\nMain Lesson\nDiffusion Models (1)\nDr. Đình Vinh\n240327-23M09-MC\nTutorial on Diffusion Models for Imaging and Vision\n\n\nMain Lesson\nDiffusion Models (2)\nDr. Đình Vinh\n240329-23M09-MC\n\n\n\nExercise Session\nDiffusion-based Image Inpainting\nTA Thái\n240331-23M09-MC\n\n\n\nMain Lesson\nProject: VAE-based Image Colorization\nDr. Tài\n240403-23M09-MC\n\n\n\nProject tutorial - PT\nProject: Diffusion-based Image Colorization\nDr. Tài\n240407-23M09-MC\n\n\n\nAssignment\nText to Image Synthesis with Stable Diffusion (and CLIP)\nTA Thái\n\n\n\n\nMain Lesson\nStable Diffusion\nDr. Đình Vinh\n240410-23M09-MC\n\n\n\nMain Lesson\nIntroduction to OpenAI’s Sora\nDr. Đình Vinh\n240412-23M09-MC\n\n\n\nProject tutorial - PT\nText to Image Synthesis with Stable Diffusion (and CLIP)\nTA Thái\n240414-23M09-MC\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nType\nContent\nInstructor\nHandout\nFurther Reading\n\n\n\n\nLLMs\nTraining a minChatGPT\nTA Thái\n240406-23M10-EC1\n\n\n\nLLMs\nLLM Finetuning for Math Solver\nTA Thắng\n240413-23M10-EC1\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nType\nContent\nInstructor\nHandout\nFurther Reading\n\n\n\n\nMLOps\nModel Pruning\nTA Bách\n240309-23M09-EC1\n\n\n\nMLOps\nMobile Deployment\nDr. Đình Vinh\n240316-23M09-EC1\n\n\n\nMLOps\nWeb Deployment\nDr. Đình Vinh\n240323-23M09-EC1\n\n\n\nMLOps\nDeployment as a Service (API)\nTA Thắng\n240330-23M09-EC1\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nType\nContent\nInstructor\nHandout\nFurther Reading\n\n\n\n\nSeminar\nSeminar: XAI\nDr. Anh Nguyen\n240405-23M09-MC",
    "crumbs": [
      "Misc",
      "Course content",
      "Module 9"
    ]
  },
  {
    "objectID": "modules/module9.html#main-lessons",
    "href": "modules/module9.html#main-lessons",
    "title": "Module 9 - Generative Models",
    "section": "",
    "text": "Type\nContent\nInstructor\nHandout\nFurther Reading\n\n\n\n\nMain Lesson \n\n\n\n\n\n\nAssignment\nStyle Transfer Exercise\nTA Khoa\n\n\n\n\nMain Lesson\nBasic Style Transfer\nDr. Vinh\n240306-23M09-MC\n\n\n\nMain Lesson\nMultimodal Style Transfer\nDr. Vinh\n240308-23M09-MC\n\n\n\nExercise Session\nTA Exercise\nTA Khoa\n240310-23M09-MC\n\n\n\nExercise Session\nTA Exercise\nTA Thái\n240317-23M09-MC\n\n\n\nMain Lesson\nGAN and DCGAN\nDr. Vinh\n240320-23M09-MC\n\n\n\nMain Lesson\nPix2Pix and CycleGAN\nDr. Vinh\n240322-23M09-MC\n\n\n\nExercise Session\nText to Image Synthesis with DCGAN\nTA Thái\n240324-23M09-MC\n\n\n\nAssignment\nImage Inpainting with DDPMs\nTA Thái\n[solution]\n\n\n\nMain Lesson\nDiffusion Models (1)\nDr. Đình Vinh\n240327-23M09-MC\nTutorial on Diffusion Models for Imaging and Vision\n\n\nMain Lesson\nDiffusion Models (2)\nDr. Đình Vinh\n240329-23M09-MC\n\n\n\nExercise Session\nDiffusion-based Image Inpainting\nTA Thái\n240331-23M09-MC\n\n\n\nMain Lesson\nProject: VAE-based Image Colorization\nDr. Tài\n240403-23M09-MC\n\n\n\nProject tutorial - PT\nProject: Diffusion-based Image Colorization\nDr. Tài\n240407-23M09-MC\n\n\n\nAssignment\nText to Image Synthesis with Stable Diffusion (and CLIP)\nTA Thái\n\n\n\n\nMain Lesson\nStable Diffusion\nDr. Đình Vinh\n240410-23M09-MC\n\n\n\nMain Lesson\nIntroduction to OpenAI’s Sora\nDr. Đình Vinh\n240412-23M09-MC\n\n\n\nProject tutorial - PT\nText to Image Synthesis with Stable Diffusion (and CLIP)\nTA Thái\n240414-23M09-MC",
    "crumbs": [
      "Misc",
      "Course content",
      "Module 9"
    ]
  },
  {
    "objectID": "modules/module9.html#extra-class-introduction-to-large-language-models-llms",
    "href": "modules/module9.html#extra-class-introduction-to-large-language-models-llms",
    "title": "Module 9 - Generative Models",
    "section": "",
    "text": "Type\nContent\nInstructor\nHandout\nFurther Reading\n\n\n\n\nLLMs\nTraining a minChatGPT\nTA Thái\n240406-23M10-EC1\n\n\n\nLLMs\nLLM Finetuning for Math Solver\nTA Thắng\n240413-23M10-EC1",
    "crumbs": [
      "Misc",
      "Course content",
      "Module 9"
    ]
  },
  {
    "objectID": "modules/module9.html#extra-class-mlops",
    "href": "modules/module9.html#extra-class-mlops",
    "title": "Module 9 - Generative Models",
    "section": "",
    "text": "Type\nContent\nInstructor\nHandout\nFurther Reading\n\n\n\n\nMLOps\nModel Pruning\nTA Bách\n240309-23M09-EC1\n\n\n\nMLOps\nMobile Deployment\nDr. Đình Vinh\n240316-23M09-EC1\n\n\n\nMLOps\nWeb Deployment\nDr. Đình Vinh\n240323-23M09-EC1\n\n\n\nMLOps\nDeployment as a Service (API)\nTA Thắng\n240330-23M09-EC1",
    "crumbs": [
      "Misc",
      "Course content",
      "Module 9"
    ]
  },
  {
    "objectID": "modules/module9.html#seminar",
    "href": "modules/module9.html#seminar",
    "title": "Module 9 - Generative Models",
    "section": "",
    "text": "Type\nContent\nInstructor\nHandout\nFurther Reading\n\n\n\n\nSeminar\nSeminar: XAI\nDr. Anh Nguyen\n240405-23M09-MC",
    "crumbs": [
      "Misc",
      "Course content",
      "Module 9"
    ]
  },
  {
    "objectID": "modules/module8.html",
    "href": "modules/module8.html",
    "title": "Module 8 - Applications of Deep Learning for NLP",
    "section": "",
    "text": "Type\nContent\nInstructor\nHandout\nFurther Reading\n\n\n\n\nMain lesson\n\n\n\n\n\n\nAssignment\nPOS and Medical NER Exercise\nTA Thái\n[solution]\n\n\n\nPreview\nIntroduction to POS Tagging\nTA Khoa\n240130-23M08-EC2\n\n\n\nMain Lesson\nFrom Text Classification to POS Tagging\nDr. Vinh\n240131-23M08MC\n\n\n\nMain Lesson\nNER for Medical Data\nDr. Vinh\n240202-23M08MC\n\n\n\nExercise Session - ES\nTA-Exercise\nTA Thái\n240204-23M08MC\n\n\n\nProject handout\nAspect-based Sentiment Analysis Project\nTA Thái\n240206-23M08MC\n\n\n\nProject tutorial\nText Project: Aspect Extraction and Content Classification (Text Classification + NER)\nTA Thái\n240206-23M08MC\n\n\n\nProject tutorial\nText Project: QA for Content Inquiry (Text Classification + NER)\nTA Thắng\n240216-23M08MC\n\n\n\nProject tutorial\nText Project: End-to-end Question Answering (Building a Searching System)\nTA Thắng\n240218-23M08MC [Handout] [Solution]\n\n\n\nMain Lesson\nText Generation\nDr. Vinh\n240221-23M08MC\n\n\n\nMain Lesson\nMachine Translation\nDr. Vinh\n240223-23M08MC\n\n\n\nExercise Session - ES\nTA Exercise (Neural Machine Translation)\nTA Thái\n240225-23M08MC\n\n\n\nProject handout\nNeural Machine Translation\nTA Thái\n[handout]\n\n\n\nProject tutorial\nPoem Generation Project\nTA Thắng\n240228-23M08MC\n\n\n\nProject tutorial\nLow-resource Machine Translation Project\nTA Thái\n240301-23M08MC\n\n\n\nProject tutorial\nText classification with Mamba Project\nTA Đức\n\n\n\n\nProject handout\nText Project: Poem Generation\nTA Thái\n240228-23M08MC\n\n\n\nProject handout\nText Project: Low-resource Machine Translation\nTA Thắng\n240301-23M08MC\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nType\nContent\nInstructor\nHandout\nFurther Reading\n\n\n\n\nResearch and Paper\nHow to write a paper (3)\nDr. Đình Vinh\n240127-23M08-EC1\n\n\n\nResearch and Paper\nHow to write a paper (4)\nDr. Đình Vinh\n240203-23M08-EC1\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nType\nContent\nInstructor\nHandout\nFurther Reading\n\n\n\n\nMLOps\nModel Quantization\nTA Bách\n240302-23M09-EC1\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nType\nContent\nInstructor\nHandout\nFurther Reading\n\n\n\n\nSeminar\nCLIP\nTA Đức\n240128-23M08-EC3\n\n\n\nSeminar\nDetecting violation of helmet rule for motorcyclists (CVPRW2024)\nDr. Vinh\n240204-23M08-EC3\n\n\n\nSeminar\nDirect Preference Optimization\nDr. Vinh\n240225-23M08-EC3\n\n\n\nSeminar\nStudy and Job in the USA\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nType\nContent\nInstructor\nHandout\nFurther Reading\n\n\n\n\nLLMs\nIntroduction to Text Generation\nDr. Vinh\n240220-23M08-EC2",
    "crumbs": [
      "Misc",
      "Course content",
      "Module 8"
    ]
  },
  {
    "objectID": "modules/module8.html#main-lessons",
    "href": "modules/module8.html#main-lessons",
    "title": "Module 8 - Applications of Deep Learning for NLP",
    "section": "",
    "text": "Type\nContent\nInstructor\nHandout\nFurther Reading\n\n\n\n\nMain lesson\n\n\n\n\n\n\nAssignment\nPOS and Medical NER Exercise\nTA Thái\n[solution]\n\n\n\nPreview\nIntroduction to POS Tagging\nTA Khoa\n240130-23M08-EC2\n\n\n\nMain Lesson\nFrom Text Classification to POS Tagging\nDr. Vinh\n240131-23M08MC\n\n\n\nMain Lesson\nNER for Medical Data\nDr. Vinh\n240202-23M08MC\n\n\n\nExercise Session - ES\nTA-Exercise\nTA Thái\n240204-23M08MC\n\n\n\nProject handout\nAspect-based Sentiment Analysis Project\nTA Thái\n240206-23M08MC\n\n\n\nProject tutorial\nText Project: Aspect Extraction and Content Classification (Text Classification + NER)\nTA Thái\n240206-23M08MC\n\n\n\nProject tutorial\nText Project: QA for Content Inquiry (Text Classification + NER)\nTA Thắng\n240216-23M08MC\n\n\n\nProject tutorial\nText Project: End-to-end Question Answering (Building a Searching System)\nTA Thắng\n240218-23M08MC [Handout] [Solution]\n\n\n\nMain Lesson\nText Generation\nDr. Vinh\n240221-23M08MC\n\n\n\nMain Lesson\nMachine Translation\nDr. Vinh\n240223-23M08MC\n\n\n\nExercise Session - ES\nTA Exercise (Neural Machine Translation)\nTA Thái\n240225-23M08MC\n\n\n\nProject handout\nNeural Machine Translation\nTA Thái\n[handout]\n\n\n\nProject tutorial\nPoem Generation Project\nTA Thắng\n240228-23M08MC\n\n\n\nProject tutorial\nLow-resource Machine Translation Project\nTA Thái\n240301-23M08MC\n\n\n\nProject tutorial\nText classification with Mamba Project\nTA Đức\n\n\n\n\nProject handout\nText Project: Poem Generation\nTA Thái\n240228-23M08MC\n\n\n\nProject handout\nText Project: Low-resource Machine Translation\nTA Thắng\n240301-23M08MC",
    "crumbs": [
      "Misc",
      "Course content",
      "Module 8"
    ]
  },
  {
    "objectID": "modules/module8.html#extra-class-research-and-paper-writing",
    "href": "modules/module8.html#extra-class-research-and-paper-writing",
    "title": "Module 8 - Applications of Deep Learning for NLP",
    "section": "",
    "text": "Type\nContent\nInstructor\nHandout\nFurther Reading\n\n\n\n\nResearch and Paper\nHow to write a paper (3)\nDr. Đình Vinh\n240127-23M08-EC1\n\n\n\nResearch and Paper\nHow to write a paper (4)\nDr. Đình Vinh\n240203-23M08-EC1",
    "crumbs": [
      "Misc",
      "Course content",
      "Module 8"
    ]
  },
  {
    "objectID": "modules/module8.html#extra-class-mlops",
    "href": "modules/module8.html#extra-class-mlops",
    "title": "Module 8 - Applications of Deep Learning for NLP",
    "section": "",
    "text": "Type\nContent\nInstructor\nHandout\nFurther Reading\n\n\n\n\nMLOps\nModel Quantization\nTA Bách\n240302-23M09-EC1",
    "crumbs": [
      "Misc",
      "Course content",
      "Module 8"
    ]
  },
  {
    "objectID": "modules/module8.html#seminar",
    "href": "modules/module8.html#seminar",
    "title": "Module 8 - Applications of Deep Learning for NLP",
    "section": "",
    "text": "Type\nContent\nInstructor\nHandout\nFurther Reading\n\n\n\n\nSeminar\nCLIP\nTA Đức\n240128-23M08-EC3\n\n\n\nSeminar\nDetecting violation of helmet rule for motorcyclists (CVPRW2024)\nDr. Vinh\n240204-23M08-EC3\n\n\n\nSeminar\nDirect Preference Optimization\nDr. Vinh\n240225-23M08-EC3\n\n\n\nSeminar\nStudy and Job in the USA",
    "crumbs": [
      "Misc",
      "Course content",
      "Module 8"
    ]
  },
  {
    "objectID": "modules/module8.html#extra-class-introduction-to-large-language-models-llms",
    "href": "modules/module8.html#extra-class-introduction-to-large-language-models-llms",
    "title": "Module 8 - Applications of Deep Learning for NLP",
    "section": "",
    "text": "Type\nContent\nInstructor\nHandout\nFurther Reading\n\n\n\n\nLLMs\nIntroduction to Text Generation\nDr. Vinh\n240220-23M08-EC2",
    "crumbs": [
      "Misc",
      "Course content",
      "Module 8"
    ]
  },
  {
    "objectID": "modules/module10.html",
    "href": "modules/module10.html",
    "title": "Module 10 - Reinforcement Learning, GNN, and LLMs",
    "section": "",
    "text": "Type\nContent\nInstructor\nHandout\nFurther Reading\n\n\n\n\nMain Lesson\nAssignment: Reinforcement Learning Exercise\nTA Thắng\n\n\n\n\nPreview\nIntroduction to Reinforcement Learning\nTA Thuận\n240416-23M10-EC2\n\n\n\nMain Lesson\nReinforcement Learning (CartPole)\nDr. Hoàng\n240417-23M10-MC\n\n\n\nMain Lesson\nReinforcement Learning (Deep Deterministic Policy Gradient)\nDr. Hoàng\n240419-23M10-MC\n\n\n\nExercise\nTA Exercise\nTA Thắng\n240421-23M10-MC\n\n\n\nAssignment\nPoint Cloud Techniques and Applications Project\nDr. Tuấn\n\n\n\n\nAssignment\nMultimodal Large Language Models Exercise\nTA Thái\n\n\n\n\nMain Lesson\nClassification for 3D Point Cloud Data\nDr. Tuấn\n240424-23M10-MC\n\n\n\nMain Lesson\nAdvances in 3D Point Cloud Data\nDr. Tuấn\n240426-23M10-MC\n\n\n\nMain Lesson\nGNN Node Classification\nDr. Đình Vinh\n240428-23M10-MC\n\n\n\nMain Lesson\nGNN (Molecular Property Prediction)\nDr. Đình Vinh\n240501-23M10-MC\n\n\n\nExercise\nTA Exercise\nTA Đức\n240503-23M10-MC\n\n\n\nProject PT\nMultitasking networks for Vision\nTA Thái\n240505-23M10-MC\n[Exercise]\n\n\nProject PT\nVideoCLIP for Video Classification\nTA Đức\n240508-23M10-MC\n\n\n\nProject PT\nMulti-agent LLM\nTA Thắng\n240510-23M10-MC\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nType\nContent\nInstructor\nHandout\nFurther Reading\n\n\n\n\nExtra class\nLLM RAG for Applications\nTA Bách\n240421-23M10-MC\n\n\n\nExtra class\nLLMs for Multimodal Data\nTA Thái\n240427-23M10-EC1\n[Modaverse] [BLIP-2] [NExT-GPT]\n\n\nExtra class\nLLM Deployment with LangChain\nTA Thắng\n240504-23M10-EC1"
  },
  {
    "objectID": "modules/module10.html#main-lessons",
    "href": "modules/module10.html#main-lessons",
    "title": "Module 10 - Reinforcement Learning, GNN, and LLMs",
    "section": "",
    "text": "Type\nContent\nInstructor\nHandout\nFurther Reading\n\n\n\n\nMain Lesson\nAssignment: Reinforcement Learning Exercise\nTA Thắng\n\n\n\n\nPreview\nIntroduction to Reinforcement Learning\nTA Thuận\n240416-23M10-EC2\n\n\n\nMain Lesson\nReinforcement Learning (CartPole)\nDr. Hoàng\n240417-23M10-MC\n\n\n\nMain Lesson\nReinforcement Learning (Deep Deterministic Policy Gradient)\nDr. Hoàng\n240419-23M10-MC\n\n\n\nExercise\nTA Exercise\nTA Thắng\n240421-23M10-MC\n\n\n\nAssignment\nPoint Cloud Techniques and Applications Project\nDr. Tuấn\n\n\n\n\nAssignment\nMultimodal Large Language Models Exercise\nTA Thái\n\n\n\n\nMain Lesson\nClassification for 3D Point Cloud Data\nDr. Tuấn\n240424-23M10-MC\n\n\n\nMain Lesson\nAdvances in 3D Point Cloud Data\nDr. Tuấn\n240426-23M10-MC\n\n\n\nMain Lesson\nGNN Node Classification\nDr. Đình Vinh\n240428-23M10-MC\n\n\n\nMain Lesson\nGNN (Molecular Property Prediction)\nDr. Đình Vinh\n240501-23M10-MC\n\n\n\nExercise\nTA Exercise\nTA Đức\n240503-23M10-MC\n\n\n\nProject PT\nMultitasking networks for Vision\nTA Thái\n240505-23M10-MC\n[Exercise]\n\n\nProject PT\nVideoCLIP for Video Classification\nTA Đức\n240508-23M10-MC\n\n\n\nProject PT\nMulti-agent LLM\nTA Thắng\n240510-23M10-MC"
  },
  {
    "objectID": "modules/module10.html#extra-class-introduction-to-large-language-models-llms",
    "href": "modules/module10.html#extra-class-introduction-to-large-language-models-llms",
    "title": "Module 10 - Reinforcement Learning, GNN, and LLMs",
    "section": "",
    "text": "Type\nContent\nInstructor\nHandout\nFurther Reading\n\n\n\n\nExtra class\nLLM RAG for Applications\nTA Bách\n240421-23M10-MC\n\n\n\nExtra class\nLLMs for Multimodal Data\nTA Thái\n240427-23M10-EC1\n[Modaverse] [BLIP-2] [NExT-GPT]\n\n\nExtra class\nLLM Deployment with LangChain\nTA Thắng\n240504-23M10-EC1"
  },
  {
    "objectID": "modules/module6.html",
    "href": "modules/module6.html",
    "title": "Module 6 - Introduction to Deep Learning",
    "section": "",
    "text": "Date\nType\nContent\nInstructor\nHandout\nFurther Reading\n\n\n\n\n06/11/23\nAssignment\nConvolutional Neural Network Exercise\nTA Thái\nDue: 12/11/2023 [solution]\n\n\n\n08/11/23\nMain Lesson\nBasic CNN (1)\nDr. Vinh\n231108\n[CS231N Note] [CNN Explainer]\n\n\n10/11/23\nMain Lesson\nBasic CNN (2)\nDr. Vinh\n231110\n\n\n\n12/11/23\nExercise Session\nTA-Exercise\nTA Thắng\n231112\n\n\n\n13/11/23\nAssignment\nConvolutional Neural Network Exercise\nTA Thắng\n[solution]\n\n\n\n15/11/23\nMain Lesson\nCNN Training\nDr. Vinh\n231115\n[VGG Paper]\n\n\n17/11/23\nMain Lesson\nCNN Generalization\nDr. Vinh\n231117\n\n\n\n19/11/23\nExercise Session\nTA-Exercise\nTA Thắng\n231119\n\n\n\n20/11/23\nAssignment\nPretrained Models for Image Exercise\nTA Thắng\n[solution]\n\n\n\n22/11/23\nMain Lesson\nAdvanced CNN Architecture\nDr. Vinh\n231122\n\n\n\n24/11/23\nMain Lesson\nTransfer Learning for CNN\nDr. Vinh\n231124\n\n\n\n26/11/23\nExercise Session\nTA-Exercise\nTA Thái\n231126\n\n\n\n27/11/23\nAssignment\nRecurrent Neural Network Exercise\nTA Thắng\n[solution]\n\n\n\n29/11/23\nMain Lesson\nBasic RNN/LSTM (Time series and Text)\nDr. Vinh\n231129\n\n\n\n01/12/23\nMain Lesson\nBasic RNN/LSTM (Time series and Text)\nDr. Vinh\n231201\n\n\n\n03/12/23\nExercise Session\nTA-Exercise\nTA Thắng\n231203\n\n\n\n04/12/23\nAssignment\nTransformer Application Exercise\n\n\n\n\n\n06/12/23\nMain Lesson\nTransformer (Encoder - Text Classification)\nDr. Vinh\n231206\nBERT Readings\n\n\n10/12/23\nAssignment\nTransformer and its applications\nTA Thái\n[solution]\n\n\n\n08/12/23\nMain Lesson\nTransformer for Image and Time series Data\nDr. Vinh\n231208\n\n\n\n13/12/23\nProject Tutorial\nImage Project: OCR with YOLOv8 and CNN (Scene Text Recognition)\nTA Thắng\n231213 [Handout] [Solution]\n\n\n\n15/12/23\nProject Tutorial\nImage-text Project: VQA\nTA Thắng\n231215 [Handout] [Solution]\n\n\n\n17/12/23\nProject Tutorial\nTime-series forecasting project\nDr. Đình Vinh\n231217 [Handout] [Solution]\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nDate\nType\nContent\nInstructor\nHandout\nFurther Reading\n\n\n\n\n09/11/23\nCompetition\nZalo AI\nTA Hùng\n231109\n\n\n\n16/11/23\nCompetition\nZalo AI\n\n231116\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nDate\nType\nContent\nInstructor\nHandout\nFurther Reading\n\n\n\n\n07/11/23\nPreview\nIntroduction to CNN\nTA Thái\n231107\n\n\n\n14/11/23\nPreview\nAdvanced CNN\n\n231114\n\n\n\n21/11/23\nPreview\nCNN and its variants\n\n231121\n\n\n\n28/11/23\nPreview\nIntroduction to Transfer Learning\n\n231128\n\n\n\n05/12/23\nPreview\nIntroduction to Transformer\n\n231205\nGenerative AI exists because of the transformer\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nDate\nType\nContent\nInstructor\nHandout\nFurther Reading\n\n\n\n\n11/11/23\nResearch & Paper\nResearch Idea - Brainstorming (1)\nDr. Đình Vinh\n231111\n\n\n\n18/11/23\nResearch & Paper\nResearch Idea - Brainstorming (2)\n\n231118\n\n\n\n25/11/23\nResearch & Paper\nHow to do Research (1)\n\n231125\n\n\n\n02/12/23\nResearch & Paper\nHow to do Research (2)\n\n231202\n\n\n\n09/12/23\nResearch & Paper\nHow to do Research (3)\n\n231209\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nDate\nType\nContent\nInstructor\nHandout\nFurther Reading\n\n\n\n\n10/12/23\nSeminar\nTransformers for Time series data\nDr. Vinh\n23121v0-1\n\n\n\n17/12/23\nSeminar\nScholarship and Feature Extraction in Time Series Data\n231217",
    "crumbs": [
      "Misc",
      "Course content",
      "Module 6"
    ]
  },
  {
    "objectID": "modules/module6.html#main-lessons",
    "href": "modules/module6.html#main-lessons",
    "title": "Module 6 - Introduction to Deep Learning",
    "section": "",
    "text": "Date\nType\nContent\nInstructor\nHandout\nFurther Reading\n\n\n\n\n06/11/23\nAssignment\nConvolutional Neural Network Exercise\nTA Thái\nDue: 12/11/2023 [solution]\n\n\n\n08/11/23\nMain Lesson\nBasic CNN (1)\nDr. Vinh\n231108\n[CS231N Note] [CNN Explainer]\n\n\n10/11/23\nMain Lesson\nBasic CNN (2)\nDr. Vinh\n231110\n\n\n\n12/11/23\nExercise Session\nTA-Exercise\nTA Thắng\n231112\n\n\n\n13/11/23\nAssignment\nConvolutional Neural Network Exercise\nTA Thắng\n[solution]\n\n\n\n15/11/23\nMain Lesson\nCNN Training\nDr. Vinh\n231115\n[VGG Paper]\n\n\n17/11/23\nMain Lesson\nCNN Generalization\nDr. Vinh\n231117\n\n\n\n19/11/23\nExercise Session\nTA-Exercise\nTA Thắng\n231119\n\n\n\n20/11/23\nAssignment\nPretrained Models for Image Exercise\nTA Thắng\n[solution]\n\n\n\n22/11/23\nMain Lesson\nAdvanced CNN Architecture\nDr. Vinh\n231122\n\n\n\n24/11/23\nMain Lesson\nTransfer Learning for CNN\nDr. Vinh\n231124\n\n\n\n26/11/23\nExercise Session\nTA-Exercise\nTA Thái\n231126\n\n\n\n27/11/23\nAssignment\nRecurrent Neural Network Exercise\nTA Thắng\n[solution]\n\n\n\n29/11/23\nMain Lesson\nBasic RNN/LSTM (Time series and Text)\nDr. Vinh\n231129\n\n\n\n01/12/23\nMain Lesson\nBasic RNN/LSTM (Time series and Text)\nDr. Vinh\n231201\n\n\n\n03/12/23\nExercise Session\nTA-Exercise\nTA Thắng\n231203\n\n\n\n04/12/23\nAssignment\nTransformer Application Exercise\n\n\n\n\n\n06/12/23\nMain Lesson\nTransformer (Encoder - Text Classification)\nDr. Vinh\n231206\nBERT Readings\n\n\n10/12/23\nAssignment\nTransformer and its applications\nTA Thái\n[solution]\n\n\n\n08/12/23\nMain Lesson\nTransformer for Image and Time series Data\nDr. Vinh\n231208\n\n\n\n13/12/23\nProject Tutorial\nImage Project: OCR with YOLOv8 and CNN (Scene Text Recognition)\nTA Thắng\n231213 [Handout] [Solution]\n\n\n\n15/12/23\nProject Tutorial\nImage-text Project: VQA\nTA Thắng\n231215 [Handout] [Solution]\n\n\n\n17/12/23\nProject Tutorial\nTime-series forecasting project\nDr. Đình Vinh\n231217 [Handout] [Solution]",
    "crumbs": [
      "Misc",
      "Course content",
      "Module 6"
    ]
  },
  {
    "objectID": "modules/module6.html#competition-training",
    "href": "modules/module6.html#competition-training",
    "title": "Module 6 - Introduction to Deep Learning",
    "section": "",
    "text": "Date\nType\nContent\nInstructor\nHandout\nFurther Reading\n\n\n\n\n09/11/23\nCompetition\nZalo AI\nTA Hùng\n231109\n\n\n\n16/11/23\nCompetition\nZalo AI\n\n231116",
    "crumbs": [
      "Misc",
      "Course content",
      "Module 6"
    ]
  },
  {
    "objectID": "modules/module6.html#python-review",
    "href": "modules/module6.html#python-review",
    "title": "Module 6 - Introduction to Deep Learning",
    "section": "",
    "text": "Date\nType\nContent\nInstructor\nHandout\nFurther Reading\n\n\n\n\n07/11/23\nPreview\nIntroduction to CNN\nTA Thái\n231107\n\n\n\n14/11/23\nPreview\nAdvanced CNN\n\n231114\n\n\n\n21/11/23\nPreview\nCNN and its variants\n\n231121\n\n\n\n28/11/23\nPreview\nIntroduction to Transfer Learning\n\n231128\n\n\n\n05/12/23\nPreview\nIntroduction to Transformer\n\n231205\nGenerative AI exists because of the transformer",
    "crumbs": [
      "Misc",
      "Course content",
      "Module 6"
    ]
  },
  {
    "objectID": "modules/module6.html#extra-class-research-paper-writing",
    "href": "modules/module6.html#extra-class-research-paper-writing",
    "title": "Module 6 - Introduction to Deep Learning",
    "section": "",
    "text": "Date\nType\nContent\nInstructor\nHandout\nFurther Reading\n\n\n\n\n11/11/23\nResearch & Paper\nResearch Idea - Brainstorming (1)\nDr. Đình Vinh\n231111\n\n\n\n18/11/23\nResearch & Paper\nResearch Idea - Brainstorming (2)\n\n231118\n\n\n\n25/11/23\nResearch & Paper\nHow to do Research (1)\n\n231125\n\n\n\n02/12/23\nResearch & Paper\nHow to do Research (2)\n\n231202\n\n\n\n09/12/23\nResearch & Paper\nHow to do Research (3)\n\n231209",
    "crumbs": [
      "Misc",
      "Course content",
      "Module 6"
    ]
  },
  {
    "objectID": "modules/module6.html#seminar",
    "href": "modules/module6.html#seminar",
    "title": "Module 6 - Introduction to Deep Learning",
    "section": "",
    "text": "Date\nType\nContent\nInstructor\nHandout\nFurther Reading\n\n\n\n\n10/12/23\nSeminar\nTransformers for Time series data\nDr. Vinh\n23121v0-1\n\n\n\n17/12/23\nSeminar\nScholarship and Feature Extraction in Time Series Data\n231217",
    "crumbs": [
      "Misc",
      "Course content",
      "Module 6"
    ]
  },
  {
    "objectID": "modules/module5.html#main-lessons",
    "href": "modules/module5.html#main-lessons",
    "title": "Module 5 - Deep Learning Prerequisites",
    "section": "Main lessons",
    "text": "Main lessons\n\n\n\n\n\n\n\n\n\n\n\nDate\nType\nContent\nInstructor\nHandout\nFurther Reading\n\n\n\n\n02/10/23\nAssignment\nLogistic Regression Exercise\nTA Thắng\nDue: 08/10/2023 [solution]\n\n\n\n04/10/23\nMain Lesson\nFrom Linear Regression to Logistic Regression\nDr. Vinh\n231004\n\n\n\n06/10/23\nMain Lesson\nLogistic Regression - Vectorization and Applications\nDr. Vinh\n231006\n[Jurafsky et al., C5, 2023]\n\n\n08/10/23\nExercise Session\nTA Exercise\nTA Thắng\n231008\n\n\n\n09/10/23\nAssignment\nSoftmax Regression Exercise\nTA Thắng\n[solution]\n\n\n\n11/10/23\nMain Lesson\nSoftmax Regression (Multiclass Classification)\nDr. Vinh\n231011\n\n\n\n13/10/23\nMain Lesson\nPytorch Framework (Implementation for regression)\nDr. Vinh\n231013\n\n\n\n15/10/23\nExercise Session\nTA Exercise\nTA Khoa\n231015\n\n\n\n16/10/23\nAssignment\nMLP Exercise\nTA Thắng\nDue: 22/10/2023 [solution]\n\n\n\n18/10/23\nMain Lesson\nMultilayer Perceptron\nDr. Vinh\n231018\n\n\n\n20/10/23\nMain Lesson\nActivations and Initializers\nDr. Vinh\n231020\n\n\n\n22/10/23\nExercise Session\nTA Exercise\nTA Khoa\n231022\n\n\n\n25/10/23\nMain Lesson\nOptimizers for Neural Network (1)\nDr. Vinh\n231025\n\n\n\n27/10/23\nMain Lesson\nOptimizers for Neural Networks (2)\nDr. Vinh\n231027\n\n\n\n29/10/23\nExercise Session\nTA-Exercise (Optimization methods)\nTA Khoa\n231029\n\n\n\n01/11/23\nProject Tutorial\nText data: Sentiment Analysis\nTA Thái\n231101 [Proposal] [Solution]\n\n\n\n03/11/23\nProject Tutorial\nTime-series Data Project: Music Genre Classification\nTA Bảo\n231103 [Proposal] [Solution]\n\n\n\n05/11/23\nProject Tutorial\nImage Data Project: Gradient Vanishing in MLP\nTA Khoa\n231105 [Proposal] [Solution]",
    "crumbs": [
      "Resources",
      "Course content",
      "Module 5"
    ]
  },
  {
    "objectID": "modules/module5.html#python-tutorial-session",
    "href": "modules/module5.html#python-tutorial-session",
    "title": "Module 5 - Deep Learning Prerequisites",
    "section": "Python Tutorial Session",
    "text": "Python Tutorial Session\n\n\n\n\n\n\n\n\n\n\n\nDate\nType\nContent\nInstructor\nHandout\nFurther Reading\n\n\n\n\n03/10/23\nPreview\nLogistic Regression\nTA Thái\n231003\n\n\n\n10/10/23\nPreview\nSoftmax Regression\n\n231010\n\n\n\n17/10/23\nPreview\nMultilayer Perceptron\n\n231017\n\n\n\n24/10/23\nPreview\nSGD+Momentum\n\n231024",
    "crumbs": [
      "Resources",
      "Course content",
      "Module 5"
    ]
  },
  {
    "objectID": "modules/module5.html#extra-class-introduction-to-computer-vision",
    "href": "modules/module5.html#extra-class-introduction-to-computer-vision",
    "title": "Module 5 - Deep Learning Prerequisites",
    "section": "Extra class: Introduction to Computer Vision",
    "text": "Extra class: Introduction to Computer Vision\n\n\n\n\n\n\n\n\n\n\n\nDate\nType\nContent\nInstructor\nHandout\nFurther Reading\n\n\n\n\n07/10/23\nComputer Vision\nIntroduction to CV and Background subtraction\nDr. Đình Vinh\n231007\n\n\n\n14/10/23\nComputer Vision\nLane Detection\nDr. Đình Vinh\n231014\n\n\n\n21/10/23\nComputer Vision\nImage Stitching (panorama)\nDr. Đình Vinh\n231021\n\n\n\n28/10/23\nComputer Vision\nFace Detection\nDr. Đình Vinh\n231028\n\n\n\n04/11/23\nComputer Vision\nObject Tracking using Mean Shift/Cam Shift\nDr. Đình Vinh\n231104 [Proposal] [Solution]",
    "crumbs": [
      "Resources",
      "Course content",
      "Module 5"
    ]
  },
  {
    "objectID": "modules/module5.html#competition-training",
    "href": "modules/module5.html#competition-training",
    "title": "Module 5 - Deep Learning Prerequisites",
    "section": "Competition Training",
    "text": "Competition Training\n\n\n\n\n\n\n\n\n\n\n\nDate\nType\nContent\nInstructor\nHandout\nFurther Reading\n\n\n\n\n05/10/23\nCompetition\nIntroduction to Imbalance Data\nTA Hùng\n231005\n\n\n\n12/10/23\nCompetition\nModel Evaluation\nTA Hùng\n231012\n\n\n\n19/10/23\nCompetition\nKalapa Challenge\nTA Hùng\n231019\n\n\n\n02/11/23\nCompetition\nData Sampling\nTA Hùng\n231102",
    "crumbs": [
      "Resources",
      "Course content",
      "Module 5"
    ]
  },
  {
    "objectID": "modules/module2.html",
    "href": "modules/module2.html",
    "title": "Module 2 - Calculus and Linear Algebra for AI",
    "section": "",
    "text": "This website is still under construction. Some contents and functions might temporarily not be available or not working properly.",
    "crumbs": [
      "Resources",
      "Course content",
      "Module 2"
    ]
  },
  {
    "objectID": "modules/module2.html#main-lessons",
    "href": "modules/module2.html#main-lessons",
    "title": "Module 2 - Calculus and Linear Algebra for AI",
    "section": "Main Lessons",
    "text": "Main Lessons\n\n\n\n\n\n\n\n\n\n\n\n Date\n Type\n Content\n Instructor\n Handout\n Further Reading\n\n\n\n\n30/05/23\nAssignment\nInterpolation and Application Exercises\nDr. Đình Vinh\nDue: 04/06/2023 solution\n\n\n\n31/05/23\nMain Lesson\nConstruct loss functions with different targets\nDr. Vinh\n230531 - M02ML01\n\n\n\n02/06/23\nMain Lesson\nInterpolation and Image upsampling\n\n230602 - M02ML02\n\n\n\n04/06/23\nExercise Session\nTA Exercise\nDr. Đình Vinh\n230604 - M02ES01\n\n\n\n05/06/23\nAssignment\nCalculus Exercise\n\nDue: 11/06/2023 [solution]\n\n\n\n07/06/23\nMain Lesson\nUnderstanding Derivative and Gradient\nDr. Vinh\n230607 - M02ML03\n\n\n\n09/06/23\nMain Lesson\nEdge Detection and Gradient-based Optimization\n\n230609 - M02ML04\n\n\n\n11/06/23\nExercise Session\nTA Exercise\nDr. Đình Vinh\n230611 - M02ES02\n\n\n\n14/06/23\nMain Lesson\nLinear Regression (1)\nDr. Vinh\n230614 - M02ML05\n\n\n\n16/06/23\nMain Lesson\nLinear Regression (2)\n\n230616 - M02ML06\n\n\n\n18/06/23\nExercise Session\nTA Exercise\nDr. Đình Vinh\n230618 - M02ES03\n\n\n\n21/06/23\nMain Lesson\nVectorization for Linear Regression (1)\nDr. Vinh\n230621 - M02ML07\n\n\n\n23/06/23\nMain Lesson\nVectorization for Linear Regression (2)\n\n230623 - M02ML08\n\n\n\n25/06/23\nExercise Session\nTA Exercise\nDr. Đình Vinh\n230625 - M02ES0\n\n\n\n26/06/23\nAssignment\nCosine Background subtraction Exercise\n\nDue: 02/07/2023 [solution]\n\n\n\n28/06/23\nMain Lesson\nBasic Linear Algebra and Its Applications\nDr. Vinh\n230628 - M02ML09\n\n\n\n30/06/23\nMain Lesson\nCosine Similarity and Decomposition\n\n230630 - M02ML10\n\n\n\n02/07/23\nExercise Session\nTA Exercise\nDr. Đình Vinh\n230702 - M02ES05",
    "crumbs": [
      "Resources",
      "Course content",
      "Module 2"
    ]
  },
  {
    "objectID": "modules/module2.html#project-tutorial",
    "href": "modules/module2.html#project-tutorial",
    "title": "Module 2 - Calculus and Linear Algebra for AI",
    "section": "Project Tutorial",
    "text": "Project Tutorial\n\n\n\n\n\n\n\n\n\n\n\n Date\n Type\n Content\n Instructor\n Handout\n Further Reading\n\n\n\n\n09/07/23\nProject Tutorial\nSVD and its Applications\nDr. Đình Vinh\n230709 - M02ET01\n\n\n\n10/07/23\nProject handout\nImage Project: Depth Information Reconstruction\nTA Thắng\n[proposal]\n\n\n\n10/07/23\nProject handout\nText Project: Text Retrieval (using Pretrained Embedding)\nTA Thắng\n[solution]\n[solution]\n\n\n10/07/23\nProject handout\nTabular Data Project: Sales Prediction (Linear and non-linear regression)\nTA Thái\n[proposal]\n\n\n\n12/07/23\nProject Tutorial\nTabular Data Project: Sales Prediction (Linear and non-linear regression)\nTA Thái\n230716 - M02PT03\n\n\n\n14/07/23\nProject Tutorial\nText Project: Text Retrieval (using Pretrained Embedding)\nTA Thắng\n230714 - M02PT02\n\n\n\n16/07/23\nProject Tutorial\nImage Project: Depth Information Reconstruction\nTA Thắng\n230712 - M02PT01",
    "crumbs": [
      "Resources",
      "Course content",
      "Module 2"
    ]
  },
  {
    "objectID": "modules/module2.html#python-tutorial-sessions",
    "href": "modules/module2.html#python-tutorial-sessions",
    "title": "Module 2 - Calculus and Linear Algebra for AI",
    "section": "Python Tutorial Sessions",
    "text": "Python Tutorial Sessions\n\n\n\n\n\n\n\n\n\n\n\n Date\n Type\n Content\n Instructor\n Handout\n Further Reading\n\n\n\n\n30/05/23\nPython Support\nList for 2D Data\nTA Tiềm\n230530 - M02CR02\n\n\n\n06/06/23\nPython Support\nList for 3D data\n\n230606 - M02CR03\n\n\n\n13/06/23\nPython Support\nArray 1D using NumPy\n\n230613 - M02CR04\n\n\n\n20/06/23\nPython Support\nArray 2D and 3D using NumPy\n\n230620 - M02CR05\n\n\n\n27/06/23\nPython Support\nBasic Numpy\n\n230627 - M02CR06",
    "crumbs": [
      "Resources",
      "Course content",
      "Module 2"
    ]
  },
  {
    "objectID": "modules/module2.html#extra-classes-database-and-sql",
    "href": "modules/module2.html#extra-classes-database-and-sql",
    "title": "Module 2 - Calculus and Linear Algebra for AI",
    "section": "Extra Classes: Database and SQL",
    "text": "Extra Classes: Database and SQL\n\n\n\n\n\n\n\n\n\n\n\n Date\n Type\n Content\n Instructor\n Handout\n Further Reading\n\n\n\n\n03/06/23\nDatabase and SQL\nDatabase-SQL (1)\nTA Bảo\n230603 - M02EC01\n\n\n\n10/06/23\nDatabase and SQL\nDatabase-SQL (2)\nTA Bảo\n230610 - M02ES02\n\n\n\n17/06/23\nDatabase and SQL\nDatabase-SQL (3)\nTA Bảo\n230617 - M02EC03\n\n\n\n24/06/23\nNoSQL\nDatabase-NoSQL\nTA Thái\n230624 - M02EC04\n\n\n\n01/07/23\nNoSQL\nDatabase-NoSQL (2)\nTA Thái\n230701 - M02EC05\n\n\n\n08/07/23\nBig Data\nSQL for Big Data\nTA Thắng\n230708 - M02EC06",
    "crumbs": [
      "Resources",
      "Course content",
      "Module 2"
    ]
  },
  {
    "objectID": "modules/module2.html#competition",
    "href": "modules/module2.html#competition",
    "title": "Module 2 - Calculus and Linear Algebra for AI",
    "section": "Competition",
    "text": "Competition\n\n\n\n\n\n\n\n\n\n\n\n Date\n Type\n Content\n Instructor\n Handout\n Further Reading\n\n\n\n\n01/06/23\nCompetition\nHyper-parameter optimization\nTA Hùng\n230601 - M02AC01\n\n\n\n08/06/23\nCompetition\nEnsembling (Voting, Averaging, and Stacked Generalization)\nTA Hùng\n230608 - M02AC02\n\n\n\n15/06/23\nCompetition\nAugmentation Strategies and Albumentation\nTA Hùng\n230615 - M02AC03\n\n\n\n22/06/23\nCompetition\nNestquant\nTA Hùng\n230622 - M02AC04\n\n\n\n29/06/23\nCompetition\nOOD-CV\nTA Hùng\n230629 - M02AC05\n\n\n\n06/07/23\nCompetition\nVision Transformer\nTA Hùng\n230706 - M02AC06",
    "crumbs": [
      "Resources",
      "Course content",
      "Module 2"
    ]
  },
  {
    "objectID": "modules/module2.html#summer-school-on-deep-learning",
    "href": "modules/module2.html#summer-school-on-deep-learning",
    "title": "Module 2 - Calculus and Linear Algebra for AI",
    "section": "Summer School on Deep Learning",
    "text": "Summer School on Deep Learning\n\n\n\n\n\n\n\n\n\n\n\n Date\n Type\n Content\n Instructor\n Handout\n Further Reading\n\n\n\n\n05/07/23\nSummer school on Deep Learning\nVIASM Summer school on Recent Advances in Deep Learning\n\nSildes",
    "crumbs": [
      "Resources",
      "Course content",
      "Module 2"
    ]
  },
  {
    "objectID": "login_portal/a2fd87338037fe28bd289eab3fecc8bfd1a3e0410/about.html",
    "href": "login_portal/a2fd87338037fe28bd289eab3fecc8bfd1a3e0410/about.html",
    "title": "About",
    "section": "",
    "text": "About this site\n\n\n\n Back to top"
  },
  {
    "objectID": "login_portal/a2fd87338037fe28bd289eab3fecc8bfd1a3e0410/index.html",
    "href": "login_portal/a2fd87338037fe28bd289eab3fecc8bfd1a3e0410/index.html",
    "title": "a2fd87338037fe28bd289eab3fecc8bfd1a3e0410",
    "section": "",
    "text": "This is a Quarto website.\nTo learn more about Quarto websites visit https://quarto.org/docs/websites.\n\n\n\n Back to top"
  },
  {
    "objectID": "modules/module5.html#further-resources",
    "href": "modules/module5.html#further-resources",
    "title": "Module 5 - Deep Learning Prerequisites",
    "section": "Further Resources",
    "text": "Further Resources",
    "crumbs": [
      "Resources",
      "Course content",
      "Module 5"
    ]
  },
  {
    "objectID": "resources/research.html",
    "href": "resources/research.html",
    "title": "Resources for Research and Paper Writing",
    "section": "",
    "text": "“No research without action, no action without research.” Kurt Lewin\n: Youtube Video | : Paper/preprint | : Website/ Blogpost # General\n\n Hamming, “You and Your Research” (June 6, 1995)\n Bí kíp nâng cao KĨ NĂNG NGHIÊN CỨU KHOA HỌC | 🎙️PhD Whisperer Ep 4\n\n\nResearch Paper Writing\n\n Research Communication is IMPORTANT so DO BETTER\n How to write an okay research paper.\n How to effortlessly write a high quality scientific paper in the field of computational engineering and sciences\n Guidelines for Writing a Good NIPS Paper\n\n\n\n\n\n Back to top"
  }
]