{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "ydXl0YUqS384"
   },
   "source": [
    "## Multi-Process One-Thread"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "id": "1ZQ7oK9MTGrw"
   },
   "outputs": [],
   "source": [
    "import multiprocessing\n",
    "from multiprocessing import Process"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "zGOGU7vVTJAf",
    "outputId": "5e20d79c-4bc5-41ca-b1d3-65a9d7db2577"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Number of cpu cores:  16\n"
     ]
    }
   ],
   "source": [
    "# Check number of available cores\n",
    "print(\"Number of cpu cores: \", multiprocessing.cpu_count())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "id": "R-pqLdzOTWz3"
   },
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "from time import sleep, time\n",
    "\n",
    "# Example of a processing function\n",
    "def process_dataframe(chunk_id, chunk_data: pd.DataFrame):\n",
    "    print(f\"Processing chunk {chunk_id}\")\n",
    "    sleep(5)\n",
    "    print(f\"The chunk {chunk_id} has been processed successfully!\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "hj8DQqxb7NqN",
    "outputId": "e52c1d7b-c491-4227-9c9f-9d9059856a09"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Process ID: 26580\n",
      "Processing chunk 0\n",
      "Process ID: 26583\n",
      "Processing chunk 1\n",
      "Processing chunk 2\n",
      "Process ID: 26588\n",
      "The chunk 0 has been processed successfully!\n",
      "The chunk 1 has been processed successfully!\n",
      "The chunk 2 has been processed successfully!\n",
      "Total time: 5.035507917404175s\n"
     ]
    }
   ],
   "source": [
    "# Make a sample dataframe\n",
    "data = [\n",
    "    ['tom', 10], ['nick', 15],\n",
    "    ['juli', 14], ['peter', 20],\n",
    "    ['jason', 27], ['anna', 11]\n",
    "]\n",
    "\n",
    "# Create the pandas DataFrame\n",
    "df = pd.DataFrame(data, columns=['Name', 'Age'])\n",
    "\n",
    "# Devide the dataframe into chunks\n",
    "chunk_size = 2\n",
    "chunks = [df[i:i+chunk_size].to_numpy() for i in range(0, len(df), chunk_size)]\n",
    "\n",
    "# Mark the starting point to measure processing time\n",
    "start = time()\n",
    "\n",
    "procs = []\n",
    "for i, chunk in enumerate(chunks):\n",
    "  # Define our process but not yet start\n",
    "  proc = Process(target=process_dataframe, args=(i, chunk,))\n",
    "  # Start the process\n",
    "  proc.start()\n",
    "  # Investigate process ID\n",
    "  print(f\"Process ID: {proc.pid}\")\n",
    "  # Manage all process definitions in a list\n",
    "  procs.append(proc)\n",
    "\n",
    "# Stop all processes to prevent resource scarcity\n",
    "# imagine zombie processes\n",
    "for proc in procs:\n",
    "    proc.join() # Wait for the process to finish\n",
    "\n",
    "# Report total elapsed time\n",
    "print(f\"Total time: {time() - start}s\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "dI1nvsCSOzYM",
    "outputId": "0374964f-9a77-4f05-f025-f3d97a4ddfc0"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Processing chunk 0Processing chunk 1\n",
      "\n",
      "The chunk 1 has been processed successfully!The chunk 0 has been processed successfully!\n",
      "\n",
      "Processing chunk 2\n",
      "The chunk 2 has been processed successfully!\n",
      "Processing chunk 0Processing chunk 1\n",
      "\n",
      "The chunk 0 has been processed successfully!The chunk 1 has been processed successfully!\n",
      "\n",
      "Processing chunk 2\n",
      "The chunk 2 has been processed successfully!\n"
     ]
    }
   ],
   "source": [
    "##############################################\n",
    "# The 2nd way to do multiprocess is via Pool #\n",
    "##############################################\n",
    "from multiprocessing import Pool\n",
    "\n",
    "# Define a pool of processes\n",
    "NUM_PROCESSES = 2\n",
    "pool = multiprocessing.Pool(NUM_PROCESSES)\n",
    "\n",
    "procs = []\n",
    "for i, chunk in enumerate(chunks):\n",
    "    procs.append(\n",
    "        # The main process does not need to wait for this function\n",
    "        pool.apply_async(process_dataframe, args=(i, chunk))\n",
    "    )\n",
    "\n",
    "for proc in procs:\n",
    "    proc.get() # Wait for the process to finish"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "id": "hNl_vM4hWJUj"
   },
   "outputs": [],
   "source": [
    "##############################################\n",
    "# The 3rd way to do multiprocess is via Map #\n",
    "##############################################\n",
    "inputs = [(i, chunk) for i, chunk in enumerate(chunks)]\n",
    "\n",
    "# If you have one arg only, please use `.map` instead\n",
    "outputs = pool.starmap(\n",
    "    process_dataframe,\n",
    "    inputs\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "ICbCCRWkUKHW",
    "outputId": "a35359df-998b-40ff-8403-453ea3309eb7"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Processing chunk 0\n",
      "The chunk 0 has been processed successfully!\n",
      "Processing chunk 1\n",
      "The chunk 1 has been processed successfully!\n",
      "Processing chunk 2\n",
      "The chunk 2 has been processed successfully!\n",
      "Total time: 15.042691946029663s\n"
     ]
    }
   ],
   "source": [
    "# Uhm... you can see that sometimes, two strings are concatnated w/o any `/n`\n",
    "# this is because all the processes writes to stdout at the same time, we need\n",
    "# to find a way to prevent other processes write to it while one is doing\n",
    "from multiprocessing import Lock\n",
    "\n",
    "lock = Lock()\n",
    "\n",
    "def process_dataframe(chunk_id, chunk_data: pd.DataFrame):\n",
    "    lock.acquire()\n",
    "    print(f\"Processing chunk {chunk_id}\")\n",
    "    sleep(5)\n",
    "    print(f\"The chunk {chunk_id} has been processed successfully!\")\n",
    "    lock.release()\n",
    "\n",
    "# Make a sample dataframe\n",
    "data = [\n",
    "    ['tom', 10], ['nick', 15],\n",
    "    ['juli', 14], ['peter', 20],\n",
    "    ['jason', 27], ['anna', 11]\n",
    "]\n",
    "\n",
    "# Create the pandas DataFrame\n",
    "df = pd.DataFrame(data, columns=['Name', 'Age'])\n",
    "\n",
    "# Devide the dataframe into chunks\n",
    "chunk_size = 2\n",
    "chunks = [df[i:i+chunk_size] for i in range(0, len(df), chunk_size)]\n",
    "\n",
    "# Mark the starting point to measure processing time\n",
    "start = time()\n",
    "\n",
    "procs = []\n",
    "for i, chunk in enumerate(chunks):\n",
    "  # Define our process but not yet start\n",
    "  proc = Process(target=process_dataframe, args=(i, chunk,))\n",
    "  # Start the process\n",
    "  proc.start()\n",
    "  # Manage all process definitions in a list\n",
    "  procs.append(proc)\n",
    "\n",
    "# Stop all processes to prevent resource scarcity\n",
    "# imagine zombie processes\n",
    "for proc in procs:\n",
    "    proc.join()\n",
    "\n",
    "# Report total elapsed time\n",
    "print(f\"Total time: {time() - start}s\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "ax3LFgHcS93u"
   },
   "source": [
    "## Multi-Threaded"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "Mt-98oQ_CRgG"
   },
   "source": [
    "Python provides the same APIs to multiprocessing with `start()` and `join()`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "QR5SBZ1poPGE",
    "outputId": "fea68c6b-174a-47f1-c482-66f016a2bc02"
   },
   "outputs": [],
   "source": [
    "from threading import Thread\n",
    "\n",
    "my_threads = []\n",
    "for i, chunk in enumerate(chunks):\n",
    "    my_thread = Thread(target=process_dataframe, args=(i, chunk,))\n",
    "    my_thread.start()\n",
    "    my_threads.append(my_thread)\n",
    "\n",
    "for my_thread in my_threads:\n",
    "    my_thread.join()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "roVklnzdXyvA"
   },
   "source": [
    "However, the way they share data is different. Let's take a look at the example below"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "apL2XLwTefei",
    "outputId": "68b2ccca-03f3-4b66-82ef-da3902d91627"
   },
   "outputs": [],
   "source": [
    "from threading import Lock\n",
    "\n",
    "# Define a lock to prevent race condition,\n",
    "# which is multiple updates into the same variable\n",
    "lock = Lock()\n",
    "\n",
    "# Share data\n",
    "shared_data = 0\n",
    "\n",
    "def increment_function():\n",
    "    global shared_data\n",
    "    with lock: # This is equal to acquire() + release()\n",
    "        shared_data += 1\n",
    "\n",
    "my_threads = []\n",
    "for _ in range(3):\n",
    "    my_thread = Thread(target=increment_function)\n",
    "    my_thread.start()\n",
    "    my_threads.append(my_thread)\n",
    "\n",
    "for my_thread in my_threads:\n",
    "    my_thread.join()\n",
    "\n",
    "print(f\"Current value of shared_data: {shared_data}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "3zlNXu-lEpUR",
    "outputId": "7164ee3b-5bd6-42bd-dc97-a652784f854c"
   },
   "outputs": [],
   "source": [
    "# Another way to access shared_data is via Queue\n",
    "from queue import Queue\n",
    "\n",
    "# Initialize the queue\n",
    "shared_queue = Queue()\n",
    "\n",
    "def increment_function():\n",
    "    shared_queue.put(1)\n",
    "\n",
    "my_threads = []\n",
    "for _ in range(3):\n",
    "    my_thread = Thread(target=increment_function)\n",
    "    my_thread.start()\n",
    "    my_threads.append(my_thread)\n",
    "\n",
    "for my_thread in my_threads:\n",
    "    my_thread.join()\n",
    "\n",
    "shared_data = 0\n",
    "while not shared_queue.empty():\n",
    "    shared_data += shared_queue.get()\n",
    "\n",
    "print(f\"Current value of shared_data: {shared_data}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "CQYIIevMMqpS",
    "outputId": "d6a0cab0-ebb0-4ecf-882a-ada0b071b0ee"
   },
   "outputs": [],
   "source": [
    "# Using a global variable is not a piece of cake in multiprocessing\n",
    "# as in multithreading\n",
    "import multiprocessing\n",
    "\n",
    "# Define a shared value between oprocesses\n",
    "shared_variable = multiprocessing.Value('i', 0)\n",
    "\n",
    "def worker_function():\n",
    "    global shared_variable\n",
    "    with shared_variable.get_lock():\n",
    "        shared_variable.value += 1\n",
    "\n",
    "procs = []\n",
    "for _ in range(3):\n",
    "  # Define our process but not yet start\n",
    "  proc = Process(target=worker_function)\n",
    "  # Start the process\n",
    "  proc.start()\n",
    "  # Manage all process definitions in a list\n",
    "  procs.append(proc)\n",
    "\n",
    "for proc in procs:\n",
    "    proc.join()\n",
    "\n",
    "print(f\"Current value of shared_data: {shared_data}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "HtAeq1cUQJ9E",
    "outputId": "1fac16a4-7a16-4a9d-ad7d-ab72a0ae1d1d"
   },
   "outputs": [],
   "source": [
    "# Or using queue\n",
    "from multiprocessing import Queue\n",
    "from multiprocessing import Process\n",
    "\n",
    "def worker_function(shared_queue):\n",
    "    shared_queue.put(1)\n",
    "\n",
    "shared_queue = Queue()\n",
    "\n",
    "procs = []\n",
    "for _ in range(3):\n",
    "    my_proc = Process(target=worker_function, args=(shared_queue,))\n",
    "    my_proc.start()\n",
    "    procs.append(my_proc)\n",
    "\n",
    "for proc in procs:\n",
    "    proc.join()\n",
    "\n",
    "# The main process retrieves data from the queue\n",
    "# and aggregate the result\n",
    "result = 0\n",
    "while not shared_queue.empty():\n",
    "    result += shared_queue.get(1)\n",
    "\n",
    "print(f\"Results from multiprocessing queue: {result}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "wVcMwStgTBIo"
   },
   "source": [
    "## One-Process One-Thread"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "jfb2IrxEIMW7",
    "outputId": "3c554bd6-c8a1-42f8-9ca5-7de11d7eb3f4"
   },
   "outputs": [
    {
     "ename": "RuntimeError",
     "evalue": "This event loop is already running",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mRuntimeError\u001b[0m                              Traceback (most recent call last)",
      "\u001b[0;32m/tmp/ipykernel_26363/3615891077.py\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m     22\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     23\u001b[0m \u001b[0;31m# Run the event loop\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 24\u001b[0;31m \u001b[0mmain\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     25\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     26\u001b[0m \u001b[0;31m# YOU WILL MEET SOME WEIRD ERRORS HERE DUE TO IPYTHON NOTEBOOK,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/tmp/ipykernel_26363/3615891077.py\u001b[0m in \u001b[0;36mmain\u001b[0;34m()\u001b[0m\n\u001b[1;32m     15\u001b[0m \u001b[0;32mdef\u001b[0m \u001b[0mmain\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     16\u001b[0m     \u001b[0mloop\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0masyncio\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mget_event_loop\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 17\u001b[0;31m     loop.run_until_complete(asyncio.gather(\n\u001b[0m\u001b[1;32m     18\u001b[0m         \u001b[0mdownload_my_1st_data_func\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     19\u001b[0m         \u001b[0mdownload_my_2nd_data_func\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda3/lib/python3.9/asyncio/base_events.py\u001b[0m in \u001b[0;36mrun_until_complete\u001b[0;34m(self, future)\u001b[0m\n\u001b[1;32m    621\u001b[0m         \"\"\"\n\u001b[1;32m    622\u001b[0m         \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_check_closed\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 623\u001b[0;31m         \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_check_running\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    624\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    625\u001b[0m         \u001b[0mnew_task\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0mfutures\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0misfuture\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mfuture\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda3/lib/python3.9/asyncio/base_events.py\u001b[0m in \u001b[0;36m_check_running\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m    581\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0m_check_running\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    582\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mis_running\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 583\u001b[0;31m             \u001b[0;32mraise\u001b[0m \u001b[0mRuntimeError\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m'This event loop is already running'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    584\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0mevents\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_get_running_loop\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32mis\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    585\u001b[0m             raise RuntimeError(\n",
      "\u001b[0;31mRuntimeError\u001b[0m: This event loop is already running"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Starting my 1st data func...\n",
      "Starting my 2nd data func...\n",
      "Completed my 1st data func!\n",
      "Completed my 2nd data func!\n"
     ]
    }
   ],
   "source": [
    "# Let's say you have one process with one thread only, how to deal with it?\n",
    "# AsyncIO helps us to do it\n",
    "import asyncio\n",
    "\n",
    "async def download_my_1st_data_func():\n",
    "    print(\"Starting my 1st data func...\")\n",
    "    await asyncio.sleep(2)\n",
    "    print(\"Completed my 1st data func!\")\n",
    "\n",
    "async def download_my_2nd_data_func():\n",
    "    print(\"Starting my 2nd data func...\")\n",
    "    await asyncio.sleep(2)\n",
    "    print(\"Completed my 2nd data func!\")\n",
    "\n",
    "def main():\n",
    "    loop = asyncio.get_event_loop()\n",
    "    loop.run_until_complete(asyncio.gather(\n",
    "        download_my_1st_data_func(),\n",
    "        download_my_2nd_data_func(),\n",
    "    ))\n",
    "    loop.close()\n",
    "\n",
    "# Run the event loop\n",
    "main()\n",
    "\n",
    "# YOU WILL MEET SOME WEIRD ERRORS HERE DUE TO IPYTHON NOTEBOOK,\n",
    "# LET'S MOVE TO A PYTHON SCRIPT"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "colab": {
   "provenance": [],
   "toc_visible": true
  },
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
