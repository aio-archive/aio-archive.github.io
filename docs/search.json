[
  {
    "objectID": "engineer/index.html",
    "href": "engineer/index.html",
    "title": "Engineer",
    "section": "",
    "text": "Computer Networking Fundamentals\n\n\n\n\n Back to top"
  },
  {
    "objectID": "academia/index.html",
    "href": "academia/index.html",
    "title": "Academia",
    "section": "",
    "text": "Paper Writing Guidelines\n\n\n\n\n Back to top"
  },
  {
    "objectID": "index.html",
    "href": "index.html",
    "title": "AIO2023 Lectures",
    "section": "",
    "text": "Welcome to the AIO2023 course! This is an extensive one-year course that cover topics in AI, from the fundamental prerequisites to the most advanced and recent topics (like Generative AI, Diffusion models, CLIP, and Large language models). Note that each module is designed so that they are independent of each other. It’s best if you take study the material module in sequential order, however, feel free to jump to the module that you prefer once you are confident about your foundation knowledge. If you already learn the basics (eg. linear algebrea concepts, calculus, probability, and basic Python coding skills) and want to learn more about AI, feel free to start from Module 4 to discover more advanced topics.\nEssentially, there are fours main parts during the course: (1) The main lectures on Wednesday and Friday weekly; (2) the extra classes / TA lessons / Project tutorials on Saturday weekly; (3) the pre-lecture lecture on Tuesday weekly; and (4) the seminars on Sunday occcasionally.\nThe overall course schedule can be wrap up in the following table."
  },
  {
    "objectID": "index.html#introduction",
    "href": "index.html#introduction",
    "title": "AIO2023 Lectures",
    "section": "1.1 Introduction",
    "text": "1.1 Introduction\nWelcome to Module 1: Introduction to Python Programming of the AIO course. The goal of this module is to teach you basic Python programming skills, spanning from the very fundamental things like variables, functions, … to Object-oriented Programming using Python and as well as the basic data structures. There are in total 3 assignments for the main lessons. There will also be one lecture that acts as a supplemnentary before the main lecture."
  },
  {
    "objectID": "index.html#projects",
    "href": "index.html#projects",
    "title": "AIO2023 Lectures",
    "section": "1.2 Projects",
    "text": "1.2 Projects\nRegarding the projects, there are three projects where you will respectively learn how to use YOLOv8, an object detection model as well as how to use Python to manipulate and crawl data from a website. Finally, the last project is about developing simple applications using ChatGPT. In particular, the three projects are:\n\nObject Detection with YOLOv8\nData Manipulation and Crawling\nChatGPT Applications"
  },
  {
    "objectID": "index.html#competition-training",
    "href": "index.html#competition-training",
    "title": "AIO2023 Lectures",
    "section": "1.3 Competition Training",
    "text": "1.3 Competition Training\nAs for competition training, this module contains three lectures with the goal of teaching you the basic skills and knowledge you need before joining an AI competition, including visualizing data, knowledge about competition tasks and metrics, and design validation."
  },
  {
    "objectID": "index.html#extra-class",
    "href": "index.html#extra-class",
    "title": "AIO2023 Lectures",
    "section": "1.4 Extra class",
    "text": "1.4 Extra class\nThe central theme of the extra class for this module is about Algorithms and Complexity. In the age of AI, still, the knowledge about algorithms and their complexity including Big-O, Brute-force exhaustive, recursion, two pointer, and dynamic programming still plays an immensely important role."
  },
  {
    "objectID": "index.html#introduction-1",
    "href": "index.html#introduction-1",
    "title": "AIO2023 Lectures",
    "section": "2.1 Introduction",
    "text": "2.1 Introduction\nWelcome to the second module of the AIO2023 course. The central theme of this module is about Calculus and Linear Algebra, the two subfields of mathematics that I cannot emphasize enough the importance of them in understanding and developing machine learning models. My take on whether to learn math when you want to learn AI is that although it is often the case that many modern ready-made libraries (eg Numpy, Sklearn, Tensorflow, PyTorch, JAX, …) already support the underlying math, having a thorough understanding of the math behind the machine learning models makes you significantly more efficient in debugging , i.e. knowing what’s wrong with the model, or what particular model specification to apply for your specific problem."
  },
  {
    "objectID": "mlops/index.html",
    "href": "mlops/index.html",
    "title": "MLOps",
    "section": "",
    "text": "Module 1\n\nLinux\n\n\n\n\n\n Back to top"
  },
  {
    "objectID": "stanford/cs336/moe.html#routing-function",
    "href": "stanford/cs336/moe.html#routing-function",
    "title": "Mixture of Experts",
    "section": "Routing function",
    "text": "Routing function\nRouting algorithms:\n\ntoken chooses expert: choose top-K experts for each token\nexpert chooses token: each expert chooses top-K tokens\nglobal routing via optimization: solve a global optimization problem to find the best routing.\n\nAlmost all MOEs boils down to choose “\\(\\text{top}-K\\)”.\n\n\nExpert sizes\nTraining objectives"
  },
  {
    "objectID": "stanford/cs336/moe.html#top-k-routing",
    "href": "stanford/cs336/moe.html#top-k-routing",
    "title": "Mixture of Experts",
    "section": "Top-K Routing",
    "text": "Top-K Routing\nMost papers do the old and classic top-k routing. How does this work?"
  },
  {
    "objectID": "stanford/cs336/moe.html#gating",
    "href": "stanford/cs336/moe.html#gating",
    "title": "Mixture of Experts",
    "section": "Gating",
    "text": "Gating\n\\[\n\\mathbf{h}_t^l = \\sum_{i=1}^{N} \\left( g_{i,t} \\text{FFN}_i \\left( \\mathbf{u}_t^l \\right) \\right) + \\mathbf{u}_t^l,\n\\]\n\\[\ng_{i,t} = \\begin{cases}\ns_{i,t}, & s_{i,t} \\in \\text{Topk}(\\{s_{j,t}|1 \\leq j \\leq N\\}, K), \\\\\n0, & \\text{otherwise},\n\\end{cases}\n\\]\n\\[\ns_{i,t} = \\text{Softmax}_i \\left( \\mathbf{u}_t^{lT} \\mathbf{e}_i^l \\right),\n\\]\nGates selected by a logistic regressor\nThis is the DeepSeek (V1-2) router (Grok, Qwen do this too)\nMixtral, DBRX, DeepSeek v3 softmaxes after the TopK"
  },
  {
    "objectID": "stanford/cs336/moe.html#rl-for-moes",
    "href": "stanford/cs336/moe.html#rl-for-moes",
    "title": "Mixture of Experts",
    "section": "RL for MoEs",
    "text": "RL for MoEs\nRL via REINFORCE, but not so much better."
  },
  {
    "objectID": "stanford/cs336/moe.html#stochastic-approximations",
    "href": "stanford/cs336/moe.html#stochastic-approximations",
    "title": "Mixture of Experts",
    "section": "Stochastic approximations",
    "text": "Stochastic approximations\n\\[G(x) = Softmax(KeepTopK(H(x), k))\\]\n\\[H(x)_i = (x \\cdot W_g)_i + StandardNormal() \\cdot Softplus((x \\cdot W_{noise})_i)\\]\n\\[KeepTopK(v, k)_i = \\begin{cases}\nv_i & \\text{if } v_i \\text{ is in the top } k \\text{ elements of } v. \\\\\n-\\infty & \\text{otherwise.}\n\\end{cases}\\]\nFrom Shazeer et al 2017 - routing decisions are stochastic with gaussian perturbations.\n\nThis naturally leads to experts that are a bit more robust.\nThe softmax means that the model learns how to rank K experts"
  },
  {
    "objectID": "stanford/cs336/l1.html",
    "href": "stanford/cs336/l1.html",
    "title": "CS336 Lecture 1 Overview",
    "section": "",
    "text": "Lecture Recording\nThree types of knowledge:\naccuracy = effciency \\(\\times\\) resources\nefficiency is way more important at larger scale (can not be wasteful).\n-&gt; What is the best model that we con build given a certain compute and budget ?"
  },
  {
    "objectID": "stanford/cs336/l1.html#before-2010s",
    "href": "stanford/cs336/l1.html#before-2010s",
    "title": "CS336 Lecture 1 Overview",
    "section": "Before 2010s",
    "text": "Before 2010s\n\nLanguage model to measure the entropy of English (Shannon, 1950)\nn-gram language model (for machine translation, speech recognition)"
  },
  {
    "objectID": "stanford/cs336/l1.html#neural-ingredients-2010s",
    "href": "stanford/cs336/l1.html#neural-ingredients-2010s",
    "title": "CS336 Lecture 1 Overview",
    "section": "Neural ingredients (2010s)",
    "text": "Neural ingredients (2010s)\n\nFirst language model (Bengio, 2003)\nSequence-to-sequence modeling\nAttention mechanism (Bahdanau, 2014)\nTransformer architecture\nMixture of Experts\nModel parallelism"
  },
  {
    "objectID": "stanford/cs336/l1.html#early-foundation-models",
    "href": "stanford/cs336/l1.html#early-foundation-models",
    "title": "CS336 Lecture 1 Overview",
    "section": "Early foundation models",
    "text": "Early foundation models\n\nELMO, BERT, Google’s T5"
  },
  {
    "objectID": "stanford/cs336/l1.html#embracing-scaling",
    "href": "stanford/cs336/l1.html#embracing-scaling",
    "title": "CS336 Lecture 1 Overview",
    "section": "Embracing scaling",
    "text": "Embracing scaling\n\nOpenAI’s GPT-2 (1.5B), GPT-3, PaLM, Chinchilla.\n\nthese are all closed models."
  },
  {
    "objectID": "stanford/cs336/l1.html#open-models",
    "href": "stanford/cs336/l1.html#open-models",
    "title": "CS336 Lecture 1 Overview",
    "section": "Open models",
    "text": "Open models\nBLOOM, Llama, Qwen, OLMo\n\nClosed model\nOpen-weight models (DeepSeek)\nOpen-source models (OLMo)"
  },
  {
    "objectID": "stanford/cs336/l1.html#kernels",
    "href": "stanford/cs336/l1.html#kernels",
    "title": "CS336 Lecture 1 Overview",
    "section": "Kernels",
    "text": "Kernels\n\n\nData movement between GPUs is even slower, but same ‘minimize data movement’ principle holds Use collective operations (e.g., gather, reduce, all-reduce) Shard (parameters, activations, gradients, optimizer states) across GPUs How to split computation: {data,tensor, pipeline,sequence) parallelism"
  },
  {
    "objectID": "stanford/cs336/l1.html#inference",
    "href": "stanford/cs336/l1.html#inference",
    "title": "CS336 Lecture 1 Overview",
    "section": "Inference",
    "text": "Inference\nGoals: generate tokens given a prompt. Inference is also needed for reinforment learning, test-time compute, evaluation, …\n\nPrefill (similar to training): tokens are given, can process all at once (compute-bound) Decode: need to generate one token at a time (memory-bound) Methods to speed up decoding: • Use cheaper model (via model pruning, quantization, distillation) • Speculative decoding: use a cheaper “draft” model to generate multiple tokens, then use the full model to score in parallel (exact decoding!) • Systems optimizations: KV caching, batching"
  },
  {
    "objectID": "stanford/cs336/l3.html#gated-activations-glu",
    "href": "stanford/cs336/l3.html#gated-activations-glu",
    "title": "CS336 Lecture 3: Architectures and Hyperparameters",
    "section": "Gated activations (*GLU)",
    "text": "Gated activations (*GLU)\nGLUs modify the ‘first part’ of a FF layer\n\\[\nFF(x) = \\max(0, xW_1) W_2\n\\]\nInstead of a linear + ReLU, augment the above with an (entrywise) linear term\n\\[\n\\max(0, xW_1) \\rightarrow \\max(0, xW_1) \\otimes (xV)\n\\]\nThis gives the gated variant (ReGLU) – note that we have an extra parameter (V)\n\\[\nFF_{\\mathrm{ReGLU}}(x) = (\\max(0, xW_1) \\otimes xV) W_2\n\\]"
  },
  {
    "objectID": "stanford/cs336/l3.html#gated-variants-of-standard-ff-layers",
    "href": "stanford/cs336/l3.html#gated-variants-of-standard-ff-layers",
    "title": "CS336 Lecture 3: Architectures and Hyperparameters",
    "section": "Gated variants of standard FF layers",
    "text": "Gated variants of standard FF layers\n\nGeGLU\n\\[\n\\mathrm{FFN}_{\\mathrm{GeGLU}}(x, W, V, W_2) = (\\mathrm{GELU}(xW) \\otimes xV) W_2\n\\]\nNotable models:\nT5 v1.1, mT5, LaMDA, Phi3, Gemma 2, Gemma 3\n\n\n\nSwiGLU\n(swish is \\(x * \\mathrm{sigmoid}(x)\\))\n\\[\n\\mathrm{FFN}_{\\mathrm{SwiGLU}}(x, W, V, W_2) = (\\mathrm{Swish}_1(xW) \\otimes xV) W_2\n\\]\nNotable models:\nLLaMa 1/2/3, PaLM, Mistral, OLMo, most models post 2023\n\nNote: Gated models use smaller dimensions for the \\(d_{ff}\\) by \\(2/3\\)"
  },
  {
    "objectID": "stanford/cs336/l3.html#hyperparameters",
    "href": "stanford/cs336/l3.html#hyperparameters",
    "title": "CS336 Lecture 3: Architectures and Hyperparameters",
    "section": "Hyperparameters",
    "text": "Hyperparameters\nTransformer hyperparameter questions you might have had in 224n..\n\nHow much bigger should the feedforward size be compared to hidden size?\nHow many heads, and should num_heads always divide hidden size?\nWhat should my vocab size be?\n\nAnd other model setting questions\n\nDo people even regularize these huge LMs?\nHow do people scale these models – very deep or very wide?"
  },
  {
    "objectID": "stanford/cs336/l3.html#surprising-consensus-hyperparameter-1",
    "href": "stanford/cs336/l3.html#surprising-consensus-hyperparameter-1",
    "title": "CS336 Lecture 3: Architectures and Hyperparameters",
    "section": "Surprising (?) consensus hyperparameter 1",
    "text": "Surprising (?) consensus hyperparameter 1\nFeedforward – model dimension ratio.\n\\[\n\\mathrm{FFN}(x) = \\max(0, xW_1 + b_1) W_2 + b_2\n\\]\nThere are two dimensions that are relevant – the feedforward dim (\\(d_{ff}\\)) and model dim (\\(d_{model}\\)). What should their relationship be?\n\\[\nd_{ff} = 4\\, d_{model}\n\\]\nThis is almost always true. There’s just a few exceptions."
  },
  {
    "objectID": "stanford/cs336/l3.html#surprising-consensus-hyperparameter-2",
    "href": "stanford/cs336/l3.html#surprising-consensus-hyperparameter-2",
    "title": "CS336 Lecture 3: Architectures and Hyperparameters",
    "section": "Surprising (?) consensus hyperparameter 2",
    "text": "Surprising (?) consensus hyperparameter 2\nHead-dim \\(\\times\\) num-heads to model-dim ratio. As a reminder, slide from 224n.\n\nMulti-head self-attention is computationally efficient\n\nEven though we compute \\(h\\) many attention heads, it’s not really more costly.\n\nWe compute \\(XQ \\in \\mathbb{R}^{n \\times d}\\), and then reshape to \\(\\mathbb{R}^{n \\times h \\times d/h}\\). (Likewise for \\(XK\\), \\(XV\\).)\nThen we transpose to \\(\\mathbb{R}^{h \\times n \\times d/h}\\); now the head axis is like a batch axis.\nAlmost everything else is identical, and the matrices are the same sizes.\n\n\n\nThis doesn’t have to be true: we can have head-dimensions \\(&gt;\\) model-dim / num-heads.\nBut most models do follow this guideline."
  },
  {
    "objectID": "stanford/cs336/assignments/a1/a1.html",
    "href": "stanford/cs336/assignments/a1/a1.html",
    "title": "CS336 Assignment 1 - Building a Transformer LM",
    "section": "",
    "text": "PDF\n\n\n\n Back to top"
  },
  {
    "objectID": "blogs/index.html",
    "href": "blogs/index.html",
    "title": "Blog",
    "section": "",
    "text": "Modality Gap in CLIP Embedding Space\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nTools I use for (AI) research\n\n\n\n\n\n\n\n\n\nMay 9, 2025\n\n\n\n\n\n\nNo matching items\n Back to top"
  },
  {
    "objectID": "blogs/tools.html",
    "href": "blogs/tools.html",
    "title": "Tools I use for (AI) research",
    "section": "",
    "text": "Warning\n\n\n\nThis is not a typical SEO-optimized blog post that you’d find that suggest using obvious tools like Google Scholar, ResearchGate, EndNote, Zotero, etc. (Though those are great too!) All of these tools listed here tools that I actually use, and are free.\n\n\n\nPaperLib\nIf you have ever written research papers and have to cite sources, one of the biggest problems you might encounter is that you have to manually search for the right citation. Google Scholar is sometimes provide the citation for the pre-print version of the paper, not the published version (with actual conference and journals). For example, if you search for the paper “Lxmert: Learning cross-modality encoder representations from transformers” on Google Scholar, you will get the following BibTeX entry:\n@article{tan2019lxmert,\n  title={Lxmert: Learning cross-modality encoder representations from transformers},\n  author={Tan, Hao and Bansal, Mohit},\n  journal={arXiv preprint arXiv:1908.07490},\n  year={2019}\n}\nwhereas what I want is the version of the paper that is published in EMNLP 2019. PaperLib’s fuzzily scrape feature allows me do this. Here’s the citation for the published version:\n@inproceedings{tan2019lxmert,\n    author = {Tan, Hao Hao and Bansal, Mohit},\n    booktitle = {EMNLP},\n    year = {2019},\n    pages = {5099--5110},\n    organization = {},\n    title = {LXMERT: Learning {Cross}-{Modality} {Encoder} {Representations} from {Transformers}},\n    volume = {},\n}\nNotice how it subtlely but every word in the brackers {}. This is to ensure that the citation is properly capitalized when you used the BibTex entry in your LaTeX document.\nThe second feature that I love about PaperLib is that it has an extension that allows me to directly import the paper. When I was reading a paper on my browser, I can just right click and import the paper into PaperLib (with the proper tag if you like). There are also other extensions built for PaperLib that you could explore by yourself.\n\nAnother feature I find useful is you can abbreviate the publication venue. In machine learning, top-tier conferences like Advances in Neural Information Processing Systems (NeurIPS) and International Conference on Learning Representations (ICLR) are abbreviated as NeurIPS and ICLR respectively. Sometime these can be inconsistent, like “Advances in Neural Information Processing Systems” vs. “Conference on Neural Information Processing Systems”. PaperLib allows you to abbreviate the venue name so that the BibTeX entry is more consistent. Here’s the list that I use:\n\nConference on Computer Vision and Pattern Recognition -&gt; CVPR\nInternational Conference on Computer Vision -&gt; ICCV\nEuropean Conference on Computer Vision -&gt; ECCV\nConference on Neural Information Processing Systems -&gt; NeurIPS\nInternational Conference on Machine Learning -&gt; ICML\nInternational Conference on Learning Representations -&gt; ICLR\nAAAI Conference on Artificial Intelligence -&gt; AAAI\nInternational Joint Conference on Artificial Intelligence -&gt; IJCAI\nAnnual Meeting of the Association for Computational Linguistics -&gt; ACL\nConference on Empirical Methods in Natural Language Processing -&gt; EMNLP\nNorth American Chapter of the ACL -&gt; NAACL\n\nYou can add yours in PaperLib -&gt; Settings -&gt; Export -&gt; Publication Abbreviation.\nThe best of all ? PaperLib is free!\n\n\nPaper Visualizer\nIf you have ever drown into research paper with too many details (of architecture, training, etc.) and want to get a quick overview, Paper Visualizer is the tool for you. It helps create a big picture using diagrams. I started using it for roughly two weeks and still exploring the other available features, but it’s already a game changer for me.\nHere’s a diagram that Paper Visualizer creates for the “Attention is All You Need” paper:\n\nThis is really useful because it creates immediate hypothesis, how the authors tested it, and what the results are, etc.\nZooming in, you get an AI features for each node to further explain the concepts:\n\nAnd you can also embed it into your website to easily share it with others, like I did below for the MERU paper:\n\n\nThe best part ? Paper Visualizer is free (for now)!\n\n\nAi2PaperFinder\nJust as the name suggest, this tool, developed by Ai2, is an AI-powered tool that allows you to find the right paper for your research. It’s a great tool for getting a quick overview of the paper, and it’s free! Just enter the keyword and it will give you a list of papers that are related to your query (with different levels of Relevance).\n\n\n\nGoogle Scholar PDF Reader\nGoogle Scholar PDF Reader is an extension for reading PDF right on your browser. The selling-point of this from other PDF viewer is that it allows me to preview the citation without having to scroll all the way to the bottom of the page and then re-scroll back to where I was.\n\nIt does come with an AI-generated outline, but I don’t use it much.\n\n\n\n\n Back to top"
  },
  {
    "objectID": "blogs/modality-gap.html#what-is-the-modality-gap",
    "href": "blogs/modality-gap.html#what-is-the-modality-gap",
    "title": "Modality Gap in CLIP Embedding Space",
    "section": "What is the Modality Gap?",
    "text": "What is the Modality Gap?\nThe modality gap refers to a geometric phenomenon in the representation space of multimodal models where embeddings from different modalities (e.g., images and text) occupy distinct, separated regions rather than being fully integrated in the shared representation space (Liang et al. 2022). This separation creates a clear geometric distance between different modalities, despite these models being explicitly trained to align cross-modal representations. For example, in CLIP (Contrastive Language-Image Pre-training), image and text embeddings are systematically located in completely separate regions of the embedding space, even though they are supposed to represent semantically similar concepts[2]."
  },
  {
    "objectID": "blogs/modality-gap.html#causes-of-the-modality-gap",
    "href": "blogs/modality-gap.html#causes-of-the-modality-gap",
    "title": "Modality Gap in CLIP Embedding Space",
    "section": "Causes of the Modality Gap",
    "text": "Causes of the Modality Gap\nResearch has identified several key factors that contribute to the emergence and persistence of the modality gap:\n\n1. Neural Network Architecture and Initialization\nThe general inductive bias of deep neural architectures creates a “cone effect” where the effective embedding space is restricted to a narrow cone for both pre-trained models and models with random weights[7][15]. Different random initializations create different embedding cones, and since multimodal models typically use two separate encoders, the representations of the two modalities are already clearly apart at initialization[1]. Theoretical analysis shows that each neural network layer shrinks the angle between embedding vectors with high probability, creating narrower cones in deeper architectures[7].\n\n\n2. Contrastive Learning Dynamics\nThe contrastive learning objective commonly used in multimodal models preserves or even enlarges the gap during training[1][15]. Research has demonstrated that:\n\nThe learnable temperature parameter in the contrastive loss function significantly influences the modality gap persistence[2][4]\nMismatched data pairs, especially at early training stages, cause the modality gap to enlarge before potentially beginning to close[4][10]\nGradient flow analysis reveals that the modality gap diminishes at an extremely slow rate (O(1/√t) in training time t), explaining why it persists in trained models[10]\n\n\n\n3. Temperature Parameter Effects\nSmall temperature settings in fixed temperature mode and almost invariably under learned temperature mode settings lead to consistent emergence of modality gaps, regardless of the initial temperature value[12]. The choice of temperature parameterization affects the rate at which the gap closes or widens[4]."
  },
  {
    "objectID": "blogs/modality-gap.html#mitigation-strategies",
    "href": "blogs/modality-gap.html#mitigation-strategies",
    "title": "Modality Gap in CLIP Embedding Space",
    "section": "Mitigation Strategies",
    "text": "Mitigation Strategies\nResearchers have proposed various approaches to mitigate the modality gap:\n\nObjective and Loss Function Modifications\n\nTemperature Control Strategies:\n\nTemperature Scheduling (TS): Enforcing a linearly increasing schedule for temperature during training[4]\nTemperature Reparameterization (TR): Replacing the original parameterization with alternatives that yield a higher gap closing rate[4]\nSmaller Learning Rate of Temperature (SLRT): Using a smaller learning rate for the temperature parameter compared to other learnable parameters[4]\nFixed on Large Temperature (FLT): Fixing the temperature at a high value throughout training[4]\n\nRemoving Repulsive Structure: Modifying the contrastive loss by removing the repulsive structure (which pushes negative examples away) helps close the modality gap[10]. This approach retains only the mechanism of pulling positive samples together.\n\n\n\nModality Swapping Approaches\n\nHard Swapping Between Modalities (HS): Randomly selecting image-text pairs and exchanging their features in the shared feature space[4]\nSoft Swapping Between Modalities (SS): Mixing the features of image-text pairs to create new pairs[4]\n\nThese approaches prevent the two modalities from remaining segregated into parallel planes, thereby reducing overall repulsion between them[4].\n\n\nRegularization Techniques\nStudies in speech translation have found that regularization plays a more important role than well-designed modality adaptation methods in addressing the modality gap[3][9][11]. Regularization helps prevent overfitting, which can exacerbate the modality gap, especially when transferring from high-resource to low-resource tasks[11].\n\n\nRepresentation Space Engineering\n\nSemantic Enhancement: Improving both inter-modality semantic consistency and intra-modality semantic completion[10]\nInter-MCR Alignment: Aligning semantically-enhanced embeddings across different multimodal contrastive representation (MCR) spaces[10]\nIntra-MCR Alignment: Realigning semantically similar embeddings across modalities within each MCR space to alleviate the modality gap[10]"
  },
  {
    "objectID": "blogs/modality-gap.html#relationship-to-downstream-task-performance",
    "href": "blogs/modality-gap.html#relationship-to-downstream-task-performance",
    "title": "Modality Gap in CLIP Embedding Space",
    "section": "Relationship to Downstream Task Performance",
    "text": "Relationship to Downstream Task Performance\nThe relationship between modality gap and downstream task performance is complex and task-dependent:\n\nPositive Impacts\n\nReducing the modality gap shows consistent improvement in image-text retrieval accuracy[4][10]\nSome research indicates that varying the modality gap distance can improve zero-shot classification performance and fairness[1][7]\n\n\n\nNuanced Effects\n\nReducing the gap does not consistently improve all downstream task performance, suggesting its role may be more nuanced than previously understood[12]\nWhile smaller modality gaps clearly lead to higher retrieval accuracy, they may not significantly affect zero-shot and linear probe classification accuracy[10]\nFeature space uniformity appears more important than modality gap reduction for visual classification tasks, including zero-shot and linear probing[10]\nFor challenging vision-language question-answering tasks, neither modality gap nor uniformity shows a strong correlation with performance[10]\n\nThis evidence suggests that the modality gap might be a geometric by-product of learning methods rather than a critical determinant of representation quality across all tasks[12]."
  },
  {
    "objectID": "blogs/modality-gap.html#open-research-questions-and-future-directions",
    "href": "blogs/modality-gap.html#open-research-questions-and-future-directions",
    "title": "Modality Gap in CLIP Embedding Space",
    "section": "Open Research Questions and Future Directions",
    "text": "Open Research Questions and Future Directions\nSeveral important research gaps remain in understanding and addressing the modality gap:\n\nTask-Specific Impacts: Further research is needed to understand why modality gap reduction benefits certain tasks (like retrieval) more than others (like classification)[10][12]\nRelationship to Other Representation Properties: The interplay between modality gap and other properties of the representation space, such as uniformity, remains underexplored[10]\nDataset Characteristics: The role of dataset characteristics in influencing the modality gap needs deeper investigation, as individual dataset characteristics significantly influence the gap’s manifestation[12]\nTheoretical Understanding: Extending gradient flow analysis to study the difficulty of closing the gap with varying levels of shared information between modalities by modeling data distributions[4]\nApplication to Finetuning: Applying insights to finetuning scenarios where domain differences between pretraining and finetuning data need to be considered[4]\nCompressibility of Dynamics: Investigating the compressibility of the Riemannian dynamics trajectory in multimodal learning to enhance efficiency and performance[4]\nFairness and Bias: Understanding how modality gap relates to fairness concerns and modality bias in recommendation systems and other applications[5]"
  },
  {
    "objectID": "blogs/modality-gap.html#conclusion",
    "href": "blogs/modality-gap.html#conclusion",
    "title": "Modality Gap in CLIP Embedding Space",
    "section": "Conclusion",
    "text": "Conclusion\nThe modality gap represents a fundamental challenge in multimodal representation learning. While significant progress has been made in understanding its causes and developing mitigation strategies, the relationship between modality gap and model performance remains complex and task-dependent. Future research should focus on a more nuanced understanding of when and how to address the modality gap based on specific applications and desired outcomes, rather than assuming its reduction is universally beneficial.\nThe field would benefit from a reevaluation of the modality gap’s significance in multimodal contrastive learning, with greater emphasis on dataset characteristics, contrastive learning methodology, and task-specific requirements[12]. As multimodal systems continue to advance, addressing these open questions will be crucial for developing more effective and robust models."
  },
  {
    "objectID": "stanford/cs336/tokenization.html",
    "href": "stanford/cs336/tokenization.html",
    "title": "Tokenization",
    "section": "",
    "text": "# Basic chr() usage\nassert chr(97) == \"a\"\nassert chr(127757) == \"🌍\"  # 127757 is actually the Earth Globe Europe-Africa emoji\n\n# Build a simple Character Tokenizer and test round-trip encoding/decoding\ntokenizer = CharacterTokenizer()\n\nstring = \"Hello, 🌍! 1rt7!\"  # @inspect string\nindices = tokenizer.encode(string)  # @inspect indices\nreconstructed_string = tokenizer.decode(indices)  # @inspect reconstructed_string\n\nassert string == reconstructed_string\n\n# Unicode has approximately 150K characters\n# [Wikipedia: List of Unicode characters]\n\n# Vocabulary size estimation\nvocabulary_size = max(indices) + 1  # This is a lower bound\n# @inspect vocabulary_size\n\n# Problems:\n# Problem 1: This is a very large vocabulary.\n# Problem 2: Many characters are quite rare (e.g., 🌍), making vocabulary inefficient.\n\n# Compression ratio\ncompression_ratio = get_compression_ratio(string, indices)  # @inspect compression_ratio\n\nByte-based tokenization\nUnicode strings can be represented as a sequence of bytes (integers between 0 and 255).\nThe most common Unicode encoding is UTF-8.\n# Some Unicode characters are represented by one byte:\nassert bytes(\"a\", encoding=\"utf-8\") == b\"a\"\n\n# Others take multiple bytes:\nassert bytes(\"🌍\", encoding=\"utf-8\") == b\"\\xf0\\x9f\\x8c\\x8d\"\n\n# Now let's build a Byte-based Tokenizer and test round-trip encoding/decoding\ntokenizer = ByteTokenizer()\n\nstring = \"Hello, 🌍！你好！\"  # @inspect string\nindices = tokenizer.encode(string)  # @inspect indices\nreconstructed_string = tokenizer.decode(indices)  # @inspect reconstructed_string\n\nassert string == reconstructed_string\n\n\nWord-based tokenization\nByte Pair Encoding (BPE) [Wikipedia] The BPE algorithm was introduced by Philip Gage in 1994 for data compression. [article] It was adapted to NLP for neural machine translation. [Sennrich+ 2015] (Previously, papers had been using word-based tokenization.) BPE was then used by GPT-2. [Radford+ 2019] Basic idea: train the tokenizer on raw text to automatically determine the vocabulary. Intuition: common sequences of characters are represented by a single token, rare sequences are represented by many tokens. The GPT-2 paper used word-based tokenizatich to break up the text into inital segments and run the original BPE algorithm on each segment. Sketcl n byte as a l token, and successively merge the most common pair of adjacent tokens.\nfrom collections import defaultdict\nfrom typing import Dict, Tuple\n\nclass BETokenizerParams:\n    def __init__(self, merges: Dict[Tuple[int, int], int], vocab: Dict[int, bytes]):\n        self.merges = merges  # pair -&gt; new index\n        self.vocab = vocab    # index -&gt; bytes\n\ndef train_bpe(string: str, num_merges: int) -&gt; BETokenizerParams:\n    # @inspect string, @inspect num_merges\n    \n    # Start with the list of bytes of string\n    indices = list(map(int, string.encode(\"utf-8\")))  # @inspect indices\n    \n    merges: Dict[Tuple[int, int], int] = {}  # (index1, index2) -&gt; merged index\n    vocab: Dict[int, bytes] = {x: bytes([x]) for x in range(256)}  # index -&gt; bytes\n\n    for i in range(num_merges):\n        # Count the number of occurrences of each pair of tokens\n        counts = defaultdict(int)\n        for index1, index2 in zip(indices, indices[1:]):  # For each adjacent pair\n            counts[(index1, index2)] += 1  # @inspect counts\n\n        # Find the most common pair\n        if not counts:\n            break  # No more pairs to merge\n\n        pair = max(counts, key=counts.get)  # @inspect pair\n        index1, index2 = pair\n\n        # Create a new index for the merged pair\n        new_index = max(vocab.keys()) + 1\n        vocab[new_index] = vocab[index1] + vocab[index2]\n        merges[(index1, index2)] = new_index\n\n        # Update the sequence (merge occurrences of the pair)\n        new_indices = []\n        skip = False\n        for j in range(len(indices)):\n            if skip:\n                skip = False\n                continue\n            if j &lt; len(indices) - 1 and (indices[j], indices[j+1]) == pair:\n                new_indices.append(new_index)\n                skip = True  # Skip next index because it's merged\n            else:\n                new_indices.append(indices[j])\n        indices = new_indices  # Update for next iteration\n\n    return BETokenizerParams(merges=merges, vocab=vocab)\n\n\n\n\n Back to top"
  },
  {
    "objectID": "stanford/cs336/l2.html",
    "href": "stanford/cs336/l2.html",
    "title": "CS336 Lecture 2",
    "section": "",
    "text": "Types of resources:\nTotal FLOPS\nH100 80GB HBM memory"
  },
  {
    "objectID": "stanford/cs336/l2.html#memory-accounting",
    "href": "stanford/cs336/l2.html#memory-accounting",
    "title": "CS336 Lecture 2",
    "section": "Memory accounting",
    "text": "Memory accounting\nTensors are the basic building block for storing everything: parameters, gradients, optimizer state, data, activations, …\nHow much memory tensors take up ?\n\nfloat32 (fp32, single precision, full precision)\n\nDefault way of storing numbers (standard).\n\n\nMemory: number of values and data type of each value.\n32 bits = 4 bytes\nOne matrix in the feedforward layer of GPT-3: 12288*4, 12288 ~ 2.3GB\n\nfloat16 (fp16, half precision)\n\n16 bits = 2 bytes\nnot great for representing very small numbers (1e-8) or very big numbers.\n\nbfloat16 (brain floating point, developed by Google Brain).\n\nallocates more number to the exponent and more to the fraction\n\n\n\nimport torch\n\nfloat32_info = torch.finfo(torch.float32)\nfloat16_info = torch.finfo(torch.float16)\nbfloat16_info = torch.finfo(torch.bfloat16)\n\nprint(float32_info)\nprint(float16_info)\nprint(bfloat16_info)\n\nfinfo(resolution=1e-06, min=-3.40282e+38, max=3.40282e+38, eps=1.19209e-07, smallest_normal=1.17549e-38, tiny=1.17549e-38, dtype=float32)\nfinfo(resolution=0.001, min=-65504, max=65504, eps=0.000976562, smallest_normal=6.10352e-05, tiny=6.10352e-05, dtype=float16)\nfinfo(resolution=0.01, min=-3.38953e+38, max=3.38953e+38, eps=0.0078125, smallest_normal=1.17549e-38, tiny=1.17549e-38, dtype=bfloat16)\n\n\n\nfp8 (developed by NVIDIA)\n\nH100s support two variants of FP8\n\n\n\n\n\n\n\n\nImplication\n\n\n\n\ntraining with float32 works but requires lots of memory\ntraining with fp8, float16 and even bfloat16 is risky (you can get instability)\nmixed precision training"
  },
  {
    "objectID": "stanford/cs336/l2.html#compute-accounting",
    "href": "stanford/cs336/l2.html#compute-accounting",
    "title": "CS336 Lecture 2",
    "section": "Compute accounting",
    "text": "Compute accounting\nBy default, Tensors are stored on CPU. To take advantage of parallelisms of GPUs, we need to move them to GPU memory.\n\n\nTensors\n\nMathematical objects\nPyTorch tensors are pointed into allocated memory with metadata describing how to get to any element of the tensor.\n\n\nViews are free, copying take both computing and memory resource.\nFLOPs (floating-point operations per second)\n\nTraining GPT-3 took 3.14e23\nTraining GPT-4 took 2e25 (speculated)\n\nA100 peak performance: 312 teraFLOPs H100 peak performance: 1979 teraFLOPs\n\nModel FLOPs ultilization (MFU)\nactual FLOPs/promised FLOPs\nMFU &gt;= 0.5 is good."
  },
  {
    "objectID": "stanford/cs336/cs336.html",
    "href": "stanford/cs336/cs336.html",
    "title": "Stanford CS336 - Language Models from Scratch (Spring 2025) (in progress)",
    "section": "",
    "text": "Assignment 1: Building a Transformer LM\nLecture 1: Overview | Tokenization\nLecture 2: Resources\nLecture 3: Architectures and Hyperparameters\nLecture 4: Mixture of Experts (MOEs)\n\n\n\n\n Back to top"
  },
  {
    "objectID": "mlops/m1/linux.html",
    "href": "mlops/m1/linux.html",
    "title": "Linux",
    "section": "",
    "text": "Understand GNU / Linux\nAn alternative solution to AT&T Unix OS — which is not a free OS. GNU is an OS, often used with the Linux kernel. An OS kernel is a program allocating a computer resources (CPU, memory, etc.) to programs.\nThere are many variants of GNU/Linux, including:\n\nUbuntu\nKali Linux\nMacOS (yes, MacOS)\n\n\n\nLinux Directory Structure\n/\n├── home\n├── usr\n│   ├── bin\n│   ├── sbin\n│   └── local\n│       ├── bin      # executable programs such as ssh, telnet, kill, and less\n│       ├── sbin     # commands to boot the system such as reboot and shutdown\n│       └── local\n│           ├── bin  # user's executable programs\n│           └── sbin # system administration commands belong to users\n├── proc             # files for monitoring processes such as /proc/cpuinfo\n├── etc              # configuration files\n└── tmp              # temporary files\nFor example, if you use ls /usr/bin, you will see a list of executable programs on your computer:\n\n\n\nBasic Commands\n\n\n\n\n\n\n\nCommand\nDescription\n\n\n\n\ncd, pwd, ls\nNavigate and list files\n\n\ncp, mv, rm\nCopy, move, and remove files\n\n\ncat, head, tail\nDisplay the contents of a file\n\n\necho\nDisplay a message\n\n\nwget, curl\nDownload a file from the internet\n\n\ntar\nArchive and compress files\n\n\nfind .name \".log\" -size +1G\nSearch for files with name containing “.log” and size larger than 1G\n\n\ngrep\nSearch for patterns in files\n\n\nchmod, chown\nChange file permissions\n\n\nhistory, exit, shutdown, uptime\nOthers\n\n\n\n\n\n\n\n\n\nFun fact (directory)\n\n\n\nA directory is just a file containing the names of other files.\n\n\n\n\nShell, Console, and Terminal\n\nShell: a program that interprets the commands you type (bash, zsh, etc.).\nConsole: a physical device that allows you to interact with the computer. (keyboard, mouse, etc.)\nTerminal: a program that allows you to interact with the computer. (xterm, gnome-terminal, etc.)\n\n\n\nFurther reading\n\ngnu.org/gnu/linux-and-gnu\n\n\n\n\n\n Back to top"
  },
  {
    "objectID": "seminars.html",
    "href": "seminars.html",
    "title": "AIO Course",
    "section": "",
    "text": "Back to top"
  },
  {
    "objectID": "faq.html",
    "href": "faq.html",
    "title": "FAQ",
    "section": "",
    "text": "1. What is this ?\nThis is a website that serves as an archive of the AIO 2023 course that I have taken, including the lecture recordings, handouts, and solutions to the exercises. Initially, there is a Google Sheet but I want something more organized and flexible so this website is born after a looong time of procastination.\nAll of the content is not owned by me, I only collect them for educational purposes. Under no circumstances should one use this for commercial purposes. Other courses will be added in the future. In addition, you could also explore other resources that I have collected over time, mostly in the AI field and academic life (and recently in engineering).\n\n\n\n\n\n2. What do you use to build this website ?\nI use Quarto—static site generator that is very powerful and flexible—to build this website, deployed (for free) via GitHub Pages.\n\n\n\n\n\n3. What if I find any bugs ?\nPlease kindly let me know via email.\n\n\n\n\n\n4. How to contribute ?\nIf you have any resources that you think should be added to this website, please kindly let me know via email.\n\n\n\n\n Back to top"
  },
  {
    "objectID": "academia/paperChecklist.html#facets-of-your-work",
    "href": "academia/paperChecklist.html#facets-of-your-work",
    "title": "Paper Writing Guidelines",
    "section": "3 facets of your work",
    "text": "3 facets of your work\n\nExplain your method through its contrasts"
  },
  {
    "objectID": "academia/paperChecklist.html#paragraphs",
    "href": "academia/paperChecklist.html#paragraphs",
    "title": "Paper Writing Guidelines",
    "section": "Paragraphs",
    "text": "Paragraphs\n\n1 paragraph = 1 idea\n\nThe paragraphs should be short (3–5 sentences)\n\nThe paragraph should have flow:\n\nlogical flow of ideas — sequential in time, general to specific, logical arguments\n\nparallel sentence structures\n\ntransition words (if necessary, do not overrely on them). Use simple ones (BUT instead of Nevertheless…)\n\n\nPairs of ideas joined by “and”, “or”, or “but” should be written in parallel form.\n\nProofread, proofread, and proofread until you are sick and obsessed with your paper"
  },
  {
    "objectID": "academia/paperChecklist.html#clutters",
    "href": "academia/paperChecklist.html#clutters",
    "title": "Paper Writing Guidelines",
    "section": "Clutters",
    "text": "Clutters\n\nThe secret of good writing is to strip every sentence to its cleanest components.\n\nClutteredness is being precise, not counting the words.\n\nAvoid: successful solution, extremely flabbergasted\n\nAvoid: It is noticeable that…\n\nMost of the time you don’t need there is / there are\n\nCan be regarded as vs. is\n\nUse passive voice sparingly and deliberately\n\nUse strong verbs; avoid: take, have, get, to be\n\nAvoid noun-ifying verbs (e.g., compare → make a comparison)\n\nEliminate negatives\n\nAvoid overuse of adverbs (generally, mostly…)"
  },
  {
    "objectID": "academia/paperChecklist.html#general",
    "href": "academia/paperChecklist.html#general",
    "title": "Paper Writing Guidelines",
    "section": "General",
    "text": "General\n\nUse precise verbs\n\nQuotation marks: Use ``something’’ instead of “something”\n\nAvoid acronyms unless universally known\n\nKeep the tone balanced between high-level and technical\n\nImportant concepts: explain using text, figures, and math\n\nAvoid ending paragraphs with only 2–3 words on a line\n\nFill the allowed page limit (e.g., write 8 full pages if the limit is 8)\n\nUse parallelism in lists and structures"
  },
  {
    "objectID": "academia/paperChecklist.html#figures-1",
    "href": "academia/paperChecklist.html#figures-1",
    "title": "Paper Writing Guidelines",
    "section": "Figures",
    "text": "Figures\n\nhttps://agustinus.kristia.de/blog/plotting/"
  },
  {
    "objectID": "academia/paperChecklist.html#tables-1",
    "href": "academia/paperChecklist.html#tables-1",
    "title": "Paper Writing Guidelines",
    "section": "Tables",
    "text": "Tables\n\nhttps://nhigham.com/2019/11/19/better-latex-tables-with-booktabs/\n\nSmall Guide to Making Nice Tables"
  },
  {
    "objectID": "academia/paperChecklist.html#general-1",
    "href": "academia/paperChecklist.html#general-1",
    "title": "Paper Writing Guidelines",
    "section": "General",
    "text": "General\n\nWriting in the Sciences (Coursera)\n\nOn Writing Well\n\nThe Elements of Style\n\nThe Science of Scientific Writing (Gopen & Swan)"
  }
]