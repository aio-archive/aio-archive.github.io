[
  {
    "objectID": "engineer/index.html",
    "href": "engineer/index.html",
    "title": "Engineer",
    "section": "",
    "text": "Computer Networking Fundamentals\n\n\n\n\n Back to top"
  },
  {
    "objectID": "academia/index.html",
    "href": "academia/index.html",
    "title": "Academia",
    "section": "",
    "text": "Paper Writing Guidelines\n\n\n\n\n Back to top"
  },
  {
    "objectID": "index.html",
    "href": "index.html",
    "title": "AIO2023 Lectures",
    "section": "",
    "text": "Welcome to the AIO2023 course! This is an extensive one-year course that cover topics in AI, from the fundamental prerequisites to the most advanced and recent topics (like Generative AI, Diffusion models, CLIP, and Large language models). Note that each module is designed so that they are independent of each other. It’s best if you take study the material module in sequential order, however, feel free to jump to the module that you prefer once you are confident about your foundation knowledge. If you already learn the basics (eg. linear algebrea concepts, calculus, probability, and basic Python coding skills) and want to learn more about AI, feel free to start from Module 4 to discover more advanced topics.\nEssentially, there are fours main parts during the course: (1) The main lectures on Wednesday and Friday weekly; (2) the extra classes / TA lessons / Project tutorials on Saturday weekly; (3) the pre-lecture lecture on Tuesday weekly; and (4) the seminars on Sunday occcasionally.\nThe overall course schedule can be wrap up in the following table."
  },
  {
    "objectID": "index.html#introduction",
    "href": "index.html#introduction",
    "title": "AIO2023 Lectures",
    "section": "1.1 Introduction",
    "text": "1.1 Introduction\nWelcome to Module 1: Introduction to Python Programming of the AIO course. The goal of this module is to teach you basic Python programming skills, spanning from the very fundamental things like variables, functions, … to Object-oriented Programming using Python and as well as the basic data structures. There are in total 3 assignments for the main lessons. There will also be one lecture that acts as a supplemnentary before the main lecture."
  },
  {
    "objectID": "index.html#projects",
    "href": "index.html#projects",
    "title": "AIO2023 Lectures",
    "section": "1.2 Projects",
    "text": "1.2 Projects\nRegarding the projects, there are three projects where you will respectively learn how to use YOLOv8, an object detection model as well as how to use Python to manipulate and crawl data from a website. Finally, the last project is about developing simple applications using ChatGPT. In particular, the three projects are:\n\nObject Detection with YOLOv8\nData Manipulation and Crawling\nChatGPT Applications"
  },
  {
    "objectID": "index.html#competition-training",
    "href": "index.html#competition-training",
    "title": "AIO2023 Lectures",
    "section": "1.3 Competition Training",
    "text": "1.3 Competition Training\nAs for competition training, this module contains three lectures with the goal of teaching you the basic skills and knowledge you need before joining an AI competition, including visualizing data, knowledge about competition tasks and metrics, and design validation."
  },
  {
    "objectID": "index.html#extra-class",
    "href": "index.html#extra-class",
    "title": "AIO2023 Lectures",
    "section": "1.4 Extra class",
    "text": "1.4 Extra class\nThe central theme of the extra class for this module is about Algorithms and Complexity. In the age of AI, still, the knowledge about algorithms and their complexity including Big-O, Brute-force exhaustive, recursion, two pointer, and dynamic programming still plays an immensely important role."
  },
  {
    "objectID": "index.html#introduction-1",
    "href": "index.html#introduction-1",
    "title": "AIO2023 Lectures",
    "section": "2.1 Introduction",
    "text": "2.1 Introduction\nWelcome to the second module of the AIO2023 course. The central theme of this module is about Calculus and Linear Algebra, the two subfields of mathematics that I cannot emphasize enough the importance of them in understanding and developing machine learning models. My take on whether to learn math when you want to learn AI is that although it is often the case that many modern ready-made libraries (eg Numpy, Sklearn, Tensorflow, PyTorch, JAX, …) already support the underlying math, having a thorough understanding of the math behind the machine learning models makes you significantly more efficient in debugging , i.e. knowing what’s wrong with the model, or what particular model specification to apply for your specific problem."
  },
  {
    "objectID": "mlops/index.html",
    "href": "mlops/index.html",
    "title": "MLOps",
    "section": "",
    "text": "Module 1: Fundamental MLOps\n\n\n\n\n\n\nCourse Schedule\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nModule\nLevel\nTheory Topics\nPractice/Projects\n\n\n\n\nLinux\n1\n+ Understand GNU/Linux+ Commands+ Server administration\n+ Install Ubuntu 22.04.+ Server administration.\n\n\nPython\n2\n+ Compiled vs interpreted language+ Python common syntaxes+ Data structures in Python+ Function\n+ Configure Python and VSCode Environment for Basic Operations+ Create a linear regression model from scratch using NumPy.+ Explore classical machine learning models and techniques for feature engineering.+ Develop an end-to-end solution for predicting house prices using classical machine learning algorithms, leveraging tools like NumPy, Pandas, Matplotlib, and Seaborn.\n\n\nBash Scripting\n1\n+ Bash Scripting+ Makefile+ CronJob\n+ Use a bash script to calculate frequencies of flower species.+ Implement an “install_me” feature for Linux installations, allowing user confirmation.+ Develop a new line watcher to detect newly appended lines.+ Create your first crawler to periodically fetch a file from the internet.+ Build an automatic Ebay Deals crawler using Selenium, BeautifulSoup and cron job.\n\n\nWebAPI\n2\n+ What happens behind a Google Search?+ API architecture styles: REST API, gRPC, Websocket and Webhook+ Software Testing: functional and non-functional+ Solutions to improve performance of a REST API+ Fundamentals of FastAPI\n+ Build a secure Chatbot with FastAPI and ChatGPT.+ Develop a real-time OCR prediction application using WebSocket.\n\n\nFunctional Testing\n1\n+ Test-driven Development (TDD)+ Software Testing Methods and Levels+ Pytest and features: fixtures, markers, parametrize, mock, etc.+ Test coverage & tox\n+ Practice unit test and integration test our API application\n\n\nContainerization & Orchestration I\n2\n+ Differentiate between Virtual Machine (VM) and Docker+ Docker architecture & popular commands+ Docker containers debugging techniques+ Docker Compose & popular commands+ Dockerfile best practices to build fast, easy to maintain and minimal security risks\n+ Set up and perform smoke tests for a local experiment tracking platform using Docker Compose.+ Refine Docker Image Performance with Multistage Builds.+ Deliver an OCR Project: Train a Model, Build an API with Image Caching, Package with an Optimized Dockerfile, and Deploy with Docker Compose.\n\n\nContainerization & Orchestration II\n1\n+ k8s cluster’s architecture+ k8s objects and their lifecycles+ k8s popular commands+ k8s debugging techniques+ Cost optimization tips while operating a k8s cluster, and applications+ Package and manage a k8s application with Helm\n+ Practice k8s commands+ Deploy the OCR Application to Kubernetes with Helm+ Debug the newly created k8s application\n\n\nCloud Services\n1\n+ Cloud vs On Premises+ Key components of Cloud Computing Architecture+ IaaS, PaaS and SaaS+ Popular Google Cloud Platform (GCP) services for data and ML+ ML system architectures on GCP\n+ Create a free-tier GCP account.+ Deploy an OCR application on a VM on GCP.+ Deploy an OCR application on k8s on GCP using NGINX API Gateway.\n\n\nCI/CD\n1\n+ Fundamentals of Git and Github+ Introduction to CI/CD+ CI/CD in ML engineering+ Jenkins\n+ Deploy Jenkins locally using Docker Compose and perform smoke tests.+ Customize the Jenkins image to meet specific requirements.+ Implement a Full CI/CD Pipeline on Jenkins: Test, Build, and Deploy+ Improve CI/CD pipeline efficiency by implementing parallel runs.\n\n\nInfrastructure as Code (IaC)\n1\n+ The importance of IaC+ Ansible and popular commands for provisioning, managing configurations and application deployments on a VM+ Terraform and its syntaxes for provisioning cloud resources\n+ Deploy a VM on Google Compute Engine (GCE) using Ansible.+ Effortless OCR Deployment with Ansible.+ Set up a Google Kubernetes Engine (GKE) cluster with Terraform.\n\n\nObservable Systems\n2\n+ Three pillars of observability: logs, metrics and traces+ Observable system architecture+ OpenTelemetry Python SDK\n+ Set up a local monitoring platform including Prometheus, Grafana, ELK, Jaeger, and additional services.+ Implement automatic and manual tracing for our OCR API using Jaeger, and manage traces.+ Expose computing resources and custom metrics for our OCR API, visualize them on Grafana dashboards, and potentially create custom graphs.+ Standardize logging practices and manage logs using ELK stack.\n\n\n\n\n\n\n\nLesson 4: Multithreading, Multiprocessing, AsyncIO\n\n\n\n\n\n Back to top"
  },
  {
    "objectID": "stanford/cs336/moe.html#routing-function",
    "href": "stanford/cs336/moe.html#routing-function",
    "title": "Mixture of Experts",
    "section": "Routing function",
    "text": "Routing function\nRouting algorithms:\n\ntoken chooses expert: choose top-K experts for each token\nexpert chooses token: each expert chooses top-K tokens\nglobal routing via optimization: solve a global optimization problem to find the best routing.\n\nAlmost all MOEs boils down to choose “\\(\\text{top}-K\\)”.\n\n\nExpert sizes\nTraining objectives"
  },
  {
    "objectID": "stanford/cs336/moe.html#top-k-routing",
    "href": "stanford/cs336/moe.html#top-k-routing",
    "title": "Mixture of Experts",
    "section": "Top-K Routing",
    "text": "Top-K Routing\nMost papers do the old and classic top-k routing. How does this work?"
  },
  {
    "objectID": "stanford/cs336/moe.html#gating",
    "href": "stanford/cs336/moe.html#gating",
    "title": "Mixture of Experts",
    "section": "Gating",
    "text": "Gating\n\\[\n\\mathbf{h}_t^l = \\sum_{i=1}^{N} \\left( g_{i,t} \\text{FFN}_i \\left( \\mathbf{u}_t^l \\right) \\right) + \\mathbf{u}_t^l,\n\\]\n\\[\ng_{i,t} = \\begin{cases}\ns_{i,t}, & s_{i,t} \\in \\text{Topk}(\\{s_{j,t}|1 \\leq j \\leq N\\}, K), \\\\\n0, & \\text{otherwise},\n\\end{cases}\n\\]\n\\[\ns_{i,t} = \\text{Softmax}_i \\left( \\mathbf{u}_t^{lT} \\mathbf{e}_i^l \\right),\n\\]\nGates selected by a logistic regressor\nThis is the DeepSeek (V1-2) router (Grok, Qwen do this too)\nMixtral, DBRX, DeepSeek v3 softmaxes after the TopK"
  },
  {
    "objectID": "stanford/cs336/moe.html#rl-for-moes",
    "href": "stanford/cs336/moe.html#rl-for-moes",
    "title": "Mixture of Experts",
    "section": "RL for MoEs",
    "text": "RL for MoEs\nRL via REINFORCE, but not so much better."
  },
  {
    "objectID": "stanford/cs336/moe.html#stochastic-approximations",
    "href": "stanford/cs336/moe.html#stochastic-approximations",
    "title": "Mixture of Experts",
    "section": "Stochastic approximations",
    "text": "Stochastic approximations\n\\[G(x) = Softmax(KeepTopK(H(x), k))\\]\n\\[H(x)_i = (x \\cdot W_g)_i + StandardNormal() \\cdot Softplus((x \\cdot W_{noise})_i)\\]\n\\[KeepTopK(v, k)_i = \\begin{cases}\nv_i & \\text{if } v_i \\text{ is in the top } k \\text{ elements of } v. \\\\\n-\\infty & \\text{otherwise.}\n\\end{cases}\\]\nFrom Shazeer et al 2017 - routing decisions are stochastic with gaussian perturbations.\n\nThis naturally leads to experts that are a bit more robust.\nThe softmax means that the model learns how to rank K experts"
  },
  {
    "objectID": "stanford/cs336/l1.html",
    "href": "stanford/cs336/l1.html",
    "title": "CS336 Lecture 1 Overview",
    "section": "",
    "text": "Lecture Recording\nThree types of knowledge:\naccuracy = effciency \\(\\times\\) resources\nefficiency is way more important at larger scale (can not be wasteful).\n-&gt; What is the best model that we con build given a certain compute and budget ?"
  },
  {
    "objectID": "stanford/cs336/l1.html#before-2010s",
    "href": "stanford/cs336/l1.html#before-2010s",
    "title": "CS336 Lecture 1 Overview",
    "section": "Before 2010s",
    "text": "Before 2010s\n\nLanguage model to measure the entropy of English (Shannon, 1950)\nn-gram language model (for machine translation, speech recognition)"
  },
  {
    "objectID": "stanford/cs336/l1.html#neural-ingredients-2010s",
    "href": "stanford/cs336/l1.html#neural-ingredients-2010s",
    "title": "CS336 Lecture 1 Overview",
    "section": "Neural ingredients (2010s)",
    "text": "Neural ingredients (2010s)\n\nFirst language model (Bengio, 2003)\nSequence-to-sequence modeling\nAttention mechanism (Bahdanau, 2014)\nTransformer architecture\nMixture of Experts\nModel parallelism"
  },
  {
    "objectID": "stanford/cs336/l1.html#early-foundation-models",
    "href": "stanford/cs336/l1.html#early-foundation-models",
    "title": "CS336 Lecture 1 Overview",
    "section": "Early foundation models",
    "text": "Early foundation models\n\nELMO, BERT, Google’s T5"
  },
  {
    "objectID": "stanford/cs336/l1.html#embracing-scaling",
    "href": "stanford/cs336/l1.html#embracing-scaling",
    "title": "CS336 Lecture 1 Overview",
    "section": "Embracing scaling",
    "text": "Embracing scaling\n\nOpenAI’s GPT-2 (1.5B), GPT-3, PaLM, Chinchilla.\n\nthese are all closed models."
  },
  {
    "objectID": "stanford/cs336/l1.html#open-models",
    "href": "stanford/cs336/l1.html#open-models",
    "title": "CS336 Lecture 1 Overview",
    "section": "Open models",
    "text": "Open models\nBLOOM, Llama, Qwen, OLMo\n\nClosed model\nOpen-weight models (DeepSeek)\nOpen-source models (OLMo)"
  },
  {
    "objectID": "stanford/cs336/l1.html#kernels",
    "href": "stanford/cs336/l1.html#kernels",
    "title": "CS336 Lecture 1 Overview",
    "section": "Kernels",
    "text": "Kernels\n\n\nData movement between GPUs is even slower, but same ‘minimize data movement’ principle holds Use collective operations (e.g., gather, reduce, all-reduce) Shard (parameters, activations, gradients, optimizer states) across GPUs How to split computation: {data,tensor, pipeline,sequence) parallelism"
  },
  {
    "objectID": "stanford/cs336/l1.html#inference",
    "href": "stanford/cs336/l1.html#inference",
    "title": "CS336 Lecture 1 Overview",
    "section": "Inference",
    "text": "Inference\nGoals: generate tokens given a prompt. Inference is also needed for reinforment learning, test-time compute, evaluation, …\n\nPrefill (similar to training): tokens are given, can process all at once (compute-bound) Decode: need to generate one token at a time (memory-bound) Methods to speed up decoding: • Use cheaper model (via model pruning, quantization, distillation) • Speculative decoding: use a cheaper “draft” model to generate multiple tokens, then use the full model to score in parallel (exact decoding!) • Systems optimizations: KV caching, batching"
  },
  {
    "objectID": "stanford/cs336/l3.html#gated-activations-glu",
    "href": "stanford/cs336/l3.html#gated-activations-glu",
    "title": "CS336 Lecture 3: Architectures and Hyperparameters",
    "section": "Gated activations (*GLU)",
    "text": "Gated activations (*GLU)\nGLUs modify the ‘first part’ of a FF layer\n\\[\nFF(x) = \\max(0, xW_1) W_2\n\\]\nInstead of a linear + ReLU, augment the above with an (entrywise) linear term\n\\[\n\\max(0, xW_1) \\rightarrow \\max(0, xW_1) \\otimes (xV)\n\\]\nThis gives the gated variant (ReGLU) – note that we have an extra parameter (V)\n\\[\nFF_{\\mathrm{ReGLU}}(x) = (\\max(0, xW_1) \\otimes xV) W_2\n\\]"
  },
  {
    "objectID": "stanford/cs336/l3.html#gated-variants-of-standard-ff-layers",
    "href": "stanford/cs336/l3.html#gated-variants-of-standard-ff-layers",
    "title": "CS336 Lecture 3: Architectures and Hyperparameters",
    "section": "Gated variants of standard FF layers",
    "text": "Gated variants of standard FF layers\n\nGeGLU\n\\[\n\\mathrm{FFN}_{\\mathrm{GeGLU}}(x, W, V, W_2) = (\\mathrm{GELU}(xW) \\otimes xV) W_2\n\\]\nNotable models:\nT5 v1.1, mT5, LaMDA, Phi3, Gemma 2, Gemma 3\n\n\n\nSwiGLU\n(swish is \\(x * \\mathrm{sigmoid}(x)\\))\n\\[\n\\mathrm{FFN}_{\\mathrm{SwiGLU}}(x, W, V, W_2) = (\\mathrm{Swish}_1(xW) \\otimes xV) W_2\n\\]\nNotable models:\nLLaMa 1/2/3, PaLM, Mistral, OLMo, most models post 2023\n\nNote: Gated models use smaller dimensions for the \\(d_{ff}\\) by \\(2/3\\)"
  },
  {
    "objectID": "stanford/cs336/l3.html#hyperparameters",
    "href": "stanford/cs336/l3.html#hyperparameters",
    "title": "CS336 Lecture 3: Architectures and Hyperparameters",
    "section": "Hyperparameters",
    "text": "Hyperparameters\nTransformer hyperparameter questions you might have had in 224n..\n\nHow much bigger should the feedforward size be compared to hidden size?\nHow many heads, and should num_heads always divide hidden size?\nWhat should my vocab size be?\n\nAnd other model setting questions\n\nDo people even regularize these huge LMs?\nHow do people scale these models – very deep or very wide?"
  },
  {
    "objectID": "stanford/cs336/l3.html#surprising-consensus-hyperparameter-1",
    "href": "stanford/cs336/l3.html#surprising-consensus-hyperparameter-1",
    "title": "CS336 Lecture 3: Architectures and Hyperparameters",
    "section": "Surprising (?) consensus hyperparameter 1",
    "text": "Surprising (?) consensus hyperparameter 1\nFeedforward – model dimension ratio.\n\\[\n\\mathrm{FFN}(x) = \\max(0, xW_1 + b_1) W_2 + b_2\n\\]\nThere are two dimensions that are relevant – the feedforward dim (\\(d_{ff}\\)) and model dim (\\(d_{model}\\)). What should their relationship be?\n\\[\nd_{ff} = 4\\, d_{model}\n\\]\nThis is almost always true. There’s just a few exceptions."
  },
  {
    "objectID": "stanford/cs336/l3.html#surprising-consensus-hyperparameter-2",
    "href": "stanford/cs336/l3.html#surprising-consensus-hyperparameter-2",
    "title": "CS336 Lecture 3: Architectures and Hyperparameters",
    "section": "Surprising (?) consensus hyperparameter 2",
    "text": "Surprising (?) consensus hyperparameter 2\nHead-dim \\(\\times\\) num-heads to model-dim ratio. As a reminder, slide from 224n.\n\nMulti-head self-attention is computationally efficient\n\nEven though we compute \\(h\\) many attention heads, it’s not really more costly.\n\nWe compute \\(XQ \\in \\mathbb{R}^{n \\times d}\\), and then reshape to \\(\\mathbb{R}^{n \\times h \\times d/h}\\). (Likewise for \\(XK\\), \\(XV\\).)\nThen we transpose to \\(\\mathbb{R}^{h \\times n \\times d/h}\\); now the head axis is like a batch axis.\nAlmost everything else is identical, and the matrices are the same sizes.\n\n\n\nThis doesn’t have to be true: we can have head-dimensions \\(&gt;\\) model-dim / num-heads.\nBut most models do follow this guideline."
  },
  {
    "objectID": "stanford/cs336/assignments/a1/a1.html",
    "href": "stanford/cs336/assignments/a1/a1.html",
    "title": "CS336 Assignment 1 - Building a Transformer LM",
    "section": "",
    "text": "PDF\n\n\n\n Back to top"
  },
  {
    "objectID": "blogs/index.html",
    "href": "blogs/index.html",
    "title": "Blog",
    "section": "",
    "text": "Modality Gap\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nTools I use for (AI) research\n\n\n\n\n\n\n\n\n\nMay 9, 2025\n\n\n\n\n\n\nNo matching items\n Back to top"
  },
  {
    "objectID": "blogs/tools.html",
    "href": "blogs/tools.html",
    "title": "Tools I use for (AI) research",
    "section": "",
    "text": "Warning\n\n\n\nThis is not a typical SEO-optimized blog post that you’d find that suggest using obvious tools like Google Scholar, ResearchGate, EndNote, Zotero, etc. (Though those are great too!) All of these tools listed here tools that I actually use, and are free.\n\n\n\nPaperLib\nIf you have ever written research papers and have to cite sources, one of the biggest problems you might encounter is that you have to manually search for the right citation. Google Scholar is sometimes provide the citation for the pre-print version of the paper, not the published version (with actual conference and journals). For example, if you search for the paper “Lxmert: Learning cross-modality encoder representations from transformers” on Google Scholar, you will get the following BibTeX entry:\n@article{tan2019lxmert,\n  title={Lxmert: Learning cross-modality encoder representations from transformers},\n  author={Tan, Hao and Bansal, Mohit},\n  journal={arXiv preprint arXiv:1908.07490},\n  year={2019}\n}\nwhereas what I want is the version of the paper that is published in EMNLP 2019. PaperLib’s fuzzily scrape feature allows me do this. Here’s the citation for the published version:\n@inproceedings{tan2019lxmert,\n    author = {Tan, Hao Hao and Bansal, Mohit},\n    booktitle = {EMNLP},\n    year = {2019},\n    pages = {5099--5110},\n    organization = {},\n    title = {LXMERT: Learning {Cross}-{Modality} {Encoder} {Representations} from {Transformers}},\n    volume = {},\n}\nNotice how it subtlely but every word in the brackers {}. This is to ensure that the citation is properly capitalized when you used the BibTex entry in your LaTeX document.\nThe second feature that I love about PaperLib is that it has an extension that allows me to directly import the paper. When I was reading a paper on my browser, I can just right click and import the paper into PaperLib (with the proper tag if you like). There are also other extensions built for PaperLib that you could explore by yourself.\n\nAnother feature I find useful is you can abbreviate the publication venue. In machine learning, top-tier conferences like Advances in Neural Information Processing Systems (NeurIPS) and International Conference on Learning Representations (ICLR) are abbreviated as NeurIPS and ICLR respectively. Sometime these can be inconsistent, like “Advances in Neural Information Processing Systems” vs. “Conference on Neural Information Processing Systems”. PaperLib allows you to abbreviate the venue name so that the BibTeX entry is more consistent. Here’s the list that I use:\n\nConference on Computer Vision and Pattern Recognition -&gt; CVPR\nInternational Conference on Computer Vision -&gt; ICCV\nEuropean Conference on Computer Vision -&gt; ECCV\nConference on Neural Information Processing Systems -&gt; NeurIPS\nInternational Conference on Machine Learning -&gt; ICML\nInternational Conference on Learning Representations -&gt; ICLR\nAAAI Conference on Artificial Intelligence -&gt; AAAI\nInternational Joint Conference on Artificial Intelligence -&gt; IJCAI\nAnnual Meeting of the Association for Computational Linguistics -&gt; ACL\nConference on Empirical Methods in Natural Language Processing -&gt; EMNLP\nNorth American Chapter of the ACL -&gt; NAACL\n\nYou can add yours in PaperLib -&gt; Settings -&gt; Export -&gt; Publication Abbreviation.\nThe best of all ? PaperLib is free!\n\n\nPaper Visualizer\nIf you have ever drown into research paper with too many details (of architecture, training, etc.) and want to get a quick overview, Paper Visualizer is the tool for you. It helps create a big picture using diagrams. I started using it for roughly two weeks and still exploring the other available features, but it’s already a game changer for me.\nHere’s a diagram that Paper Visualizer creates for the “Attention is All You Need” paper:\n\nThis is really useful because it creates immediate hypothesis, how the authors tested it, and what the results are, etc.\nZooming in, you get an AI features for each node to further explain the concepts:\n\nAnd you can also embed it into your website to easily share it with others, like I did below for the MERU paper:\n\n\nThe best part ? Paper Visualizer is free (for now)!\n\n\nAi2PaperFinder\nJust as the name suggest, this tool, developed by Ai2, is an AI-powered tool that allows you to find the right paper for your research. It’s a great tool for getting a quick overview of the paper, and it’s free! Just enter the keyword and it will give you a list of papers that are related to your query (with different levels of Relevance).\n\n\n\nGoogle Scholar PDF Reader\nGoogle Scholar PDF Reader is an extension for reading PDF right on your browser. The selling-point of this from other PDF viewer is that it allows me to preview the citation without having to scroll all the way to the bottom of the page and then re-scroll back to where I was.\n\nIt does come with an AI-generated outline, but I don’t use it much.\n\n\n\n\n Back to top"
  },
  {
    "objectID": "blogs/modality-gap.html#what-is-the-modality-gap",
    "href": "blogs/modality-gap.html#what-is-the-modality-gap",
    "title": "Modality Gap in CLIP Embedding Space",
    "section": "What is the Modality Gap?",
    "text": "What is the Modality Gap?\nThe modality gap refers to a geometric phenomenon in the representation space of multimodal models where embeddings from different modalities (e.g., images and text) occupy distinct, separated regions rather than being fully integrated in the shared representation space (Liang et al. 2022). This separation creates a clear geometric distance between different modalities, despite these models being explicitly trained to align cross-modal representations. For example, in CLIP (Contrastive Language-Image Pre-training), image and text embeddings are systematically located in completely separate regions of the embedding space, even though they are supposed to represent semantically similar concepts[2]."
  },
  {
    "objectID": "blogs/modality-gap.html#causes-of-the-modality-gap",
    "href": "blogs/modality-gap.html#causes-of-the-modality-gap",
    "title": "Modality Gap in CLIP Embedding Space",
    "section": "Causes of the Modality Gap",
    "text": "Causes of the Modality Gap\nResearch has identified several key factors that contribute to the emergence and persistence of the modality gap:\n\n1. Neural Network Architecture and Initialization\nThe general inductive bias of deep neural architectures creates a “cone effect” where the effective embedding space is restricted to a narrow cone for both pre-trained models and models with random weights[7][15]. Different random initializations create different embedding cones, and since multimodal models typically use two separate encoders, the representations of the two modalities are already clearly apart at initialization[1]. Theoretical analysis shows that each neural network layer shrinks the angle between embedding vectors with high probability, creating narrower cones in deeper architectures[7].\n\n\n2. Contrastive Learning Dynamics\nThe contrastive learning objective commonly used in multimodal models preserves or even enlarges the gap during training[1][15]. Research has demonstrated that:\n\nThe learnable temperature parameter in the contrastive loss function significantly influences the modality gap persistence[2][4]\nMismatched data pairs, especially at early training stages, cause the modality gap to enlarge before potentially beginning to close[4][10]\nGradient flow analysis reveals that the modality gap diminishes at an extremely slow rate (O(1/√t) in training time t), explaining why it persists in trained models[10]\n\n\n\n3. Temperature Parameter Effects\nSmall temperature settings in fixed temperature mode and almost invariably under learned temperature mode settings lead to consistent emergence of modality gaps, regardless of the initial temperature value[12]. The choice of temperature parameterization affects the rate at which the gap closes or widens[4]."
  },
  {
    "objectID": "blogs/modality-gap.html#mitigation-strategies",
    "href": "blogs/modality-gap.html#mitigation-strategies",
    "title": "Modality Gap in CLIP Embedding Space",
    "section": "Mitigation Strategies",
    "text": "Mitigation Strategies\nResearchers have proposed various approaches to mitigate the modality gap:\n\nObjective and Loss Function Modifications\n\nTemperature Control Strategies:\n\nTemperature Scheduling (TS): Enforcing a linearly increasing schedule for temperature during training[4]\nTemperature Reparameterization (TR): Replacing the original parameterization with alternatives that yield a higher gap closing rate[4]\nSmaller Learning Rate of Temperature (SLRT): Using a smaller learning rate for the temperature parameter compared to other learnable parameters[4]\nFixed on Large Temperature (FLT): Fixing the temperature at a high value throughout training[4]\n\nRemoving Repulsive Structure: Modifying the contrastive loss by removing the repulsive structure (which pushes negative examples away) helps close the modality gap[10]. This approach retains only the mechanism of pulling positive samples together.\n\n\n\nModality Swapping Approaches\n\nHard Swapping Between Modalities (HS): Randomly selecting image-text pairs and exchanging their features in the shared feature space[4]\nSoft Swapping Between Modalities (SS): Mixing the features of image-text pairs to create new pairs[4]\n\nThese approaches prevent the two modalities from remaining segregated into parallel planes, thereby reducing overall repulsion between them[4].\n\n\nRegularization Techniques\nStudies in speech translation have found that regularization plays a more important role than well-designed modality adaptation methods in addressing the modality gap[3][9][11]. Regularization helps prevent overfitting, which can exacerbate the modality gap, especially when transferring from high-resource to low-resource tasks[11].\n\n\nRepresentation Space Engineering\n\nSemantic Enhancement: Improving both inter-modality semantic consistency and intra-modality semantic completion[10]\nInter-MCR Alignment: Aligning semantically-enhanced embeddings across different multimodal contrastive representation (MCR) spaces[10]\nIntra-MCR Alignment: Realigning semantically similar embeddings across modalities within each MCR space to alleviate the modality gap[10]"
  },
  {
    "objectID": "blogs/modality-gap.html#relationship-to-downstream-task-performance",
    "href": "blogs/modality-gap.html#relationship-to-downstream-task-performance",
    "title": "Modality Gap in CLIP Embedding Space",
    "section": "Relationship to Downstream Task Performance",
    "text": "Relationship to Downstream Task Performance\nThe relationship between modality gap and downstream task performance is complex and task-dependent:\n\nPositive Impacts\n\nReducing the modality gap shows consistent improvement in image-text retrieval accuracy[4][10]\nSome research indicates that varying the modality gap distance can improve zero-shot classification performance and fairness[1][7]\n\n\n\nNuanced Effects\n\nReducing the gap does not consistently improve all downstream task performance, suggesting its role may be more nuanced than previously understood[12]\nWhile smaller modality gaps clearly lead to higher retrieval accuracy, they may not significantly affect zero-shot and linear probe classification accuracy[10]\nFeature space uniformity appears more important than modality gap reduction for visual classification tasks, including zero-shot and linear probing[10]\nFor challenging vision-language question-answering tasks, neither modality gap nor uniformity shows a strong correlation with performance[10]\n\nThis evidence suggests that the modality gap might be a geometric by-product of learning methods rather than a critical determinant of representation quality across all tasks[12]."
  },
  {
    "objectID": "blogs/modality-gap.html#open-research-questions-and-future-directions",
    "href": "blogs/modality-gap.html#open-research-questions-and-future-directions",
    "title": "Modality Gap in CLIP Embedding Space",
    "section": "Open Research Questions and Future Directions",
    "text": "Open Research Questions and Future Directions\nSeveral important research gaps remain in understanding and addressing the modality gap:\n\nTask-Specific Impacts: Further research is needed to understand why modality gap reduction benefits certain tasks (like retrieval) more than others (like classification)[10][12]\nRelationship to Other Representation Properties: The interplay between modality gap and other properties of the representation space, such as uniformity, remains underexplored[10]\nDataset Characteristics: The role of dataset characteristics in influencing the modality gap needs deeper investigation, as individual dataset characteristics significantly influence the gap’s manifestation[12]\nTheoretical Understanding: Extending gradient flow analysis to study the difficulty of closing the gap with varying levels of shared information between modalities by modeling data distributions[4]\nApplication to Finetuning: Applying insights to finetuning scenarios where domain differences between pretraining and finetuning data need to be considered[4]\nCompressibility of Dynamics: Investigating the compressibility of the Riemannian dynamics trajectory in multimodal learning to enhance efficiency and performance[4]\nFairness and Bias: Understanding how modality gap relates to fairness concerns and modality bias in recommendation systems and other applications[5]"
  },
  {
    "objectID": "blogs/modality-gap.html#conclusion",
    "href": "blogs/modality-gap.html#conclusion",
    "title": "Modality Gap in CLIP Embedding Space",
    "section": "Conclusion",
    "text": "Conclusion\nThe modality gap represents a fundamental challenge in multimodal representation learning. While significant progress has been made in understanding its causes and developing mitigation strategies, the relationship between modality gap and model performance remains complex and task-dependent. Future research should focus on a more nuanced understanding of when and how to address the modality gap based on specific applications and desired outcomes, rather than assuming its reduction is universally beneficial.\nThe field would benefit from a reevaluation of the modality gap’s significance in multimodal contrastive learning, with greater emphasis on dataset characteristics, contrastive learning methodology, and task-specific requirements[12]. As multimodal systems continue to advance, addressing these open questions will be crucial for developing more effective and robust models."
  },
  {
    "objectID": "stanford/cs336/tokenization.html",
    "href": "stanford/cs336/tokenization.html",
    "title": "Tokenization",
    "section": "",
    "text": "# Basic chr() usage\nassert chr(97) == \"a\"\nassert chr(127757) == \"🌍\"  # 127757 is actually the Earth Globe Europe-Africa emoji\n\n# Build a simple Character Tokenizer and test round-trip encoding/decoding\ntokenizer = CharacterTokenizer()\n\nstring = \"Hello, 🌍! 1rt7!\"  # @inspect string\nindices = tokenizer.encode(string)  # @inspect indices\nreconstructed_string = tokenizer.decode(indices)  # @inspect reconstructed_string\n\nassert string == reconstructed_string\n\n# Unicode has approximately 150K characters\n# [Wikipedia: List of Unicode characters]\n\n# Vocabulary size estimation\nvocabulary_size = max(indices) + 1  # This is a lower bound\n# @inspect vocabulary_size\n\n# Problems:\n# Problem 1: This is a very large vocabulary.\n# Problem 2: Many characters are quite rare (e.g., 🌍), making vocabulary inefficient.\n\n# Compression ratio\ncompression_ratio = get_compression_ratio(string, indices)  # @inspect compression_ratio\n\nByte-based tokenization\nUnicode strings can be represented as a sequence of bytes (integers between 0 and 255).\nThe most common Unicode encoding is UTF-8.\n# Some Unicode characters are represented by one byte:\nassert bytes(\"a\", encoding=\"utf-8\") == b\"a\"\n\n# Others take multiple bytes:\nassert bytes(\"🌍\", encoding=\"utf-8\") == b\"\\xf0\\x9f\\x8c\\x8d\"\n\n# Now let's build a Byte-based Tokenizer and test round-trip encoding/decoding\ntokenizer = ByteTokenizer()\n\nstring = \"Hello, 🌍！你好！\"  # @inspect string\nindices = tokenizer.encode(string)  # @inspect indices\nreconstructed_string = tokenizer.decode(indices)  # @inspect reconstructed_string\n\nassert string == reconstructed_string\n\n\nWord-based tokenization\nByte Pair Encoding (BPE) [Wikipedia] The BPE algorithm was introduced by Philip Gage in 1994 for data compression. [article] It was adapted to NLP for neural machine translation. [Sennrich+ 2015] (Previously, papers had been using word-based tokenization.) BPE was then used by GPT-2. [Radford+ 2019] Basic idea: train the tokenizer on raw text to automatically determine the vocabulary. Intuition: common sequences of characters are represented by a single token, rare sequences are represented by many tokens. The GPT-2 paper used word-based tokenizatich to break up the text into inital segments and run the original BPE algorithm on each segment. Sketcl n byte as a l token, and successively merge the most common pair of adjacent tokens.\nfrom collections import defaultdict\nfrom typing import Dict, Tuple\n\nclass BETokenizerParams:\n    def __init__(self, merges: Dict[Tuple[int, int], int], vocab: Dict[int, bytes]):\n        self.merges = merges  # pair -&gt; new index\n        self.vocab = vocab    # index -&gt; bytes\n\ndef train_bpe(string: str, num_merges: int) -&gt; BETokenizerParams:\n    # @inspect string, @inspect num_merges\n    \n    # Start with the list of bytes of string\n    indices = list(map(int, string.encode(\"utf-8\")))  # @inspect indices\n    \n    merges: Dict[Tuple[int, int], int] = {}  # (index1, index2) -&gt; merged index\n    vocab: Dict[int, bytes] = {x: bytes([x]) for x in range(256)}  # index -&gt; bytes\n\n    for i in range(num_merges):\n        # Count the number of occurrences of each pair of tokens\n        counts = defaultdict(int)\n        for index1, index2 in zip(indices, indices[1:]):  # For each adjacent pair\n            counts[(index1, index2)] += 1  # @inspect counts\n\n        # Find the most common pair\n        if not counts:\n            break  # No more pairs to merge\n\n        pair = max(counts, key=counts.get)  # @inspect pair\n        index1, index2 = pair\n\n        # Create a new index for the merged pair\n        new_index = max(vocab.keys()) + 1\n        vocab[new_index] = vocab[index1] + vocab[index2]\n        merges[(index1, index2)] = new_index\n\n        # Update the sequence (merge occurrences of the pair)\n        new_indices = []\n        skip = False\n        for j in range(len(indices)):\n            if skip:\n                skip = False\n                continue\n            if j &lt; len(indices) - 1 and (indices[j], indices[j+1]) == pair:\n                new_indices.append(new_index)\n                skip = True  # Skip next index because it's merged\n            else:\n                new_indices.append(indices[j])\n        indices = new_indices  # Update for next iteration\n\n    return BETokenizerParams(merges=merges, vocab=vocab)\n\n\n\n\n Back to top"
  },
  {
    "objectID": "stanford/cs336/l2.html",
    "href": "stanford/cs336/l2.html",
    "title": "CS336 Lecture 2",
    "section": "",
    "text": "Types of resources:\nTotal FLOPS\nH100 80GB HBM memory"
  },
  {
    "objectID": "stanford/cs336/l2.html#memory-accounting",
    "href": "stanford/cs336/l2.html#memory-accounting",
    "title": "CS336 Lecture 2",
    "section": "Memory accounting",
    "text": "Memory accounting\nTensors are the basic building block for storing everything: parameters, gradients, optimizer state, data, activations, …\nHow much memory tensors take up ?\n\nfloat32 (fp32, single precision, full precision)\n\nDefault way of storing numbers (standard).\n\n\nMemory: number of values and data type of each value.\n32 bits = 4 bytes\nOne matrix in the feedforward layer of GPT-3: 12288*4, 12288 ~ 2.3GB\n\nfloat16 (fp16, half precision)\n\n16 bits = 2 bytes\nnot great for representing very small numbers (1e-8) or very big numbers.\n\nbfloat16 (brain floating point, developed by Google Brain).\n\nallocates more number to the exponent and more to the fraction\n\n\n\nimport torch\n\nfloat32_info = torch.finfo(torch.float32)\nfloat16_info = torch.finfo(torch.float16)\nbfloat16_info = torch.finfo(torch.bfloat16)\n\nprint(float32_info)\nprint(float16_info)\nprint(bfloat16_info)\n\nfinfo(resolution=1e-06, min=-3.40282e+38, max=3.40282e+38, eps=1.19209e-07, smallest_normal=1.17549e-38, tiny=1.17549e-38, dtype=float32)\nfinfo(resolution=0.001, min=-65504, max=65504, eps=0.000976562, smallest_normal=6.10352e-05, tiny=6.10352e-05, dtype=float16)\nfinfo(resolution=0.01, min=-3.38953e+38, max=3.38953e+38, eps=0.0078125, smallest_normal=1.17549e-38, tiny=1.17549e-38, dtype=bfloat16)\n\n\n\nfp8 (developed by NVIDIA)\n\nH100s support two variants of FP8\n\n\n\n\n\n\n\n\nImplication\n\n\n\n\ntraining with float32 works but requires lots of memory\ntraining with fp8, float16 and even bfloat16 is risky (you can get instability)\nmixed precision training"
  },
  {
    "objectID": "stanford/cs336/l2.html#compute-accounting",
    "href": "stanford/cs336/l2.html#compute-accounting",
    "title": "CS336 Lecture 2",
    "section": "Compute accounting",
    "text": "Compute accounting\nBy default, Tensors are stored on CPU. To take advantage of parallelisms of GPUs, we need to move them to GPU memory.\n\n\nTensors\n\nMathematical objects\nPyTorch tensors are pointed into allocated memory with metadata describing how to get to any element of the tensor.\n\n\nViews are free, copying take both computing and memory resource.\nFLOPs (floating-point operations per second)\n\nTraining GPT-3 took 3.14e23\nTraining GPT-4 took 2e25 (speculated)\n\nA100 peak performance: 312 teraFLOPs H100 peak performance: 1979 teraFLOPs\n\nModel FLOPs ultilization (MFU)\nactual FLOPs/promised FLOPs\nMFU &gt;= 0.5 is good."
  },
  {
    "objectID": "stanford/cs336/cs336.html",
    "href": "stanford/cs336/cs336.html",
    "title": "Stanford CS336 - Language Models from Scratch (Spring 2025) (in progress)",
    "section": "",
    "text": "Assignment 1: Building a Transformer LM\nLecture 1: Overview | Tokenization\nLecture 2: Resources\nLecture 3: Architectures and Hyperparameters\nLecture 4: Mixture of Experts (MOEs)\n\n\n\n\n Back to top"
  },
  {
    "objectID": "mlops/m1/linux.html",
    "href": "mlops/m1/linux.html",
    "title": "Linux",
    "section": "",
    "text": "Understand GNU / Linux\nAn alternative solution to AT&T Unix OS — which is not a free OS. GNU is an OS, often used with the Linux kernel. An OS kernel is a program allocating a computer resources (CPU, memory, etc.) to programs.\nThere are many variants of GNU/Linux, including:\n\nUbuntu\nKali Linux\nMacOS (yes, MacOS)\n\n\n\nLinux Directory Structure\n/\n├── home\n├── usr\n│   ├── bin\n│   ├── sbin\n│   └── local\n│       ├── bin      # executable programs such as ssh, telnet, kill, and less\n│       ├── sbin     # commands to boot the system such as reboot and shutdown\n│       └── local\n│           ├── bin  # user's executable programs\n│           └── sbin # system administration commands belong to users\n├── proc             # files for monitoring processes such as /proc/cpuinfo\n├── etc              # configuration files\n└── tmp              # temporary files\nFor example, if you use ls /usr/bin, you will see a list of executable programs on your computer:\n\n\n\nBasic Commands\n\n\n\n\n\n\n\nCommand\nDescription\n\n\n\n\ncd, pwd, ls\nNavigate and list files\n\n\ncp, mv, rm\nCopy, move, and remove files\n\n\ncat, head, tail\nDisplay the contents of a file\n\n\necho\nDisplay a message\n\n\nwget, curl\nDownload a file from the internet\n\n\ntar\nArchive and compress files\n\n\nfind .name \".log\" -size +1G\nSearch for files with name containing “.log” and size larger than 1G\n\n\ngrep\nSearch for patterns in files\n\n\nchmod, chown\nChange file permissions\n\n\nhistory, exit, shutdown, uptime\nOthers\n\n\n\n\n\n\n\n\n\nFun fact (directory)\n\n\n\nA directory is just a file containing the names of other files.\n\n\n\n\nShell, Console, and Terminal\n\nShell: a program that interprets the commands you type (bash, zsh, etc.).\nConsole: a physical device that allows you to interact with the computer. (keyboard, mouse, etc.)\nTerminal: a program that allows you to interact with the computer. (xterm, gnome-terminal, etc.)\n\n\n\nFurther reading\n\ngnu.org/gnu/linux-and-gnu\n\n\n\n\n\n Back to top"
  },
  {
    "objectID": "seminars.html",
    "href": "seminars.html",
    "title": "AIO Course",
    "section": "",
    "text": "Back to top"
  },
  {
    "objectID": "faq.html",
    "href": "faq.html",
    "title": "FAQ",
    "section": "",
    "text": "1. What is this ?\nThis is a website that serves as an archive of the AIO 2023 course that I have taken, including the lecture recordings, handouts, and solutions to the exercises. Initially, there is a Google Sheet but I want something more organized and flexible so this website is born after a looong time of procastination.\nAll of the content is not owned by me, I only collect them for educational purposes. Under no circumstances should one use this for commercial purposes. Other courses will be added in the future. In addition, you could also explore other resources that I have collected over time, mostly in the AI field and academic life (and recently in engineering).\n\n\n\n\n\n2. What do you use to build this website ?\nI use Quarto—static site generator that is very powerful and flexible—to build this website, deployed (for free) via GitHub Pages.\n\n\n\n\n\n3. What if I find any bugs ?\nPlease kindly let me know via email.\n\n\n\n\n\n4. How to contribute ?\nIf you have any resources that you think should be added to this website, please kindly let me know via email.\n\n\n\n\n Back to top"
  },
  {
    "objectID": "academia/paperChecklist.html#facets-of-your-work",
    "href": "academia/paperChecklist.html#facets-of-your-work",
    "title": "Paper Writing Guidelines",
    "section": "3 facets of your work",
    "text": "3 facets of your work\n\nExplain your method through its contrasts"
  },
  {
    "objectID": "academia/paperChecklist.html#paragraphs",
    "href": "academia/paperChecklist.html#paragraphs",
    "title": "Paper Writing Guidelines",
    "section": "Paragraphs",
    "text": "Paragraphs\n\n1 paragraph = 1 idea\n\nThe paragraphs should be short (3–5 sentences)\n\nThe paragraph should have flow:\n\nlogical flow of ideas — sequential in time, general to specific, logical arguments\n\nparallel sentence structures\n\ntransition words (if necessary, do not overrely on them). Use simple ones (BUT instead of Nevertheless…)\n\n\nPairs of ideas joined by “and”, “or”, or “but” should be written in parallel form.\n\nProofread, proofread, and proofread until you are sick and obsessed with your paper"
  },
  {
    "objectID": "academia/paperChecklist.html#clutters",
    "href": "academia/paperChecklist.html#clutters",
    "title": "Paper Writing Guidelines",
    "section": "Clutters",
    "text": "Clutters\n\nThe secret of good writing is to strip every sentence to its cleanest components.\n\nClutteredness is being precise, not counting the words.\n\nAvoid: successful solution, extremely flabbergasted\n\nAvoid: It is noticeable that…\n\nMost of the time you don’t need there is / there are\n\nCan be regarded as vs. is\n\nUse passive voice sparingly and deliberately\n\nUse strong verbs; avoid: take, have, get, to be\n\nAvoid noun-ifying verbs (e.g., compare → make a comparison)\n\nEliminate negatives\n\nAvoid overuse of adverbs (generally, mostly…)"
  },
  {
    "objectID": "academia/paperChecklist.html#general",
    "href": "academia/paperChecklist.html#general",
    "title": "Paper Writing Guidelines",
    "section": "General",
    "text": "General\n\nUse precise verbs\n\nQuotation marks: Use ``something’’ instead of “something”\n\nAvoid acronyms unless universally known\n\nKeep the tone balanced between high-level and technical\n\nImportant concepts: explain using text, figures, and math\n\nAvoid ending paragraphs with only 2–3 words on a line\n\nFill the allowed page limit (e.g., write 8 full pages if the limit is 8)\n\nUse parallelism in lists and structures"
  },
  {
    "objectID": "academia/paperChecklist.html#figures-1",
    "href": "academia/paperChecklist.html#figures-1",
    "title": "Paper Writing Guidelines",
    "section": "Figures",
    "text": "Figures\n\nhttps://agustinus.kristia.de/blog/plotting/"
  },
  {
    "objectID": "academia/paperChecklist.html#tables-1",
    "href": "academia/paperChecklist.html#tables-1",
    "title": "Paper Writing Guidelines",
    "section": "Tables",
    "text": "Tables\n\nhttps://nhigham.com/2019/11/19/better-latex-tables-with-booktabs/\n\nSmall Guide to Making Nice Tables"
  },
  {
    "objectID": "academia/paperChecklist.html#general-1",
    "href": "academia/paperChecklist.html#general-1",
    "title": "Paper Writing Guidelines",
    "section": "General",
    "text": "General\n\nWriting in the Sciences (Coursera)\n\nOn Writing Well\n\nThe Elements of Style\n\nThe Science of Scientific Writing (Gopen & Swan)"
  },
  {
    "objectID": "academia/paperChecklist.html#title-and-abstract-1",
    "href": "academia/paperChecklist.html#title-and-abstract-1",
    "title": "Paper Writing Guidelines",
    "section": "1. 🎯 Title and Abstract",
    "text": "1. 🎯 Title and Abstract"
  },
  {
    "objectID": "academia/paperChecklist.html#introduction-1",
    "href": "academia/paperChecklist.html#introduction-1",
    "title": "Paper Writing Guidelines",
    "section": "2. 📚 Introduction",
    "text": "2. 📚 Introduction"
  },
  {
    "objectID": "academia/paperChecklist.html#related-work",
    "href": "academia/paperChecklist.html#related-work",
    "title": "Paper Writing Guidelines",
    "section": "3. 🔍 Related Work",
    "text": "3. 🔍 Related Work\n\n3.1 All cited works are connected to your method, baseline, or task.\n3.2 At least one baseline from the top-3 most cited recent papers on the topic is mentioned.\n3.3 Related work does not exceed 1.5 pages (unless survey-style paper).\n3.4 You may use LLMs for searching the related work, but double triple check each of the paper – do not trust LLMs!!!!\n3.5 Bonus: use related work section to introduce baseline algorithms – show a table for your proposal better than the existing ones"
  },
  {
    "objectID": "academia/paperChecklist.html#method-1",
    "href": "academia/paperChecklist.html#method-1",
    "title": "Paper Writing Guidelines",
    "section": "4. 🧪 Method",
    "text": "4. 🧪 Method\n\n4.1 All symbols are defined before use.\n4.2 Each equation is referenced with inline explanation (e.g., “Eq. (3) defines the loss over…”). If an equation is never referenced, consider making it inline to save space.\n4.3 All modules or components of the method are illustrated or described in text or figures.\n4.4 Each subsection ideally aligns with parts of the overview figure. Add a short summary paragraph before diving into subsections.\n4.5 You do not need both overview figure and pseudo code in the main text – move the pseudo code to the appendix\n4.6 The method is reproducible without referring to the appendix or external code—reviewers should understand everything from the main text.\n4.7 Bonus: Can anything be removed from this section without reducing clarity? Do not hesitate to cut: more math ≠ better paper."
  },
  {
    "objectID": "academia/paperChecklist.html#experiments",
    "href": "academia/paperChecklist.html#experiments",
    "title": "Paper Writing Guidelines",
    "section": "5. 📊 Experiments",
    "text": "5. 📊 Experiments\n\n5.1 At least 3 datasets are used (unless the paper introduces a new dataset).\n5.2 At least 3 baseline methods are compared. Are they state-of-the-art? Justify why these baselines are chosen.\n5.3 At least 1 ablation study is included.\n5.4 Standard deviation or confidence intervals are reported where appropriate.\n5.5 Hardware environment, software libraries, and hyperparameter settings are described.\n5.6 Negative results (if any) are explained, not omitted—failure cases are valuable.\n5.7 Evaluation metrics are clearly defined and justified.\n5.8 All figures and tables are referenced in the main text.\n5.9 Beyond showing numbers and saying “we perform well,” at least one deeper insight or analysis is provided (e.g., why it works, where it fails).\n5.10 Bonus: Think about how easy others can reproduce your work? If you have any “dirty tricks” – remove them pls."
  },
  {
    "objectID": "academia/paperChecklist.html#writing-quality-and-style",
    "href": "academia/paperChecklist.html#writing-quality-and-style",
    "title": "Paper Writing Guidelines",
    "section": "6. 🧾 Writing Quality and Style",
    "text": "6. 🧾 Writing Quality and Style\n\n6.1 All abbreviations are defined at first use (even ML, LLM, etc.) – do not redefine them again and again.\n6.2 No sentence exceeds 25 words without a comma or period.\n6.3 No paragraph exceeds 10 lines.\n6.4 Passive voice usage &lt; 30% of the total number of sentences.\n6.5 Bonus: Have you noticed that your paper are full of the fancy LLM words, like encompass, intricate, etc?"
  },
  {
    "objectID": "academia/paperChecklist.html#figures-and-tables",
    "href": "academia/paperChecklist.html#figures-and-tables",
    "title": "Paper Writing Guidelines",
    "section": "7. 🖼️ Figures and Tables",
    "text": "7. 🖼️ Figures and Tables\n\n7.1 Each figure/table has a caption ≥ 2 lines that includes interpretation or context. Do not just place it without explanation—reviewers will get lost.\n7.2 Font size in all figures is ≥ 8pt and all labels are fully visible (not cropped).\n7.3 Plots use colors that remain distinguishable when printed in grayscale—some reviewers will print your paper.\n7.4 Each method mentioned in the results appears in either the legend or table column headers.\n7.5 Figures appear at the top of pages rather than mid-text or at the bottom (soft rule, but improves readability).\n7.6 Figures and tables are not redundant—each provides new or complementary information.\nBonus: All figures are in lossless formats (e.g., PDF for vector graphics). Absolutely no low-resolution images allowed."
  },
  {
    "objectID": "academia/paperChecklist.html#structure-and-formatting",
    "href": "academia/paperChecklist.html#structure-and-formatting",
    "title": "Paper Writing Guidelines",
    "section": "8. 🧱 Structure and Formatting",
    "text": "8. 🧱 Structure and Formatting\n\n8.1 All LaTeX warnings and bad boxes have been resolved.\n8.2 Section headers follow the standard paper structure (e.g., Introduction, Method, Experiments, etc.).\n8.3 All appendix sections are explicitly referenced in the main text (e.g., “Appendix B.2 shows…”).\n8.4 No orphan lines anywhere in the paper—avoid single-line section headers or short lines at the top/bottom of columns.\n8.5 No two figures or tables are placed consecutively without explanatory text between them."
  },
  {
    "objectID": "academia/paperChecklist.html#references-1",
    "href": "academia/paperChecklist.html#references-1",
    "title": "Paper Writing Guidelines",
    "section": "9. 📎 References",
    "text": "9. 📎 References\n\n9.1 All references are in the correct format for the target venue.\n9.2 All datasets, toolkits, and models used are cited.\n9.3 At least one paper from the target venue (conference/journal) is cited.\n9.4 Self-citations ≤ 20% of total citations.\n9.5 BibTeX file has been deduplicated and spell-checked."
  },
  {
    "objectID": "academia/paperChecklist.html#citation-sanity-check-llm-generated-risk",
    "href": "academia/paperChecklist.html#citation-sanity-check-llm-generated-risk",
    "title": "Paper Writing Guidelines",
    "section": "10. 🛑 Citation Sanity Check (LLM-Generated Risk)",
    "text": "10. 🛑 Citation Sanity Check (LLM-Generated Risk)\n\n10.1 All citations were manually verified to exist—title, authors, venue, and year match a real, published paper.\n10.2 No hallucinated references from LLM tools are included.\n10.3 If a citation was generated by ChatGPT, Copilot, or similar, it has been cross-checked on Google Scholar, Semantic Scholar, or publisher sites."
  },
  {
    "objectID": "academia/paperChecklist.html#sanity-checks-before-submission",
    "href": "academia/paperChecklist.html#sanity-checks-before-submission",
    "title": "Paper Writing Guidelines",
    "section": "11. 🧠 Sanity Checks Before Submission",
    "text": "11. 🧠 Sanity Checks Before Submission\n\n11.1 PDF compiles in Overleaf/TeX with no errors or bad boxes.\n11.2 File name follows the submission guideline format (e.g., no underscores or author names if anonymized).\n11.3 No author-identifying information exists in metadata, supplementary files, or file names. Check your code repository and images too.\n11.4 The paper length complies with the page limit, including references and appendices (if counted).\n11.5 The paper has been read start-to-finish by someone not on the author list, without them needing to stop for clarification.\n11.6 All co-authors are listed and properly acknowledged—this is surprisingly often overlooked.\n11.7 Bonus: After submission, log in from a different device and OS (e.g., Mac, Windows) to verify that the uploaded version renders correctly.\n\nhttps://docs.google.com/document/d/1AoF6bPJp-muWnsZLMmfcxo1fmAu1izUzZXDFHar-35o/edit?usp=sharing cs-paper-checklist\nKey Preparation Decide on a narrative - best done in close collaboration with a mentor Draft an abstract Do a round of reading through and editing - try reading it aloud to yourself Get sign-off from a mentor (if possible) Make a bullet point outline of the paper, emphasising sections and section titles, key figures and key experimental results. Do this in a google doc Get sign-off from a mentor (if possible) Double check exactly when the submission deadline is, ensure you’re converting time zones properly Writing Stage Note: These can largely be done in any order Draft the introduction. Do this in Overleaf in the correct conference template Get LLM feedback and edit Get 1+ round of feedback from a mentor & iterate Draft the prose of the main body (excl related work, and excl appendices which can be very last minute). It’s OK to leave some things blank or as placeholders, just make sure you know what’s missing For anything to do with fiddly LaTeX, use an LLM unless you really know what you’re doing, and if you’ve been stuck for &gt;10 mins try to ask someone for help Get the key experimental results and put them into the paper as numbers or figures Make draft figures Draft the related work (if you don’t know what to do, use deep research, then ask a mentor) Do a round of editing and polishing on the whole thing - focus on the intro and figures Run it through multiple LLMs for feedback. Put several good papers in their context window too. Always tell it “here is my colleague’s/someone else’s paper” and ask “what is the biggest problem with this” to avoid sycophancy Ask a mentor to give feedback on the whole thing (if possible) Do this multiple times if possible Get feedback on the whole thing from one person not on the project, eg via a paper swap. Take this seriously. Have them repeat back to you what they think your contribution is, why it matters, and what evidence you provide for it/why that evidence is compelling - if this is wrong, this is a big deal and you need to edit until it’s fixed! Do a second round of editing and polishing on the whole thing Decide on the authorship order: If there are 2 or more main contributors, I recommend being co-first authors By default I recommend randomising the first author order, and putting an asterisk saying it was randomised (do NOT do alphabetical, it’s not standard in ML and screws over people with surnames late in the alphabet) If you both agree one person contributed more, they should go first If there’s disagreement, go and read Chris Olah’s blog post about credit allocation If there’s still disagreement, please co-write an author contributions statement, then see if you now agree If there’s still disagreement, try to get a neutral party to adjudicate Think carefully for anyone who contributed to the project and should maybe be an author, check in with them. Try hard to not snub people. Write the appendices - please leave this to the end as they’re rarely read, and can be cut for time if needed Optional: Make your code available using https://anonymous.4open.science/ and link from the paper - this shows reproducibility/effort The code doesn’t need to be polished (and you can likely get away with it not being runnable) right now, the key thing is that it exists. Make it nice before Arxiv/the camera ready Final Stages Once you have a draft you’re happy with, check how many pages it is. If you’re above the official page limit, start cutting down page count Ask yourself which subsections are least important, and move them to the appendix Condense your figures, eg combining graphs into one figure with several facet plots Do minor tweaks/word level editing for conciseness Ask an LLM how to be more concise I’d guess this can shave down 0.5-1.5 pages of space - if you need more, do the others first! [If NeurIPS] Do the NeurIPS checklist!! Do not leave it to the last minute, it’s long and you get desk rejected without it Sanity check for anonymity There are no github or HuggingFace links that expose a username If you need to share code, use https://anonymous.4open.science/ If you need to share model weights make an anonymous HuggingFace account Ensure your name is not in there, or other identifying information - give it to an LLM and ask Check your paper for TODO and ??, fix them Submission Submit the abstract Do this at least 24 hours before the deadline, and keep re-submitting Choose keywords and topic according to “do I want reviewers who looked for this topic/keyword to be reviewing my paper”, which is not always the same as where it best fits. Submit the paper Do this at least 24 hours before the deadline and keep re-submitting! You do NOT want to have your internet go down or OpenReview to crash at the last minute. Public release You can release your work publicly even during the anonymous peer review process for most ML conferences. But prioritise getting a good conference submission first. Make a list of all the additional polishing/new results you didn’t have time for, and think are worth it Estimate how long each will take, and how valuable it would be Remove the ones that don’t seem worth it Do what remains Put them in the paper If you cut important content to meet the page limit, put it back in - Arxiv does not need to be identical to what is published at the conference Add an acknowledgements section - be generous, thank everyone who gave feedback or otherwise helped Co-write an author contribution section - try to emphasise the parts you’re each proudest of! This is a positive-sum game Have the email of a corresponding author, typically one or both first authors Draft a tweet thread There’s an art to writing good tweet threads. Note that 95% of people only see tweet 1, so it needs to be great and is where most of your effort should go. Tweet 2 matters a bit, later tweets don’t. It needs to: Be accessible to an ML person who has no initial idea that your tweet is about a paper at all Communicate the key insight of the paper in 280 characters with simple language Have an eye catching figure Do not have any links in tweet 1, it de-boosts it Get feedback from someone good at Twitter, if possible Draft a LessWrong post Clean up the code and make it public on Github Link in the LW post and at the end of the paper abstract Make sure to have a license. MIT or Apache 2.0 License are good if you want the code to be easily usable by others Bonus: Have a helpful README Bonus: Have a python notebook showing how to run the basics of your code Optional: Draft a blog post. This is for giving an accessible summary of the work. Begin with a bullet point outline If you think there’s genuinely interesting takeaways, tacit knowledge, nuance, advice for future work, wild speculation, etc, please talk about it! The paper format sadly disincentives this kind of valuable knowledge Release Put the paper on Arxiv Note that it takes about 1 working day for the paper to appear publicly on Arxiv If you have not submitted to Arxiv before you’ll need someone with 3+ papers to endorse you for the ML category (cs.LG), get this out of the way earlier if possible Post the tweet thread once the paper is on Arxiv. Post the blog post\nhttps://www.alignmentforum.org/posts/eJGptPbbFPZGLpjsp/highly-opinionated-advice-on-how-to-write-ml-papers"
  },
  {
    "objectID": "mlops/m1/python.html",
    "href": "mlops/m1/python.html",
    "title": "Python",
    "section": "",
    "text": "Environment\nNên dùng Anaconda khi dev local (vì sự thuận tiện bởi các package có sẵn), và dùng local environment khi deploy.\nSmoke test: test lần đầu tiên về cái mình vừa cài đặt.\nNếu dùng chung môi trường sẽ dễ bị conflict với các project khác (version của thư viện, …) -&gt; best practice là tạo môi trường ảo riêng.\n\n\nCompiled vs. Interpreted language\nCompiled language: biên dịch, phải chuyển (build) thành machine code (ngôn ngữ máy có thể hiểu được) VD: Rust, C++, C, Go, …\nInterpreted language: thực thi từng dòng một.\n\nA script -&gt; Ready to run\nA script (Python, Ruby) -&gt; Byte code (máy chưa hiểu được luôn) -&gt; Thực thi (Python thuộc định nghĩa này, với byte code được lưu ở pycache).\n\nInterpreted language thường chậm hơn compiled language.\n\nimport argparse\nimport numpy\n\ndef main(args):\n    a = args.a\n    b = args.b\n    print(\"Sum of a and b is: \", a + b)\n\nif __name__ == \"__main__\":\n    parser = argparse.ArgumentParser()\n    parser.add_argument(\"--a\", type=int, default=1, required=True)\n    parser.add_argument(\"--b\", type=int, default=2, required=True)\n    args = parser.parse_args()\n    main(args)\nRun\npython main.py --a 1 --b 2\n\n\n\n\n\n\n\n\n\nCompiled language\nInterpreted language\n\n\n\n\nExecution speed\nFast\nSlow\n\n\nCompilation steps\nAn extra “build” step\nNo\n\n\nModification complexity\nRebuild at every change\nDon’t need to rebuild\n\n\n\nNote: The definitions of compiled and interpreted language are varied, but it gives us some ideas why Python is slower than C++.\n\n\nData structures\n\nmy_tuple = (1,2,3,1)\nprint(f\"My tuple is: {my_tuple}\")\n\nMy tuple is: (1, 2, 3, 1)\n\n\n\n\n\n\n\n\nNote\n\n\n\nf-string can be bad in terms of security reasons.\n\n\nBasics of data structures:\n\nList: mutable, used to add or remove elements\nTuple: immutable, used to store related data\nDictionary: mutable, key-value pairs, used to store data that can be looked up by a key\nSet: mutable, unordered, used to store unique elements\n\nTuple thường vẫn có thể được dùng làm dictionary key. (Bất kì object nào có thể hashable đều có thể làm key). Hash là khả năng tạo ra sự duy nhất.\n\n\nFunctions and decorators\nDecorator là một hàm nhận vào hàm khác, dùng @ để đặt trước hàm được decorate.\nCác ứng dụng của decorator:\n\nAirflow, kubeflow\n\n\n# Import the library\nimport argparse\n\n# Import process_time to calculate the real amount\n# of time spent on this process, please take a look\n# at the below articles to understand the difference\n# between time(), perf_counter(), and process_time()\n# https://www.linkedin.com/pulse/timetime-vs-timeperfcounter-python-raghavendraa-battula/\n# https://sentry.io/answers/measure-elapsed-time-in-python/\nfrom time import process_time\n\nfrom loguru import logger\n\n# Reference: https://blog.miguelgrinberg.com/post/the-ultimate-guide-to-python-decorators-part-iii-decorators-with-arguments\n# and take a look at this article for decorators with classes https://www.freecodecamp.org/news/python-decorators-explained-with-examples/\ndef measure_execution_time_with_args(*d_args, **d_kwargs):\n    def wrapper(input_func):\n        def inner(*args, **kwargs):\n            # Calculate the starting time\n            start = process_time()\n            # Execute the inner function\n            print(\"Debugging measure_execution_time arguments:\")\n            print(\"d_args: \", d_args)\n            print(\"d_kwargs: \", d_kwargs)\n            result = input_func(*args, **kwargs)\n            # Calculate the end time\n            end = process_time()\n            logger.info(f\"Elapsed time: {end - start}\")\n            return result\n\n        return inner\n\n    return wrapper\n\n\ndef measure_execution_time(input_func):\n    def inner(*args, **kwargs):\n        # Calculate the starting time\n        start = process_time()\n        # Execute the inner function\n        result = input_func(*args, **kwargs)\n        # Calculate the end time\n        end = process_time()\n        logger.info(f\"Elapsed time: {end - start}\")\n        return result\n\n    return inner\n\n# You can also replace with the decorator with arguments below\n# @measure_execution_time_with_args(10, myarg=100, myarg2=200)\n@measure_execution_time\ndef predict(x):\n    model = {\"a\": 1, \"b\": 2}\n    logger.info(\"Getting predictions!\")\n    if x in model:\n        return model[x]\n    else:\n        raise ValueError(f\"Could not predict {x}\")\n\n\ndef main(args):\n    x = args.x\n    print(predict(x))\n\n\n# This is only executed when invoking this script\nif __name__ == \"__main__\":\n    # Create a parser to receive the command line arguments\n    parser = argparse.ArgumentParser()\n    # Define all arguments\n    parser.add_argument(\"-x\", type=str, required=True)\n    # Parse the arguments\n    args = parser.parse_args()\n    # Run the main function\n    main(args)\nx\n\n\n\n\n Back to top"
  },
  {
    "objectID": "blogs/modality-gap.html",
    "href": "blogs/modality-gap.html",
    "title": "Modality Gap",
    "section": "",
    "text": "Paper compilation (Email me if you want to add other relevant papers that is not included.)\n\n\n\nTitle\nYear\nVenue\nType\n\n\n\n\nMind the Gap: Understanding the Modality Gap in Multi-modal Contrastive Representation Learning\n2022\nNeurIPS\nFundamental\n\n\nExplaining and Mitigating the Modality Gap in Contrastive Multimodal Learning\n2024\narXiv\nTheoretical\n\n\nTwo effects, one trigger: On the modality gap, object bias, and information imbalance in contrastive visionlanguage representation learning\n2024\narXiv\nTheoretical\n\n\nTowards understanding the modality gap in CLIP\n2024\nOpenReview\nTheoretical\n\n\nIt’s not a modality gap: Characterizing and addressing the contrastive gap\n2024\narXiv\nTheoretical\n\n\nBridge the Modality and Capability Gaps in Vision-Language Model Selection\n2024\nOpenReview\nTheoretical\n\n\nFill the Gap: Quantifying and Reducing the Modality Gap in Image-Text Representation Learning\n2025\narXiv\nTheoretical\n\n\nPost-pre-training for Modality Alignment in Vision-Language Foundation Models\n2025\narXiv\nTheoretical\n\n\nCross-Modal Mapping: Mitigating the Modality Gap for Few-Shot Image Classification\n2024\narXiv\nTheoretical\n\n\nMitigating the Modality Gap: Few-Shot Out-of-Distribution Detection with Multi-modal Prototypes and Image Bias Estimation\n2025\narXiv\nTheoretical\n\n\nHow to Bridge the Gap between Modalities: Survey on Multimodal Large Language Model\n2023\narXiv\nSurvey\n\n\nHarnessing the Universal Geometry of Embeddings\n2025\narXiv\nTheoretical\n\n\n\n\nBlogs\n\nUnderstanding and Comparing Latent Space Characteristics of Multi-Modal Models\n\n\n\n\n\n Back to top"
  },
  {
    "objectID": "mlops/m1/lesson4.html#multi-process-one-thread",
    "href": "mlops/m1/lesson4.html#multi-process-one-thread",
    "title": "Lesson 4 - Multithreading, Multiprocessing, AsyncIO",
    "section": "Multi-Process One-Thread",
    "text": "Multi-Process One-Thread\n\nimport multiprocessing\nfrom multiprocessing import Process\n\n\n# Check number of available cores\nprint(\"Number of cpu cores: \", multiprocessing.cpu_count())\n\nNumber of cpu cores:  16\n\n\n\nimport pandas as pd\nfrom time import sleep, time\n\n# Example of a processing function\ndef process_dataframe(chunk_id, chunk_data: pd.DataFrame):\n    print(f\"Processing chunk {chunk_id}\")\n    sleep(5)\n    print(f\"The chunk {chunk_id} has been processed successfully!\")\n\n\n# Make a sample dataframe\ndata = [\n    ['tom', 10], ['nick', 15],\n    ['juli', 14], ['peter', 20],\n    ['jason', 27], ['anna', 11]\n]\n\n# Create the pandas DataFrame\ndf = pd.DataFrame(data, columns=['Name', 'Age'])\n\n# Devide the dataframe into chunks\nchunk_size = 2\nchunks = [df[i:i+chunk_size].to_numpy() for i in range(0, len(df), chunk_size)]\n\n# Mark the starting point to measure processing time\nstart = time()\n\nprocs = []\nfor i, chunk in enumerate(chunks):\n  # Define our process but not yet start\n  proc = Process(target=process_dataframe, args=(i, chunk,))\n  # Start the process\n  proc.start()\n  # Investigate process ID\n  print(f\"Process ID: {proc.pid}\")\n  # Manage all process definitions in a list\n  procs.append(proc)\n\n# Stop all processes to prevent resource scarcity\n# imagine zombie processes\nfor proc in procs:\n    proc.join() # Wait for the process to finish\n\n# Report total elapsed time\nprint(f\"Total time: {time() - start}s\")\n\nProcess ID: 26580\nProcessing chunk 0\nProcess ID: 26583\nProcessing chunk 1\nProcessing chunk 2\nProcess ID: 26588\nThe chunk 0 has been processed successfully!\nThe chunk 1 has been processed successfully!\nThe chunk 2 has been processed successfully!\nTotal time: 5.035507917404175s\n\n\n\n##############################################\n# The 2nd way to do multiprocess is via Pool #\n##############################################\nfrom multiprocessing import Pool\n\n# Define a pool of processes\nNUM_PROCESSES = 2\npool = multiprocessing.Pool(NUM_PROCESSES)\n\nprocs = []\nfor i, chunk in enumerate(chunks):\n    procs.append(\n        # The main process does not need to wait for this function\n        pool.apply_async(process_dataframe, args=(i, chunk))\n    )\n\nfor proc in procs:\n    proc.get() # Wait for the process to finish\n\nProcessing chunk 0Processing chunk 1\n\nThe chunk 1 has been processed successfully!The chunk 0 has been processed successfully!\n\nProcessing chunk 2\nThe chunk 2 has been processed successfully!\nProcessing chunk 0Processing chunk 1\n\nThe chunk 0 has been processed successfully!The chunk 1 has been processed successfully!\n\nProcessing chunk 2\nThe chunk 2 has been processed successfully!\n\n\n\n##############################################\n# The 3rd way to do multiprocess is via Map #\n##############################################\ninputs = [(i, chunk) for i, chunk in enumerate(chunks)]\n\n# If you have one arg only, please use `.map` instead\noutputs = pool.starmap(\n    process_dataframe,\n    inputs\n)\n\n\n# Uhm... you can see that sometimes, two strings are concatnated w/o any `/n`\n# this is because all the processes writes to stdout at the same time, we need\n# to find a way to prevent other processes write to it while one is doing\nfrom multiprocessing import Lock\n\nlock = Lock()\n\ndef process_dataframe(chunk_id, chunk_data: pd.DataFrame):\n    lock.acquire()\n    print(f\"Processing chunk {chunk_id}\")\n    sleep(5)\n    print(f\"The chunk {chunk_id} has been processed successfully!\")\n    lock.release()\n\n# Make a sample dataframe\ndata = [\n    ['tom', 10], ['nick', 15],\n    ['juli', 14], ['peter', 20],\n    ['jason', 27], ['anna', 11]\n]\n\n# Create the pandas DataFrame\ndf = pd.DataFrame(data, columns=['Name', 'Age'])\n\n# Devide the dataframe into chunks\nchunk_size = 2\nchunks = [df[i:i+chunk_size] for i in range(0, len(df), chunk_size)]\n\n# Mark the starting point to measure processing time\nstart = time()\n\nprocs = []\nfor i, chunk in enumerate(chunks):\n  # Define our process but not yet start\n  proc = Process(target=process_dataframe, args=(i, chunk,))\n  # Start the process\n  proc.start()\n  # Manage all process definitions in a list\n  procs.append(proc)\n\n# Stop all processes to prevent resource scarcity\n# imagine zombie processes\nfor proc in procs:\n    proc.join()\n\n# Report total elapsed time\nprint(f\"Total time: {time() - start}s\")\n\nProcessing chunk 0\nThe chunk 0 has been processed successfully!\nProcessing chunk 1\nThe chunk 1 has been processed successfully!\nProcessing chunk 2\nThe chunk 2 has been processed successfully!\nTotal time: 15.042691946029663s"
  },
  {
    "objectID": "mlops/m1/lesson4.html#multi-threaded",
    "href": "mlops/m1/lesson4.html#multi-threaded",
    "title": "Lesson 4 - Multithreading, Multiprocessing, AsyncIO",
    "section": "Multi-Threaded",
    "text": "Multi-Threaded\nPython provides the same APIs to multiprocessing with start() and join().\n\nfrom threading import Thread\n\nmy_threads = []\nfor i, chunk in enumerate(chunks):\n    my_thread = Thread(target=process_dataframe, args=(i, chunk,))\n    my_thread.start()\n    my_threads.append(my_thread)\n\nfor my_thread in my_threads:\n    my_thread.join()\n\nHowever, the way they share data is different. Let’s take a look at the example below\n\nfrom threading import Lock\n\n# Define a lock to prevent race condition,\n# which is multiple updates into the same variable\nlock = Lock()\n\n# Share data\nshared_data = 0\n\ndef increment_function():\n    global shared_data\n    with lock: # This is equal to acquire() + release()\n        shared_data += 1\n\nmy_threads = []\nfor _ in range(3):\n    my_thread = Thread(target=increment_function)\n    my_thread.start()\n    my_threads.append(my_thread)\n\nfor my_thread in my_threads:\n    my_thread.join()\n\nprint(f\"Current value of shared_data: {shared_data}\")\n\n\n# Another way to access shared_data is via Queue\nfrom queue import Queue\n\n# Initialize the queue\nshared_queue = Queue()\n\ndef increment_function():\n    shared_queue.put(1)\n\nmy_threads = []\nfor _ in range(3):\n    my_thread = Thread(target=increment_function)\n    my_thread.start()\n    my_threads.append(my_thread)\n\nfor my_thread in my_threads:\n    my_thread.join()\n\nshared_data = 0\nwhile not shared_queue.empty():\n    shared_data += shared_queue.get()\n\nprint(f\"Current value of shared_data: {shared_data}\")\n\n\n# Using a global variable is not a piece of cake in multiprocessing\n# as in multithreading\nimport multiprocessing\n\n# Define a shared value between oprocesses\nshared_variable = multiprocessing.Value('i', 0)\n\ndef worker_function():\n    global shared_variable\n    with shared_variable.get_lock():\n        shared_variable.value += 1\n\nprocs = []\nfor _ in range(3):\n  # Define our process but not yet start\n  proc = Process(target=worker_function)\n  # Start the process\n  proc.start()\n  # Manage all process definitions in a list\n  procs.append(proc)\n\nfor proc in procs:\n    proc.join()\n\nprint(f\"Current value of shared_data: {shared_data}\")\n\n\n# Or using queue\nfrom multiprocessing import Queue\nfrom multiprocessing import Process\n\ndef worker_function(shared_queue):\n    shared_queue.put(1)\n\nshared_queue = Queue()\n\nprocs = []\nfor _ in range(3):\n    my_proc = Process(target=worker_function, args=(shared_queue,))\n    my_proc.start()\n    procs.append(my_proc)\n\nfor proc in procs:\n    proc.join()\n\n# The main process retrieves data from the queue\n# and aggregate the result\nresult = 0\nwhile not shared_queue.empty():\n    result += shared_queue.get(1)\n\nprint(f\"Results from multiprocessing queue: {result}\")"
  },
  {
    "objectID": "mlops/m1/lesson4.html#one-process-one-thread",
    "href": "mlops/m1/lesson4.html#one-process-one-thread",
    "title": "Lesson 4 - Multithreading, Multiprocessing, AsyncIO",
    "section": "One-Process One-Thread",
    "text": "One-Process One-Thread\n\n# Let's say you have one process with one thread only, how to deal with it?\n# AsyncIO helps us to do it\nimport asyncio\n\nasync def download_my_1st_data_func():\n    print(\"Starting my 1st data func...\")\n    await asyncio.sleep(2)\n    print(\"Completed my 1st data func!\")\n\nasync def download_my_2nd_data_func():\n    print(\"Starting my 2nd data func...\")\n    await asyncio.sleep(2)\n    print(\"Completed my 2nd data func!\")\n\ndef main():\n    loop = asyncio.get_event_loop()\n    loop.run_until_complete(asyncio.gather(\n        download_my_1st_data_func(),\n        download_my_2nd_data_func(),\n    ))\n    loop.close()\n\n# Run the event loop\nmain()\n\n# YOU WILL MEET SOME WEIRD ERRORS HERE DUE TO IPYTHON NOTEBOOK,\n# LET'S MOVE TO A PYTHON SCRIPT\n\n\n---------------------------------------------------------------------------\nRuntimeError                              Traceback (most recent call last)\n/tmp/ipykernel_26363/3615891077.py in &lt;module&gt;\n     22 \n     23 # Run the event loop\n---&gt; 24 main()\n     25 \n     26 # YOU WILL MEET SOME WEIRD ERRORS HERE DUE TO IPYTHON NOTEBOOK,\n\n/tmp/ipykernel_26363/3615891077.py in main()\n     15 def main():\n     16     loop = asyncio.get_event_loop()\n---&gt; 17     loop.run_until_complete(asyncio.gather(\n     18         download_my_1st_data_func(),\n     19         download_my_2nd_data_func(),\n\n~/anaconda3/lib/python3.9/asyncio/base_events.py in run_until_complete(self, future)\n    621         \"\"\"\n    622         self._check_closed()\n--&gt; 623         self._check_running()\n    624 \n    625         new_task = not futures.isfuture(future)\n\n~/anaconda3/lib/python3.9/asyncio/base_events.py in _check_running(self)\n    581     def _check_running(self):\n    582         if self.is_running():\n--&gt; 583             raise RuntimeError('This event loop is already running')\n    584         if events._get_running_loop() is not None:\n    585             raise RuntimeError(\n\nRuntimeError: This event loop is already running\n\n\n\nStarting my 1st data func...\nStarting my 2nd data func...\nCompleted my 1st data func!\nCompleted my 2nd data func!"
  },
  {
    "objectID": "mlops/m1/lesson4.html",
    "href": "mlops/m1/lesson4.html",
    "title": "Lesson 4 - Multithreading, Multiprocessing, AsyncIO",
    "section": "",
    "text": "Thread cùng chia sẻ bộ nhớ với nhau khi cùng process. GIL để hạn chế việc nhiều thread cùng ghi vào variable. Ngoài ra, các tác vụ ngoài I/O bound (gọi API, truy cập database, …), GIL sẽ khoá -&gt; không tận dụng được sức mạnh của multi-thread.\nThông thường việc sử dụng nhiều thread là tốt nhất. Tuy nhiên các tác vụ CPU bound chỉ nên dùng 1 thread.\nChỉ nên dùng await khi có tác vụ I/O bound."
  }
]