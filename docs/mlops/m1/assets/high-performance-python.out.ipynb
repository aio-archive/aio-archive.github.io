{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "# Multi-Process One-Thread"
      ],
      "id": "0ced23f8-602c-4607-b28d-bc9444868c57"
    },
    {
      "cell_type": "code",
      "execution_count": 1,
      "metadata": {},
      "outputs": [],
      "source": [
        "import multiprocessing\n",
        "from multiprocessing import Process"
      ],
      "id": "cell-2"
    },
    {
      "cell_type": "code",
      "execution_count": 2,
      "metadata": {
        "outputId": "5e20d79c-4bc5-41ca-b1d3-65a9d7db2577",
        "quarto-private-1": {
          "key": "colab",
          "value": {
            "base_uri": "https://localhost:8080/"
          }
        }
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Number of cpu cores:  16"
          ]
        }
      ],
      "source": [
        "# Check number of available cores\n",
        "print(\"Number of cpu cores: \", multiprocessing.cpu_count())"
      ],
      "id": "cell-3"
    },
    {
      "cell_type": "code",
      "execution_count": 3,
      "metadata": {},
      "outputs": [],
      "source": [
        "import pandas as pd\n",
        "from time import sleep, time\n",
        "\n",
        "# Example of a processing function\n",
        "def process_dataframe(chunk_id, chunk_data: pd.DataFrame):\n",
        "    print(f\"Processing chunk {chunk_id}\")\n",
        "    sleep(5)\n",
        "    print(f\"The chunk {chunk_id} has been processed successfully!\")"
      ],
      "id": "cell-4"
    },
    {
      "cell_type": "code",
      "execution_count": 4,
      "metadata": {
        "outputId": "e52c1d7b-c491-4227-9c9f-9d9059856a09",
        "quarto-private-1": {
          "key": "colab",
          "value": {
            "base_uri": "https://localhost:8080/"
          }
        }
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Process ID: 26580\n",
            "Processing chunk 0\n",
            "Process ID: 26583\n",
            "Processing chunk 1\n",
            "Processing chunk 2\n",
            "Process ID: 26588\n",
            "The chunk 0 has been processed successfully!\n",
            "The chunk 1 has been processed successfully!\n",
            "The chunk 2 has been processed successfully!\n",
            "Total time: 5.035507917404175s"
          ]
        }
      ],
      "source": [
        "# Make a sample dataframe\n",
        "data = [\n",
        "    ['tom', 10], ['nick', 15],\n",
        "    ['juli', 14], ['peter', 20],\n",
        "    ['jason', 27], ['anna', 11]\n",
        "]\n",
        "\n",
        "# Create the pandas DataFrame\n",
        "df = pd.DataFrame(data, columns=['Name', 'Age'])\n",
        "\n",
        "# Devide the dataframe into chunks\n",
        "chunk_size = 2\n",
        "chunks = [df[i:i+chunk_size].to_numpy() for i in range(0, len(df), chunk_size)]\n",
        "\n",
        "# Mark the starting point to measure processing time\n",
        "start = time()\n",
        "\n",
        "procs = []\n",
        "for i, chunk in enumerate(chunks):\n",
        "  # Define our process but not yet start\n",
        "  proc = Process(target=process_dataframe, args=(i, chunk,))\n",
        "  # Start the process\n",
        "  proc.start()\n",
        "  # Investigate process ID\n",
        "  print(f\"Process ID: {proc.pid}\")\n",
        "  # Manage all process definitions in a list\n",
        "  procs.append(proc)\n",
        "\n",
        "# Stop all processes to prevent resource scarcity\n",
        "# imagine zombie processes\n",
        "for proc in procs:\n",
        "    proc.join() # Wait for the process to finish\n",
        "\n",
        "# Report total elapsed time\n",
        "print(f\"Total time: {time() - start}s\")"
      ],
      "id": "cell-5"
    },
    {
      "cell_type": "code",
      "execution_count": 5,
      "metadata": {
        "outputId": "0374964f-9a77-4f05-f025-f3d97a4ddfc0",
        "quarto-private-1": {
          "key": "colab",
          "value": {
            "base_uri": "https://localhost:8080/"
          }
        }
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Processing chunk 0Processing chunk 1\n",
            "\n",
            "The chunk 1 has been processed successfully!The chunk 0 has been processed successfully!\n",
            "\n",
            "Processing chunk 2\n",
            "The chunk 2 has been processed successfully!\n",
            "Processing chunk 0Processing chunk 1\n",
            "\n",
            "The chunk 0 has been processed successfully!The chunk 1 has been processed successfully!\n",
            "\n",
            "Processing chunk 2\n",
            "The chunk 2 has been processed successfully!"
          ]
        }
      ],
      "source": [
        "##############################################\n",
        "# The 2nd way to do multiprocess is via Pool #\n",
        "##############################################\n",
        "from multiprocessing import Pool\n",
        "\n",
        "# Define a pool of processes\n",
        "NUM_PROCESSES = 2\n",
        "pool = multiprocessing.Pool(NUM_PROCESSES)\n",
        "\n",
        "procs = []\n",
        "for i, chunk in enumerate(chunks):\n",
        "    procs.append(\n",
        "        # The main process does not need to wait for this function\n",
        "        pool.apply_async(process_dataframe, args=(i, chunk))\n",
        "    )\n",
        "\n",
        "for proc in procs:\n",
        "    proc.get() # Wait for the process to finish"
      ],
      "id": "cell-6"
    },
    {
      "cell_type": "code",
      "execution_count": 6,
      "metadata": {},
      "outputs": [],
      "source": [
        "##############################################\n",
        "# The 3rd way to do multiprocess is via Map #\n",
        "##############################################\n",
        "inputs = [(i, chunk) for i, chunk in enumerate(chunks)]\n",
        "\n",
        "# If you have one arg only, please use `.map` instead\n",
        "outputs = pool.starmap(\n",
        "    process_dataframe,\n",
        "    inputs\n",
        ")"
      ],
      "id": "cell-7"
    },
    {
      "cell_type": "code",
      "execution_count": 7,
      "metadata": {
        "outputId": "a35359df-998b-40ff-8403-453ea3309eb7",
        "quarto-private-1": {
          "key": "colab",
          "value": {
            "base_uri": "https://localhost:8080/"
          }
        }
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Processing chunk 0\n",
            "The chunk 0 has been processed successfully!\n",
            "Processing chunk 1\n",
            "The chunk 1 has been processed successfully!\n",
            "Processing chunk 2\n",
            "The chunk 2 has been processed successfully!\n",
            "Total time: 15.042691946029663s"
          ]
        }
      ],
      "source": [
        "# Uhm... you can see that sometimes, two strings are concatnated w/o any `/n`\n",
        "# this is because all the processes writes to stdout at the same time, we need\n",
        "# to find a way to prevent other processes write to it while one is doing\n",
        "from multiprocessing import Lock\n",
        "\n",
        "lock = Lock()\n",
        "\n",
        "def process_dataframe(chunk_id, chunk_data: pd.DataFrame):\n",
        "    lock.acquire()\n",
        "    print(f\"Processing chunk {chunk_id}\")\n",
        "    sleep(5)\n",
        "    print(f\"The chunk {chunk_id} has been processed successfully!\")\n",
        "    lock.release()\n",
        "\n",
        "# Make a sample dataframe\n",
        "data = [\n",
        "    ['tom', 10], ['nick', 15],\n",
        "    ['juli', 14], ['peter', 20],\n",
        "    ['jason', 27], ['anna', 11]\n",
        "]\n",
        "\n",
        "# Create the pandas DataFrame\n",
        "df = pd.DataFrame(data, columns=['Name', 'Age'])\n",
        "\n",
        "# Devide the dataframe into chunks\n",
        "chunk_size = 2\n",
        "chunks = [df[i:i+chunk_size] for i in range(0, len(df), chunk_size)]\n",
        "\n",
        "# Mark the starting point to measure processing time\n",
        "start = time()\n",
        "\n",
        "procs = []\n",
        "for i, chunk in enumerate(chunks):\n",
        "  # Define our process but not yet start\n",
        "  proc = Process(target=process_dataframe, args=(i, chunk,))\n",
        "  # Start the process\n",
        "  proc.start()\n",
        "  # Manage all process definitions in a list\n",
        "  procs.append(proc)\n",
        "\n",
        "# Stop all processes to prevent resource scarcity\n",
        "# imagine zombie processes\n",
        "for proc in procs:\n",
        "    proc.join()\n",
        "\n",
        "# Report total elapsed time\n",
        "print(f\"Total time: {time() - start}s\")"
      ],
      "id": "cell-8"
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## Multi-Threaded\n",
        "\n",
        "Python provides the same APIs to multiprocessing with `start()` and\n",
        "`join()`."
      ],
      "id": "5a529e03-f55b-414d-82f8-5d3f857d6416"
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "outputId": "fea68c6b-174a-47f1-c482-66f016a2bc02",
        "quarto-private-1": {
          "key": "colab",
          "value": {
            "base_uri": "https://localhost:8080/"
          }
        }
      },
      "outputs": [],
      "source": [
        "from threading import Thread\n",
        "\n",
        "my_threads = []\n",
        "for i, chunk in enumerate(chunks):\n",
        "    my_thread = Thread(target=process_dataframe, args=(i, chunk,))\n",
        "    my_thread.start()\n",
        "    my_threads.append(my_thread)\n",
        "\n",
        "for my_thread in my_threads:\n",
        "    my_thread.join()"
      ],
      "id": "cell-11"
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "However, the way they share data is different. Let’s take a look at the\n",
        "example below"
      ],
      "id": "e1a60e92-bff4-4bc8-972d-7b39868f0a9e"
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "outputId": "68b2ccca-03f3-4b66-82ef-da3902d91627",
        "quarto-private-1": {
          "key": "colab",
          "value": {
            "base_uri": "https://localhost:8080/"
          }
        }
      },
      "outputs": [],
      "source": [
        "from threading import Lock\n",
        "\n",
        "# Define a lock to prevent race condition,\n",
        "# which is multiple updates into the same variable\n",
        "lock = Lock()\n",
        "\n",
        "# Share data\n",
        "shared_data = 0\n",
        "\n",
        "def increment_function():\n",
        "    global shared_data\n",
        "    with lock: # This is equal to acquire() + release()\n",
        "        shared_data += 1\n",
        "\n",
        "my_threads = []\n",
        "for _ in range(3):\n",
        "    my_thread = Thread(target=increment_function)\n",
        "    my_thread.start()\n",
        "    my_threads.append(my_thread)\n",
        "\n",
        "for my_thread in my_threads:\n",
        "    my_thread.join()\n",
        "\n",
        "print(f\"Current value of shared_data: {shared_data}\")"
      ],
      "id": "cell-13"
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "outputId": "7164ee3b-5bd6-42bd-dc97-a652784f854c",
        "quarto-private-1": {
          "key": "colab",
          "value": {
            "base_uri": "https://localhost:8080/"
          }
        }
      },
      "outputs": [],
      "source": [
        "# Another way to access shared_data is via Queue\n",
        "from queue import Queue\n",
        "\n",
        "# Initialize the queue\n",
        "shared_queue = Queue()\n",
        "\n",
        "def increment_function():\n",
        "    shared_queue.put(1)\n",
        "\n",
        "my_threads = []\n",
        "for _ in range(3):\n",
        "    my_thread = Thread(target=increment_function)\n",
        "    my_thread.start()\n",
        "    my_threads.append(my_thread)\n",
        "\n",
        "for my_thread in my_threads:\n",
        "    my_thread.join()\n",
        "\n",
        "shared_data = 0\n",
        "while not shared_queue.empty():\n",
        "    shared_data += shared_queue.get()\n",
        "\n",
        "print(f\"Current value of shared_data: {shared_data}\")"
      ],
      "id": "cell-14"
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "outputId": "d6a0cab0-ebb0-4ecf-882a-ada0b071b0ee",
        "quarto-private-1": {
          "key": "colab",
          "value": {
            "base_uri": "https://localhost:8080/"
          }
        }
      },
      "outputs": [],
      "source": [
        "# Using a global variable is not a piece of cake in multiprocessing\n",
        "# as in multithreading\n",
        "import multiprocessing\n",
        "\n",
        "# Define a shared value between oprocesses\n",
        "shared_variable = multiprocessing.Value('i', 0)\n",
        "\n",
        "def worker_function():\n",
        "    global shared_variable\n",
        "    with shared_variable.get_lock():\n",
        "        shared_variable.value += 1\n",
        "\n",
        "procs = []\n",
        "for _ in range(3):\n",
        "  # Define our process but not yet start\n",
        "  proc = Process(target=worker_function)\n",
        "  # Start the process\n",
        "  proc.start()\n",
        "  # Manage all process definitions in a list\n",
        "  procs.append(proc)\n",
        "\n",
        "for proc in procs:\n",
        "    proc.join()\n",
        "\n",
        "print(f\"Current value of shared_data: {shared_data}\")"
      ],
      "id": "cell-15"
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "outputId": "1fac16a4-7a16-4a9d-ad7d-ab72a0ae1d1d",
        "quarto-private-1": {
          "key": "colab",
          "value": {
            "base_uri": "https://localhost:8080/"
          }
        }
      },
      "outputs": [],
      "source": [
        "# Or using queue\n",
        "from multiprocessing import Queue\n",
        "from multiprocessing import Process\n",
        "\n",
        "def worker_function(shared_queue):\n",
        "    shared_queue.put(1)\n",
        "\n",
        "shared_queue = Queue()\n",
        "\n",
        "procs = []\n",
        "for _ in range(3):\n",
        "    my_proc = Process(target=worker_function, args=(shared_queue,))\n",
        "    my_proc.start()\n",
        "    procs.append(my_proc)\n",
        "\n",
        "for proc in procs:\n",
        "    proc.join()\n",
        "\n",
        "# The main process retrieves data from the queue\n",
        "# and aggregate the result\n",
        "result = 0\n",
        "while not shared_queue.empty():\n",
        "    result += shared_queue.get(1)\n",
        "\n",
        "print(f\"Results from multiprocessing queue: {result}\")"
      ],
      "id": "cell-16"
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## One-Process One-Thread"
      ],
      "id": "89beafe2-59e6-4d94-b7dc-ee276b24cf80"
    },
    {
      "cell_type": "code",
      "execution_count": 8,
      "metadata": {
        "outputId": "3c554bd6-c8a1-42f8-9ca5-7de11d7eb3f4",
        "quarto-private-1": {
          "key": "colab",
          "value": {
            "base_uri": "https://localhost:8080/"
          }
        }
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Starting my 1st data func...\n",
            "Starting my 2nd data func...\n",
            "Completed my 1st data func!\n",
            "Completed my 2nd data func!"
          ]
        }
      ],
      "source": [
        "# Let's say you have one process with one thread only, how to deal with it?\n",
        "# AsyncIO helps us to do it\n",
        "import asyncio\n",
        "\n",
        "async def download_my_1st_data_func():\n",
        "    print(\"Starting my 1st data func...\")\n",
        "    await asyncio.sleep(2)\n",
        "    print(\"Completed my 1st data func!\")\n",
        "\n",
        "async def download_my_2nd_data_func():\n",
        "    print(\"Starting my 2nd data func...\")\n",
        "    await asyncio.sleep(2)\n",
        "    print(\"Completed my 2nd data func!\")\n",
        "\n",
        "def main():\n",
        "    loop = asyncio.get_event_loop()\n",
        "    loop.run_until_complete(asyncio.gather(\n",
        "        download_my_1st_data_func(),\n",
        "        download_my_2nd_data_func(),\n",
        "    ))\n",
        "    loop.close()\n",
        "\n",
        "# Run the event loop\n",
        "main()\n",
        "\n",
        "# YOU WILL MEET SOME WEIRD ERRORS HERE DUE TO IPYTHON NOTEBOOK,\n",
        "# LET'S MOVE TO A PYTHON SCRIPT"
      ],
      "id": "cell-18"
    }
  ],
  "nbformat": 4,
  "nbformat_minor": 5,
  "metadata": {
    "colab": {
      "provenance": [],
      "toc_visible": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3 (ipykernel)",
      "language": "python"
    },
    "language_info": {
      "name": "python",
      "codemirror_mode": {
        "name": "ipython",
        "version": "3"
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.9.13"
    }
  }
}