---
title: "CS336 Lecture 3: Architectures and Hyperparameters"
---

# Starting point: the "original" Transformer

Choices in the standard Transformer:

- Positional encoding vs. absolute/relative positional encoding

- Multi-head attention vs. single-head attention

- Feed-forward network (FFN) vs. Gated FFN

- Embedding size vs. sequence length

- Learning rate schedule vs. constant learning rate

- Dropout rate vs. no dropout   

Architecture variations: 
- Pre vs. post-norm: almost all modern LMs use pre-norm (but BERT was post-norm) 

| **Post-LN Transformer** | **Pre-LN Transformer** |
|------------------------|------------------------|
|$x_{l,i}^{\text{post},1} = \text{MultiHeadAtt}(x_{l,i}^{\text{post}}, [x_{l,1}^{\text{post}}, \ldots, x_{l,n}^{\text{post}}])$|$x_{l,i}^{\text{pre},1} = \text{LayerNorm}(x_{l,i}^{\text{pre}})$|
|$x_{l,i}^{\text{post},2} = x_{l,i}^{\text{post}} + x_{l,i}^{\text{post},1}$|$x_{l,i}^{\text{pre},2} = \text{MultiHeadAtt}(x_{l,i}^{\text{pre},1}, [x_{l,1}^{\text{pre},1}, \ldots, x_{l,n}^{\text{pre},1}])$|
|$x_{l,i}^{\text{post},3} = \text{LayerNorm}(x_{l,i}^{\text{post},2})$|$x_{l,i}^{\text{pre},3} = x_{l,i}^{\text{pre}} + x_{l,i}^{\text{pre},2}$|
|$x_{l,i}^{\text{post},4} = \text{ReLU}(x_{l,i}^{\text{post},3} W^{1,l} + b^{1,l}) W^{2,l} + b^{2,l}$|$x_{l,i}^{\text{pre},4} = \text{LayerNorm}(x_{l,i}^{\text{pre},3})$|
|$x_{l,i}^{\text{post},5} = x_{l,i}^{\text{post},3} + x_{l,i}^{\text{post},4}$|$x_{l,i}^{\text{pre},5} = \text{ReLU}(x_{l,i}^{\text{pre},4} W^{1,l} + b^{1,l}) W^{2,l} + b^{2,l}$|
|$x_{l+1,i}^{\text{post}} = \text{LayerNorm}(x_{l,i}^{\text{post},5})$|$x_{l+1,i}^{\text{pre}} = x_{l,i}^{\text{pre},5} + x_{l,i}^{\text{pre},3}$|
|  | **Final LayerNorm:**$x_{\text{Final},i}^{\text{pre}} \leftarrow \text{LayerNorm}(x_{L+1,i}^{\text{pre}})$|
 
 Pre-norm is a more stable architecture to train. 

 - New things: "double" norm: Grok, Gemma2

- LayerNorm vs. RMSNorm:

Original Transformer: LayerNorm - normalizes the mean and variance across $d_\text{model}$:
$$
y = \dfrac{x - \mathbb{E}[x]}{\sqrt{\mathbb{E}[(x - \mathbb{E}[x])^2] + \epsilon}}
$$

Notable models: GPT3/2/1, OPT, GPT-J, BLOOM

RMSNorm: normalizes the root mean square across $d_\text{model}$:
$$
y = \dfrac{x}{\sqrt{\dfrac{1}{d_\text{model}} \sum_{i=1}^{d_\text{model}} x_i^2 + \epsilon}}
$$

Why RMSNorm? 

- faster than LayerNorm but just as good (fewer operations, no mean calculation)

- fewer parameters (no bias)

| Model                  | Params  | Ops    | Step/s | Early loss         | Final loss | SGLUE  | XSum   | WebQ   | WMT EnDe |
|------------------------|---------|--------|--------|--------------------|------------|--------|--------|--------|----------|
| Vanilla Transformer    | 223M    | 11.1T  | 3.50   | 2.182 ± 0.005      | 1.838      | 71.66  | 17.78  | 23.02  | 26.62    |
| **RMS Norm**           | 223M    | 11.1T  | 3.68   | 2.167 ± 0.008      | **1.821**  | **75.45** | **17.94** | **24.07** | **27.14** |
| Rezero                 | 223M    | 11.1T  | 3.51   | 2.262 ± 0.003      | 1.939      | 61.69  | 15.64  | 20.90  | 26.37    |
| Rezero + LayerNorm     | 223M    | 11.1T  | 3.26   | 2.223 ± 0.006      | 1.858      | 70.42  | 17.58  | 23.02  | 26.29    |
| Rezero + RMS Norm      | 223M    | 11.1T  | 3.34   | 2.221 ± 0.009      | 1.875      | 70.33  | 17.32  | 23.02  | 26.19    |
| Fixup                  | 223M    | 11.1T  | 2.95   | 2.382 ± 0.012      | 2.067      | 58.56  | 14.42  | 23.02  | 26.31    |

More generally: dropping the bias term

Original Transformer:
$$
FFN(x) = max(0, xW_1 + b_1)W_2 + b_2
$$

Most implementations:

$$
FFN(x) = \sigma(xW_1 + b_1)W_2
$$

Reasons: memory (similar to RMSNorm), better optimization stability.

Basically everyone does pre-norm:

- Observations - nicer gradient propagation, fewer spike

- Intuition - keep the good parts of residual connections

- Some people add a second norm outside the residual stream (NOT post-norm)

Most people do RMSnorm:

- In practice, works as well as LayerNorm

- But, has fewer parameters to move around, which saves on wallclock time

- People more generally drop bias terms since the compute/param tradeoffs are not great.

"Zoo" of activations: ReLU, GELU, **SwiGLU**, GeGLU, ReGLU, SeLU, LiGLU, Swish, ...

- ReLU: Original Transformer, T5, Gopher, Chinchilla, OPT

- GeLU: GPT1/2/3, GPT-J, BLOOM

- SwiGLU: Llama, PaLM, T5, most models post-2023

## Gated activations (*GLU)

GLUs modify the ‘first part’ of a FF layer

$$
FF(x) = \max(0, xW_1) W_2
$$

Instead of a linear + ReLU, augment the above with an (entrywise) linear term

$$
\max(0, xW_1) \rightarrow \max(0, xW_1) \otimes (xV)
$$

This gives the gated variant (ReGLU) – note that we have an extra parameter (V)

$$
FF_{\mathrm{ReGLU}}(x) = (\max(0, xW_1) \otimes xV) W_2
$$

## Gated variants of standard FF layers

### **GeGLU**

$$
\mathrm{FFN}_{\mathrm{GeGLU}}(x, W, V, W_2) = (\mathrm{GELU}(xW) \otimes xV) W_2
$$

**Notable models:**  
T5 v1.1, mT5, LaMDA, Phi3, Gemma 2, Gemma 3

---

### **SwiGLU** 

(swish is $x * \mathrm{sigmoid}(x)$)

$$
\mathrm{FFN}_{\mathrm{SwiGLU}}(x, W, V, W_2) = (\mathrm{Swish}_1(xW) \otimes xV) W_2
$$

**Notable models:**  
LLaMa 1/2/3, PaLM, Mistral, OLMo, *most models post 2023*

---

*Note: Gated models use smaller dimensions for the $d_{ff}$ by $2/3$*


# Serial vs. parallel layers

Normal Transformer blocks are serial: they compute attention, then FFN. Could we parallelize the transformer block ?

# Variations in position embeddings

- Sine embedding

- Absolute embedding

- Relative embedding

- Rope embedding (most models converge to this)

## Hyperparameters

Transformer hyperparameter questions you might have had in 224n..

- How much bigger should the feedforward size be compared to hidden size?
- How many heads, and should num_heads always divide hidden size?
- What should my vocab size be?

And other model setting questions

- Do people even regularize these huge LMs?
- How do people scale these models – very deep or very wide?


## Surprising (?) consensus hyperparameter 1

Feedforward – model dimension ratio.

$$
\mathrm{FFN}(x) = \max(0, xW_1 + b_1) W_2 + b_2
$$

There are two dimensions that are relevant – the feedforward dim ($d_{ff}$) and model dim ($d_{model}$). What should their relationship be?

$$
d_{ff} = 4\, d_{model}
$$

This is _almost always_ true. There’s just a few exceptions.

## Surprising (?) consensus hyperparameter 2

Head-dim $\times$ num-heads to model-dim ratio. As a reminder, slide from 224n.

> ### **Multi-head self-attention is computationally efficient**
>
> - Even though we compute $h$ many attention heads, it's not *really* more costly.
>   - We compute $XQ \in \mathbb{R}^{n \times d}$, and then reshape to $\mathbb{R}^{n \times h \times d/h}$. (Likewise for $XK$, $XV$.)
>   - Then we transpose to $\mathbb{R}^{h \times n \times d/h}$; now the head axis is like a batch axis.
>   - Almost everything else is identical, and **the matrices are the same sizes.**

This doesn’t *have to* be true: we can have head-dimensions $>$ model-dim / num-heads.

But most models do follow this guideline.

# Vocabulary size

Monolingual models: 30-50k vocab (GPT, GPT2/3, T5, LLaMA)

Multilingual models: 100k-250k vocab (mT5, PaLM, GPT4, DeepSeek, ...)

# Dropout and other regularization

Do we need regularization during pre-training ?

Argument against:
- there is a lot of data, more than parameters

- SGD only does a single pass on a corpus (hard to memorize)

In practice...

| Model                | Dropout* | Weight decay |
|----------------------|----------|--------------|
| Original transformer | 0.1      | 0            |
| GPT2                 | 0.1      | 0.1          |
| T5                   | 0.1      | 0            |
| GPT3                 | 0.1      | 0.1          |
| T5 v1.1              | 0        | 0            |
| PaLM                 | 0        | (variable)   |
| OPT                  | 0.1      | 0.1          |
| LLaMA                | 0        | 0.1          |
| Qwen 14B             | 0.1      | 0.1          |

Many older models used dropout during pretraining

Newer models (except Qwen) rely only on weight decay

# GQA/MQA — Reducing attention head costs



