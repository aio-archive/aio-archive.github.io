---
title: CS336 Lecture 1 Overview
date: 26 April, 2025
---

[Lecture Recording](https://www.youtube.com/watch?v=SQ3fZ1sAqXI&t=25s)

Three types of knowledge:

- mechanics: how things work (what a Transformer is, ...)

- mindset: squeeze out the most out of the hardware

- intuitions: which data and modeling decisions yields good accuracy.

accuracy = effciency $\times$ resources

efficiency is way more important at larger scale (can not be wasteful).

-> What is the best model that we con build given a certain compute and budget ?

# Current Landscape

## Before 2010s

- Language model to measure the entropy of English (Shannon, 1950)

- n-gram language model (for machine translation, speech recognition)

## Neural ingredients (2010s)

- First language model (Bengio, 2003)

- Sequence-to-sequence modeling

- Attention mechanism (Bahdanau, 2014)

- Transformer architecture

- Mixture of Experts

- Model parallelism

## Early foundation models

- ELMO, BERT, Google's T5

## Embracing scaling

- OpenAI's GPT-2 (1.5B), GPT-3, PaLM, Chinchilla.

*these are all closed models.*

## Open models

BLOOM, Llama, Qwen, OLMo

- Closed model

- Open-weight models (DeepSeek)

- Open-source models (OLMo)

![](assets/figs/design.png)

# Tokenization

break up string into popular segments.

Byte-pair encoding (BPE) tokenizer

Tokenizer-free approaches: use bytes directly.

Variants:

![](assets/figs/variants.png)

# Training

Optimizer (Muon, SOAP)

Learning rate schedule (cosine, WSD)

Batch size

Regularization (dropout, weight decay)

Hyperparameters

There might be vast difference between well-tuned model and vanila Transformer.

# Systems

## Kernels

![](assets/figs/gpu-analogy.png)

![](assets/figs/parallelism.png)

Data movement between GPUs is even slower, but same 'minimize data movement' principle holds
Use collective operations (e.g., gather, reduce, all-reduce)
Shard (parameters, activations, gradients, optimizer states) across GPUs How to split computation: {data,tensor, pipeline,sequence) parallelism

## Inference

Goals: generate tokens given a prompt.
Inference is also needed for reinforment learning, test-time compute, evaluation, ...

![](assets/figs/inference.png)

Prefill (similar to training): tokens are given, can process all at once (compute-bound)
Decode: need to generate one token at a time (memory-bound)
Methods to speed up decoding:
• Use cheaper model (via model pruning, quantization, distillation)
• Speculative decoding: use a cheaper "draft" model to generate multiple tokens, then use the full model to score in parallel (exact decoding!)
• Systems optimizations: KV caching, batching

# Scaling laws

- Do experiments at small scale, predict hyperparameters/loss at large scale.

Compute-optimal scaling laws (Chinchilla paper)

![](assets/figs/chinchila.png)

# Evaluation

• Perplexity: textbook evaluation for language models
• Standardized testing (e.g., MMLU, HellaSwag, GSM8K)
• Instruction following (e.g., AlpacaEval, IFEval, WildBench)
• Scaling test-time compute: chain-of-thought, ensembling
• LM-as-a-judge: evaluate generative tasks
• Full system: RAG, agents

Common Crawl is NOISY.

# Data curation

• Data does not just fall from the sky.
look_at_web_data ( )
• Sources: webpages crawled from the Internet, books, arXiv papers, GitHub code, etc.
• Appeal to fair use to train on copyright data? [Henderson+ 2023]
• Might have to license data (e.g., Google with Reddit data) [article]
• Formats: HTML, PDF, directories (not text!)

# Data processing

• Transformation: convert HTML/PDF to text (preserve content, some structure, rewriting)
• Filtering: keep high quality data, remove harmful content (via classifiers)
Deduplication: save compute, avoid memorization; use Bloom filters or MinHash

# Alignment

Make the models actually useful

DPO, PPO, GRPO, ...

# Efficiency drives design decisions
Today, we are compute-constrilined, so design decisions will reflect squeezing the most out of given hardware.
Data processing: avoid wasting precious compute updating on bad / irrelevant data
• Tokenization: working with raw bytes is elegant, but compute-inefficient with today's model architectures.
• Model architecture: many changes motivated by reducing memory or FLOPS (e.g., sharing KV caches, sliding window attention)
• Training: we can get away with a single epoch!
• Scaling laws: use less compute on smaller models to do hyperparameter tuning
• Alignment: if tune model more to desired use cases, require smaller base models