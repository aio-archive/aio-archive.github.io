---
title: Multimodal Machine Learning
title-block-banner: true
date: 23 September 2023
categories: multimodal, 11-777
format:
  html: 
    grid:
      body-width: 1000px
---

> The content of this blog post is based on [11-777 Multimodal Machine Learning course](https://cmu-multicomp-lab.github.io/mmml-course/fall2023/) taught by LP Morency and Paul Liang. 

<i class="bi bi-camera-video"></i> [Video](https://www.youtube.com/watch?v=DPkwjgaRvyI&list=PL-Fhd_vrvisMYs8A5j7sj8YW1wHhoJSmW&index=1&t=33s) | <i class="bi bi-filetype-ppt"></i> [Slide](https://piazza.com/class_profile/get_resource/ll1e0syhci53g6/llvjl0qrqr138u)

<iframe width="1000" height="500" src="https://www.youtube.com/embed/DPkwjgaRvyI?si=cXZ-UFJMbjOdnfEQ" title="YouTube video player" frameborder="0" allow="accelerometer; autoplay; clipboard-write; encrypted-media; gyroscope; picture-in-picture; web-share" referrerpolicy="strict-origin-when-cross-origin" allowfullscreen></iframe>

# What is multimodal ?

## Definition

Multiple **modalities** refer to the different ways information can be expressed or perceived, such as through vision, sound, taste, touch, or other senses. Some modalities, like vision or sound, are more **concrete**, as they directly engage with sensory experiences. Other modalities are more **abstract**. These include things like **object categories** (grouping objects based on shared characteristics), **mathematical expressions** (using symbols to convey numerical relationships), or **logical structures** (like reasoning and relationships in abstract problem-solving). Abstract modalities are often more removed from direct sensory experience, relying on symbolic or conceptual representations instead of perceptual input. A more research-oriented defintion of multimodal is:

::: {.callout-note}
# Definition
Multimodal is the scientific study of **heterogeneous** and **interconnected** data.
:::

where *interconnected* refers to connected and interacting between modalities.

## Dimensions of modality heterogeneity

### Heterogenenous modalities

**Heterogeneous modalities** refer to information being expressed or perceived through different forms, each with its own unique qualities, structures, and representations. For example, a single concept can be communicated through text, images, sounds, or physical gestures, each modality offering a distinct way of understanding the information.

In contrast, **abstract modalities**—such as mathematical formulas or logical statements—tend to be more **homogeneous** because they rely on consistent, symbolic representations that abstract away from the sensory diversity of more concrete forms. These abstract representations often prioritize precision and consistency across contexts, unlike heterogeneous modalities, which vary based on the form in which the information is presented.

### Dimensions of heterogeneity

Information present in different modalities will often show diverse qualities, structures and representations.

What is the fundamental unit or element in the new modality that will carry enough meaningful information, while also ensuring sufficient distinction between elements (such as objects in an image)?

Element representations (discrete, continuous, granularity)

Element distributions (density, frequency)

Stucture (temporal, spatial, latent, explicit)

Information (abstraction, entropy)

Noise (uncertainy, noise, missing data)

Relevance (task, context dependence)


Connection refers to the overlap of information between different modalities. For example, if we consider language and vision, how much information is shared between the two?

## Modality connections and interactions

### Connected modalities

The **strength of the connection between modalities** can be understood in different ways:

**Statistical connections** involve patterns in data. For instance, there may be an **association**, where two modalities co-occur or are correlated. An example of this is when the word "dog" often appears in descriptions of images containing dogs. There’s also **dependency**, where one event or signal in one modality depends on another, such as a sound following a specific visual action (like a door slamming shut after someone pushes it).

![[Pasted image 20240923172923.png]]

On the other hand, **semantic connections** focus on the meaning shared between modalities. **Grounding** refers to how concepts are anchored across modalities, like the word "tree" referring to the same object that is visually recognized as a tree. **Relationships** involve deeper connections, where modalities interact on a meaningful level, such as how the visual depiction of a smiling face might semantically relate to the word "happiness."

For example, in a video, the connection between a speaker's words ("It's raining") and the visual of raindrops outside might be **statistically strong** due to the co-occurrence of the word "rain" and the visual of rain, while also being **semantically grounded** because both modalities refer to the same concept—rain.

### Interacting modalities

The modality comes together and create something new that neither the modalities have by itself. 

The taxonomy of interacting modalities distinguishes between redundant and non-redundant relationships.

In **redundant interactions**, modalities overlap in the information they provide. If the interaction is **equivalent**, they convey the same information without influencing each other, like seeing and hearing the word "apple" simultaneously. In **enhancement**, the modalities complement each other, adding depth or clarity, as in watching a video with matching audio.
![[Pasted image 20240923174232.png]]
For **non-redundant interactions**, the modalities offer different information. In **independence**, they function separately, while in **dominance**, one modality overtakes the other, such as language overriding visual input. **Modulation** occurs when one modality alters the perception of another, and **emergence** happens when combining modalities creates something entirely new, producing an effect that neither could achieve alone.

# Core technical and conceptual challenges

| **Category**   | **Description**                                                                                                       | **Subcategories**                                  |
|----------------|-----------------------------------------------------------------------------------------------------------------------|----------------------------------------------------|
| Representation | Learning representations that reflect cross-modal interactions between individual elements, across different modalities | - Fusion<br>- Coordination<br>- Fission            |
| Alignment      | Identifying and modeling cross-modal connections between all elements of multiple modalities, building from the data structure | - Discrete alignment<br>- Continuous alignment<br>- Contextualized representation |
| Reasoning      | Combining knowledge, usually through multiple inferential steps, exploiting multimodal alignment and problem structure | - Structure modeling<br>- Intermediate concepts<br>- Inference paradigm<br>- External knowledge |
| Generation     | Learning a generative process to produce raw modalities that reflects cross-modal interactions, structure and coherence | - Summarization<br>- Translation<br>- Creation     |
| Transference   | Transfer knowledge between modalities, usually to help the target modality which may be noisy or with limited resources | - Transfer<br>- Co-learning via generation<br>- Co-learning via representation |
| Quantification | Empirical and theoretical study to better understand heterogeneity, cross-modal interactions and the multimodal learning process | - Heterogeneity<br>- Interactions<br>- Learning    |


The **core technical challenges** in multimodal machine learning, as outlined in the paper "Foundations and Recent Trends in Multimodal Machine Learning" by Paul Liang, Amir Zadeh, and Louis-Philippe Morency, can be grouped into six key areas:

## Representation
This challenge focuses on how to learn representations that effectively capture the interactions between elements from different modalities. The main question is how to represent the relationships between local elements (like objects in an image) and global structures. Sub-challenges include:

- **Fusion**, where multiple inputs are combined into one representation (fewer representations than modalities).

- **Coordination**, where each modality is represented individually, but their connections are maintained (modalities and representations are equal).

- **Fission**, where more representations than modalities are created (factorizing into multiple representations for richer understanding).

## Alignment
The challenge of alignment involves identifying and modeling cross-modal relationships at a granular level, where elements from different modalities (like words in language or objects in images) must be connected. This often requires working with structured data, such as breaking an image into a list of objects for grounding and linking it to textual descriptions.

## Reasoning
Reasoning in multimodal systems refers to the ability to perform inference by integrating knowledge from multiple modalities. This often requires multiple inferential steps, building on the aligned elements of different modalities, and solving tasks that require a deeper understanding of the relationships between modalities.

## Generation
The generation challenge involves creating new content based on multimodal inputs. This could involve tasks such as:

- **Summarization**, where the system compresses multimodal data into a coherent summary.

- **Translation**, converting information from one modality to another (e.g., image-to-text).

- **Creation**, where entirely new content is generated from multimodal inputs.

## Transference
This challenge focuses on transferring knowledge between modalities. Often, one modality may have more data or higher quality data than another. The goal is to leverage the stronger modality to help the weaker one. Sub-challenges include:

- **Co-learning via representation**, where shared representations help both modalities.

- **Co-learning via generation**, where generation techniques help bridge gaps.

- **Transfer**, where knowledge from one modality aids another.

## Quantification
Finally, quantification involves understanding and evaluating how multimodal learning works, both empirically and theoretically. This challenge includes studying the interactions between modalities, measuring heterogeneity, and assessing the effectiveness of cross-modal learning processes.

